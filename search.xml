<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Ubuntu下口袋妖怪终端主题安装]]></title>
    <url>%2F2017%2F12%2F13%2FUbuntu%E4%B8%8B%E5%8F%A3%E8%A2%8B%E5%A6%96%E6%80%AA%E7%BB%88%E7%AB%AF%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[之前看到开源中国的文章，介绍了国内最火的开源项目——Python篇。看到一个比较有意思的项目——终端的口袋妖怪主题。之前介绍安装WRF的博客，细心的同学已经发现，我已然安装上了那个主题，今天就来介绍下Ubuntu下口袋妖怪终端主题安装吧。 先上图。 这回先贴出这个链接。 可能是国内最火的开源项目——Python篇 还有做这个主题的github链接。 Pokemon-Terminal github链接 我用的Linux是Ubuntu，接下来就按照这个进行安装。首先这个主题支持的终端模拟器主要是以下三个：iTerm2， Terminology，Tilix。这里选择安装Tilix，首先添加这个终端模拟器仓库的公钥。这里我都是以root超级用户权限操作的，如果没有的话，请在命令前面加sudo。 1add-apt-repository ppa:webupd8team/terminix 1apt update 安装Tilix。 1apt install tilix 然后打开Tilix，并锁定到启动器。 有可能出现配置问题，建议是在bashrc中添加语句。 123if [ $TILIX_ID ] || [ $VTE_VERSION ]; then source /etc/profile.d/vte.shfi 具体问题可参照如下网址：终端配置 接下来设置配置方案。 不过下面的教程是针对该主题的1.0.7版本，目前主题已经更新到1.1.1版本。如果现在安装的话，更方便的方式是，先安装Python3.6，同时安装setuptools（推荐新立得安装），然后下载github仓库。接着只需要如下的命令即可。 1sudo python3.6 setup.py install 下面的npm安装方式只针对之前的版本（只是顺带记录，不过现在也提供了这个方式，但还是建议上面的安装方法）然后安装npm，最主要Ubuntu16.04默认有个python3.5.2，npm直接安装是跟Python版本对应，而新主题要求在3.6的Python以上，也可以考虑升级原有的Python。 1apt install npm 接着安装pokemon-terminal。 1npm install --global pokemon-terminal 大功告成。 主要参考：国外大神安装教程]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows和Linux双系统安装教程]]></title>
    <url>%2F2017%2F12%2F12%2FWindows%E5%92%8CLinux%E5%8F%8C%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[最近刚刚完成了Windows和Linux双系统（这里以Ubuntu安装为例）的安装，应某奔同学要求，这里简单记录下安装过程。 1 系统启动盘准备因为不想装第二个系统的时候重做系统盘，所以这里做的系统启动盘本身就是Windows和Linux双系统引导的。这里用的工具前面在docker安装笔记的时候已经介绍过：YUMI。可以翻翻前面的博客进行了解。 花式安装蓝鲸鱼札记 百度搜索的时候，要准确定位，请搜YUMI USB，如果想了解点其他的（咳咳咳），去掉USB。不过另外一点是有人推荐用EFI的版本（虽然觉得差别不大），制作系统启动盘前面的博客也介绍过了。唯一要注意的就是为了做成多引导启动，在选择windows系统制作的时候，记得选择multiboot。 2 Windows系统安装Windows和Linux双系统安装，通常是选择先装Windows，然后再安装Linux，最后通过Windows引导Linux系统启动。 Windows安装是比较方便的，网上教程颇多，对于还没有系统的电脑来说，其实就是把做好的启动盘插入，然后狂按某FX键（反正视电脑型号和品牌而定）。主要是设置boot，选择从U盘启动。如果是在Windows系统下重装系统，就只需要把我们的系统盘放进去。然后双击setup.exe就好了。 3 分出给Linux系统的磁盘空间安装完成之后，首先先从Windows的磁盘里面分出給Linux系统的空间。 右击”我的电脑”→”管理”→”存储”→”磁盘管理” 接下来只要选择你要分出空间的盘，右击压缩卷。这个画红圈的就是设置Linux系统的大小，1024M是1G，根据情况分区。 4 安装Linux系统接下来就插入系统启动盘，重启电脑，通常系统启动盘会引导进入图形安装界面，选择Linux安装。Ubuntu这里会让你选择Win10和Ubuntu共存，还是清除Windows系统，还有一个其它选项，这里选择其它选项。接下来就是Linux系统分区了。这个分区的内容以及具体安装教程参照前面的博客，之前在虚拟机上安装Ubuntu的时候我已经提过了，这里就不详细介绍。 VMware Workstation下安装Ubuntu 64位系统 注意：分区分完了，不能直接安装，最后选用引导的选项必须改选为前面Linux分区里面的”/boot”分区。不能选U盘，也不能选其他盘。然后就可以一路安装了。最后重启电脑。 于是——你发现开起来还是Windows。嗯，主要原因需要用一个多重系统启动引导的文件，这个时候最后一个主人公就要登场了。 5 EasyBCD安装使用最后一个主人公就是EasyBCD啦，这个软件是用于系统配置创建多重启动系统的引导文件，也就是新创建一个启动文件，可以让你的电脑在启动的时候，有进入何种系统的选择。 EasyBCD官网地址 “添加新条目” -“Linux/BSD”-类型“Grub 2”驱动器“自动定位和加载”-“添加条目” 一波操作猛如虎。然后重启电脑就可以了。 最后效果。 顺带感谢一波网上大神，附上链接。 知乎：怎样安装 Windows 7 与 Linux 的双系统？ 电脑安装双系统（win+Linux）的一些重要步骤总结]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Windows 10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python开篇——简介、pip和conda]]></title>
    <url>%2F2017%2F11%2F29%2FPython%E5%BC%80%E7%AF%87%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B%E3%80%81pip%E5%92%8Cconda%2F</url>
    <content type="text"><![CDATA[人生苦短，我用Python。 那我还是来介绍一下它（凑一波字数）吧。 Python（英国发音：/ˈpaɪθən/ 美国发音：/ˈpaɪθɑːn/）, 是一种面向对象的解释型计算机程序设计语言，由荷兰人Guido van Rossum于1989年发明，第一个公开发行版发行于1991年。Python是纯粹的自由软件， 源代码和解释器CPython遵循 GPL(GNU General Public License)协议。Python语法简洁清晰，特色之一是强制用空白符(white space)作为语句缩进。Python具有丰富和强大的库。它常被昵称为胶水语言，能够把用其他语言制作的各种模块（尤其是C/C++）很轻松地联结在一起。常见的一种应用情形是，使用Python快速生成程序的原型（有时甚至是程序的最终界面），然后对其中有特别要求的部分，用更合适的语言改写，比如3D游戏中的图形渲染模块，性能要求特别高，就可以用C/C++重写，而后封装为Python可以调用的扩展类库。需要注意的是在您使用扩展类库时可能需要考虑平台问题，某些可能不提供跨平台的实现。7月20日，IEEE发布2017年编程语言排行榜：Python高居首位 。 百度百科 也有不少新闻提了，Python在数据科学领域位居首位。可以说接着深度学习和机器学习的火热，Python再次大热了一波，当然说Python是深度学习的语言也不准确，事实上深度学习里的Python主要是胶水语言的作用，比如大家喜闻乐见的Tensorflow，其实底层是C++，只不过提供了Python接口。 总体来说Python就是一句话，写得快，跑得慢。同样的功能实现，C和C++可能要100行，Python可能只需要10行（数字不一定准，但是能节省写的时间是真的）。同时最强大的大概是Python丰富和强大的库了（这点跟R还是蛮像的，所以都很流行）。 这里简介一些重点库。 1 Python常用库爬虫：Scrapy（举世闻名，分布式爬虫框架，不仅仅是简单的库）、beautifulsoup4、urllib、urllib2、selenium等。 机器学习：scikit-learn（灰常牛逼的一个库，几乎所有机器学习算法都囊括了应该），NLTK（自然语言处理工具包 ）等（用一个等的原因是，说起来除了scikit-learn，我还真不知道还有啥，hhh）。 网站：Django（重量级网页框架）、Flask（轻量级网页框架）等。 数据处理科学计算：Numpy（数组矩阵神器）、Scipy（科学计算神器）、Pandas（熊猫包，R语言玩家转Python的最爱）等。 可视化：matplotlib（matlab风格式的包）、seaborn（散点图矩阵神器）、ggplot（R语言可视化神器的Python版本）、plotly（这个神器是个js库，不过也有各种流行的语言接口）等。 深度学习：看知乎吧（强行蹭一波热点，深度学习是机器学习的一部分） 地学相关：basemap（画地图的库）、cartopy（画地图的库）、Folium（leaflet的Python版本 ）、GDAL（开源GIS库）、geocoder（地理编码神器）、geopandas（地理数据的熊猫包）、geopy（还没玩过）、PySAL（空间计量经济学的一个神包）等（跟地学相关的包实在太多，后面有空的话，考虑会重点介绍几个包）。 另外这里多提两个关于ArcGIS的包，一个是arcpy，熟悉ArcGIS的同学知道，这个是ArcGIS内嵌Python的神器，可以非常方便调用ArcGIS各项功能，但是有一点就是不开源（毕竟人家是商业软件嘛，所以那些老想着在自己安装的Python上import arcpy的同学可以省省功夫了），另外多提一个Esri公司新推出的ArcGIS API for Python，这个在前面的用户大会观感上提到过，是基于portal和online的一套API，还是有些可以玩的价值，后面也会考虑介绍这个内容。 2 Python安装安装这个事情实在太小了。毕竟开源语言，一路next安装完毕。唯一问题可能是要配个环境变量。 Python官网 顺带一提就是现在的2.0和3.0之争。Python 2.0到3.0过渡确实还做得一般，但是3.0有它的好处，2.0目前就是比较稳定。很多包都暂时没迁移到Python3.0上。但是最近numpy的一个通知，正在显示3.0时代的到来。 numpy团队宣布2020年停止支持2.0 笔者自己也还是用的2.7，不过还是在考虑学习3的事情（其实也不是很麻烦），如果你刚刚起步就从3.0开始也没毛病。 当然如果为了科学计算，可以考虑直接安装Anaconda，而不是从Python开始一步一步安装。 anaconda （一个开源的Python发行版本）anaconda指的是一个开源的Python发行版本，其包含了conda、Python等180多个科学包及其依赖项。[1] 因为包含了大量的科学包，Anaconda 的下载文件比较大（约 500 MB），如果只需要某些包，或者需要节省带宽或存储空间，也可以使用Miniconda这个较小的发行版（仅包含conda和 Python）。 百度百科 3 pip和conda最后讲讲题目里的这俩货，其实conda在上面介绍Anaconda的时候已经讲到了，这两个都是Python包管理的工具。还是很不错的。这个部分也介绍下这两个工具怎么用。 pip安装就不提了，不清楚可以自己百度。 安装包的命令如下： 1pip install packages 然而一顿操作猛如虎之后。 事实上，我们去找一下pip的文件就明白为什么了。 位置是Python安装路径/Scripts下。pip是个exe呀。这回只要打开cmd，定位到pip的exe文件夹下，然后pip install就OK了。用查看已经安装的包的命令测试。 1pip list conda其实功能命令跟pip差别不大，这里就不多做介绍了。也是献上conda查看包的结果图。]]></content>
      <categories>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客评论新神器——Valine]]></title>
    <url>%2F2017%2F11%2F24%2Fhexo%E5%8D%9A%E5%AE%A2%E8%AF%84%E8%AE%BA%E6%96%B0%E7%A5%9E%E5%99%A8%E2%80%94%E2%80%94Valine%2F</url>
    <content type="text"><![CDATA[接着前文的hexo博客继续优化。其实也没什么大动作，这回主要是对评论系统做些整理。本文如题目所示：介绍一个神器，Valine。 本来hexo博客用的是gitment，我也非常喜欢，看着逼格就超高呀。无奈我用着bug略多，而且毕竟有github账户的小伙伴似乎并不多。于是我就忍痛准备换评论系统。然后在最近刚刚加入的hexo博客群里，看见了一个神器。也就是本篇主人公——Valine.js。 具体配置就见如下的文章吧。它的定义—— 一款极简的无后端评论系统。 在多说和网易云跟帖相继倒闭的情况下，这个简直是救人一命胜造七级浮屠呀。 Valine – 一款极简的评论系统 Valine官网 这个评论系统是基于LeanCloud的，大家应该对这个很熟悉，对，Hexo的博客阅读量统计也是它。官网网址如下，需要注册一个账户。 LeanCloud官网 另外多提一句，最近为了更好更快升级，我重新在本地部署了下我的hexo博客，就是把NexT主题利用Git方式克隆到本地，以后升级就比较快了。这个内容可以参见官网。也因为克隆了最新版本，发现已经集成了gitment和valinejs（神速）。所以配置起来就很方便了。 这里重点讲一下关于邮件提醒的事。 这样设置一下就可以了。不清楚的查看下面的链接也可以。 valline详细配置官网 Valine 评论系统中的邮件提醒设置 最后多说一句。支持导出评论和数据分析。]]></content>
      <categories>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+NexT博客优化第二弹]]></title>
    <url>%2F2017%2F11%2F22%2Fhexo%2BNexT%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96%E7%AC%AC%E4%BA%8C%E5%BC%B9%2F</url>
    <content type="text"><![CDATA[最近对hexo和NexT博客又做了一次优化。主要干了三件事。 博客地址 顶部加载条实现： 这个如果用的是比较新的NexT主题，只需要在配置文件里面进行修改就可以了。旧的话，就需要对/next/layout/_partials/head.swig文件做些修改，添加对应的代码。 123456789101112131415&lt;script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"&gt;&lt;/script&gt;&lt;link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet"&gt;&lt;style&gt;.pace .pace-progress &#123;background: #1E92FB; /*进度条颜色*/height: 3px;&#125;.pace .pace-progress-inner &#123;box-shadow: 0 0 10px #1E92FB, 0 0 5px #1E92FB; /*阴影颜色*/&#125;.pace .pace-activity &#123;border-top-color: #1E92FB; /*上边框颜色*/border-left-color: #1E92FB; /*左边框颜色*/&#125;&lt;/style&gt; 具体的可以点击上次那篇推荐的文章。 hexo的next主题个性化教程：打造炫酷网站 另外就是增加了词云和运行时间。 词云其实就是标签做的，放在侧边栏上。需要安装插件。 npm install hexo-tag-cloud@^2.0.* --save 接着在next/layout/_macro/sidebar.swig添加如下内容。 123456789101112&#123;% if site.tags.length &gt; 1 %&#125;&lt;script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"&gt;&lt;/script&gt;&lt;script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"&gt;&lt;/script&gt;&lt;div class="widget-wrap"&gt;&lt;h3 class="widget-title"&gt;Tag Cloud&lt;/h3&gt;&lt;div id="myCanvasContainer" class="widget tagcloud"&gt;&lt;canvas width="250" height="250" id="resCanvas" style="width=100%"&gt;&#123;&#123; list_tags() &#125;&#125;&lt;/canvas&gt;&lt;/div&gt;&lt;/div&gt;&#123;% endif %&#125; 运行时间的话，在next/layout/_custom/sidebar.swig文件中先添加。 123456789101112131415161718192021222324252627&lt;div id="days"&gt;&lt;/div&gt;&lt;/script&gt;&lt;script language="javascript"&gt;function show_date_time()&#123;window.setTimeout("show_date_time()", 1000);BirthDay=new Date("05/27/2017 15:00:00");today=new Date();timeold=(today.getTime()-BirthDay.getTime());sectimeold=timeold/1000secondsold=Math.floor(sectimeold);msPerDay=24*60*60*1000e_daysold=timeold/msPerDaydaysold=Math.floor(e_daysold);e_hrsold=(e_daysold-daysold)*24;hrsold=setzero(Math.floor(e_hrsold));e_minsold=(e_hrsold-hrsold)*60;minsold=setzero(Math.floor((e_hrsold-hrsold)*60));seconds=setzero(Math.floor((e_minsold-minsold)*60));document.getElementById('days').innerHTML="已运行"+daysold+"天"+hrsold+"时"+minsold+"分"+seconds+"秒";&#125;function setzero(i)&#123;if (i&lt;10)&#123;i="0" + i&#125;;return i;&#125;show_date_time();&lt;/script&gt; 接着在next/layout/_macro/sidebar.swig文件中修改。 123456789101112131415161718&#123;# Blogroll #&#125;&#123;% if theme.links %&#125; &lt;div class="links-of-blogroll motion-element &#123;&#123; "links-of-blogroll-" + theme.links_layout | default('inline') &#125;&#125;"&gt; &lt;div class="links-of-blogroll-title"&gt; &lt;i class="fa fa-fw fa-&#123;&#123; theme.links_icon | default('globe') | lower &#125;&#125;"&gt;&lt;/i&gt; &#123;&#123; theme.links_title &#125;&#125;&amp;nbsp; &lt;i class="fa fa-fw fa-&#123;&#123; theme.links_icon | default('globe') | lower &#125;&#125;"&gt;&lt;/i&gt; &lt;/div&gt; &lt;ul class="links-of-blogroll-list"&gt; &#123;% for name, link in theme.links %&#125; &lt;li class="links-of-blogroll-item"&gt; &lt;a href="&#123;&#123; link &#125;&#125;" title="&#123;&#123; name &#125;&#125;" target="_blank"&gt;&#123;&#123; name &#125;&#125;&lt;/a&gt; &lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt; &#123;% include '../_custom/sidebar.swig' %&#125; &lt;/div&gt; &#123;% endif %&#125; 觉得需要调整颜色的还可以在/next/source/css/_custom/custom.styl加入如下的语句。 1234567// 自定义的侧栏时间样式#days &#123; display: block; color: rgb(7, 179, 155); font-size: 13px; margin-top: 15px;&#125; 另外参照着增加了Readme，增加了一些图标等。 有找到了几篇还不错的文章。以及本次优化参考的主要文章链接。 打造个性超赞博客Hexo+NexT+GithubPages的超深度优化 在移动设备下启用NexT主题的目录页面和回到顶部按钮 Hexo博客中使用标签云hexo-tag-cloud Hexo 标签云插件github地址]]></content>
      <categories>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Endnote使用小记]]></title>
    <url>%2F2017%2F11%2F20%2FEndnote%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[如何高效快速管理阅读的文献是研究生学习中的第一步。这里就推荐下目前正在使用的Endnote（当然还有其他平台，这里就对我用的Endnote重点介绍下）。这里提三个简单的功能小记。 1 导入Web of Science的参考文献首先是进入Web of Science首页，这里顺带提供一个外网访问的方法。使用机构登陆，选择中科院CAS。 原理是中科院集成在WOS的机构用户登录中，是机构用户登录中唯一一个国内授权账号体系，任何用户只要注册了中国科技网通行证（目前为免费注册）即可通过该机构用户转至登录，即可实现利用中科院机构账号访问WOS的若干数据库。 具体可以参照如下的链接：科研干货之——随时随地访问Web of Science 有用过地理空间数据云的同学应该会对这个通行证比较熟悉，是同一个通行证。具体注册就不讲了。 这次选择的样例文章是：Urban Energy Index for Buildings (UEIB): A new method to evaluate the effect of urban form on buildings’energy demand。 来自于景观生态学中除了Landscape Ecology之外的另一本旗舰刊物——Landscape and Urban Planning。 勾选，并点击保存至“Endnote desktop”。 记录内容选择包括摘要的那个（这个其实根据个人需求）。 存储ciw文件。 Endnote中操作。 选择Web of Science Core Collection(TR)和Unicode(UTF-8) 接着就成功导入了。可以点击画圈部分链接pdf。 2 在Endnote上做笔记在看的时候我们想做些简单的笔记。 确实这篇文章最大亮点之一就是这个图摘要。第一眼就看到了这个图之后，我就直接点击下载了。 3 在Endnote上做文献阅读后的Summary在看完英文文献之后，我觉得必要的工作是做summary。那么如何在Endnote上做这个工作呢？ 首先点击“edit” Note或者Research Notes都可以。 双击打开，可以在这两个下面写你的summary 保存之后还可以显示在首页。 具体的还可以看知乎上的解答。 EndNote X7 如何做笔记？]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Endnote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何实现一个以中国为中心的世界地图]]></title>
    <url>%2F2017%2F11%2F14%2F%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E4%BB%A5%E4%B8%AD%E5%9B%BD%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84%E4%B8%96%E7%95%8C%E5%9C%B0%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[最近屡屡有小伙伴为各种目的在询问有没有中国位于中心的世界地图。在某位同学的强烈要求下，我决定稍微记录下这个以我大中华为中心的世界地图的做法。效果图： 原始数据。 1 ArcGIS第一种就简单介绍下ArcGIS平台上如何操作吧。 首先在ArcGIS软件中，右击Layers（图层）→Properties（属性）→Coordinate System（坐标系） 然后如图所示点击生成一个新的Projected Coordinate System（投影坐标系）。 按照如图所示设置。 并用Save As，导出一个.prj的投影文件。 接着用Arctoolbox的投影工具进行投影变换（我本身数据是WGS1984的地理坐标系）。 选择投影的时候可以直接import。 等待运行。 结果图。 如上，其实过程不复杂。最关键的这个使得中国能居于中间的原因是投影参数里面的第三个参数——Central Meridian，也就是中央经线。有兴趣还可以自行调整，我这里设150结果如上，也可以自行设定，只需要双击投影文件修改属性即可。 2 R第二种介绍下R语言的方法。R语言做空间数据的这些处理最主要的两个包就是sp和rgdal。所以在处理前请先安装这两个包。 接下来直接进入正题。 我们需要先读入空间数据，然后对空间数据进行投影变换。 如何读空间数据就请点击前面我写过的文章，戳 其实关键步骤就是用prj4字符串构造出我们需要的投影坐标系。关于这一点，推荐看下面的这篇博客学习。 坐标详解与PROJ.4使用说明 此外推荐几个网站用来查询相关坐标系信息。 epsg开源查询网站，托管在github上 空间参考官网网站 OGR驱动中的矢量格式、读写情况以及代码 这里直接给出对应的prj4字符串。”+proj=eqc +lat_ts=30 +lat_0=0 +lon_0=150 +ellps=WGS84 +datum=WGS84 +units=m +no_defs” 用sptransform转换投影坐标系，结果如图。 打完收工。 贴个R语言源码图。]]></content>
      <categories>
        <category>GIS</category>
      </categories>
      <tags>
        <tag>ArcGIS</tag>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[花式安装蓝鲸鱼札记]]></title>
    <url>%2F2017%2F11%2F08%2F%E8%8A%B1%E5%BC%8F%E5%AE%89%E8%A3%85%E8%93%9D%E9%B2%B8%E9%B1%BC%E6%9C%AD%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[大家好，蓝鲸鱼又来了。 自从上篇搞完蓝鲸鱼在Win 7下安装之后，笔者就看到了另一篇文章。 Win10 Docker 安装使用 感觉仿佛很棒的样子。于是我们就开始愉快地新系（zuo）统（si）旅程，笔者顺手就升级了Win 10系统。这里顺带介绍一个神器。 YUMI是一个多系统USB启动盘制作的软件。最早是在知乎回答上看到。 如何制作支持安装多系统（Win/Linux）的U盘启动盘？ YUMI官网 也可以点击这个百度网盘地址 操作手册。 一键安装即可。 好。回到正题。Win 10升完之后，我遇上了各种有的没的bug。。。包括蓝鲸鱼！ 当然还是先简介下怎么安装。 算了。。。自己看官网文档去。 官网 简单地说。我遇上的问题就是如下。 其实总结起来，就是网络网卡一些问题。百度、必应、谷歌都未果。但是在github上找到了对应的issues。 Hyper-V was unable to find a virtual switch with name “DockerNAT”. 然后历经磨难，突然发现，是我的毒霸把一堆服务给禁止了。然后没有按顺序一个个开启，一堆毛病。于是就愉快地开启docker之旅。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win7下蓝鲸鱼安装以及Xshell连接操作]]></title>
    <url>%2F2017%2F11%2F06%2FWin7%E4%B8%8B%E8%93%9D%E9%B2%B8%E9%B1%BC%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8AXshell%E8%BF%9E%E6%8E%A5%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[我爱吃金拱门，开封菜也不错，但我觉得最好吃的是小红帽，那我们就来安装个蓝鲸鱼吧。 1 Docker简介隆重推出我们的主人公——蓝鲸鱼，Docker先生。 接下来让我抄一段百度百科的简介。 Docker （基于Go语言开发！基于Go语言开发！基于Go语言开发！）是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。看不够的点这里，看不懂的点这里。 嗯，所以这东西吧，你可以把它当成小型虚拟机，貌似目前的需求也差不多，配置消耗相对虚拟机小得多，相对来说能快速部署环境是个优点。 具体的也可以查看Docker官网 2 Win 7下安装过程目前来看，Docker跟Linux和MacOS系统应该还是更相容的，但是毕竟我们是研究GIS的人员，Windows才是王道（谁让ArcGIS只有windows版本呢？呃，不过其实QGIS和GRASS GIS就有各种平台版本，功能也相当强大)。那么如何在Win 7下安装Docker呢？其实这样子我们通过Docker去运行Linux环境（用Docker的话讲，这叫容器）的话，相对更方便些，消耗资源也比虚拟机小，这也是我开始安装的目的。本文就来介绍下。 目前Docker在Win 10最新系统安装已经十分方便，而且不需要依赖Virtual Box。 具体就看官网吧 直接下载Docker for Windows的app安装就可以。 而Win 7的话，上面那个不支持，所以还得依赖Virtual Box（其实说来还是得靠虚拟机）。接下来就来讲讲怎么安装吧。 Win 7上使用Docker Toolbox.exe进行安装。 github下载地址，但是这个地址国内下载很慢（而且似乎不是很新，也可以从Docker官网下载）。可以用下面的另一个地址。 下载地址 下载下来之后，就只需双击exe开始安装。 如果第二步已经安装过Git和Virtual也可以不勾选，中间还会让安装Oracle的一些东西，全部安装即可。但是后面就不能直接双击.sh文件运行docker了，得在git bash下面运行。 安装完毕之后，应该会出现这两个文件。 通常双击什么的快速启动终端就可以了，第一次需要配置花的时间久一点。或者也可以到Docker安装文件夹下，双击.sh文件。不过后面发现这会报错，找不到boot2docker.iso文件，而这个文件就在Docker安装文件夹下。 先将这个iso文件拷贝到，C:[用户]Administrator.docker\machine\cache下，再运行就.sh文件或者运行Docker Quickstart Terminal就成功了。 3 用Xshell连接自己的Docker用单纯的命令行有很多限制，一般可以用终端模拟器来连接，这里用的Xshell，也可以用其他终端模拟器。 这里用户名为docker，密码为tcuser（默认）。 连接成功。接下来运行个hello world。 docker run hello-world 4 docker hub注册这个就是可以将自己的镜像push到仓库里的账户，登陆的话，只需要敲入如下命令。 docker login 填入账户和密码即可。 账户名最好用小写字母和数字即可。 一些参考博客及文档。 创建Docker Hub账号&amp;库 你的Docker Hub账户 使用Docker Hub | Docker 中文指南 DOCKER windows 7 详细安装教程 Windows7 上运行docker实战 完整记录在 windows7 下使用 docker 的过程 Docker在windows下的使用【一】]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hugo使用备忘录]]></title>
    <url>%2F2017%2F11%2F05%2Fhugo%E4%BD%BF%E7%94%A8%E5%A4%87%E5%BF%98%E5%BD%95%2F</url>
    <content type="text"><![CDATA[最近在github上发现了除了hexo外的另一个静态网页神器：hugo，这里就简单记录下使用的一些记录。 这里抄一下hugo官方文档的介绍。 Hugo是由Go语言实现的静态网站生成器。简单、易用、高效、易扩展、快速部署。 官方文档地址 这里多说几句Go语言。该语言貌似最近挺火热，很多人看好的一门新语言。以后有空闲可以考虑看看这个语言，不得不说最近在考虑使用docker的问题（配置环境十分方便），突然发现这东西好像也是Go写的。 进入正题，使用hugo其实没那么麻烦，因为可以用编译好的exe文件（windows系统下）。具体的安装教程见如下： Install hugo 翻译一下：第一步，从Hugo Release页面下载hugo的压缩包。第二步，在你喜欢的地方创建一个Hugo和bin的文件夹：’C:\Hugo\bin\’。第三步，将下载的压缩包放到刚刚建立的文件夹里面。第四步，解压压缩包文件，提取hugo.exe到’C:\Hugo\bin\’里。第五步，在你的windows环境变量中加入Hugo exe路径。 顺带一提，最近用hugo的原因是，刚刚发现的一个开源hugo的学术主页框架项目。 项目地址 接着就开始建立站点了。 在你的Hugo exe所在文件夹下打开cmd（按住shift右击，可以在当前文件夹打开cmd）。 hugo new [创建站点文件夹路径名字] 这样就算创建好了。接下来就只需要拷贝主题即可，用git clone，这里需要开启Git bash。 拷贝主题之后，针对配置文件进行修改。主要修改的是content里的一些markdown的文档。其实是各个主页。而其他配置则通过根目录的config.toml，熟悉hexo的同学上手速度应该快一些，跟.yml文件类似。 接下来记录一些常用命令。 本地预览命令。 hugo server 访问端口，localhost:1313。 生成public文件夹，baseUrl填部署的仓库地址，这里用的github部署。 hugo --theme=academic --baseUrl=&quot;https://xxxx&quot; 切换到public文件夹，并push到远程仓库。 cd public git init git remote add origin https://github.com/xxxx/xxx.git(从仓库的clone那里复制) git add -A git commit -m &quot;first commit&quot; git push -u origin master 之后的更新就只需要后面三句命令就可以了。]]></content>
      <categories>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>hugo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客优化相关内容]]></title>
    <url>%2F2017%2F10%2F31%2Fhexo%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[记录最近hexo博客优化的一些网址。我的博客 增加了RSS订阅，支付宝打赏，建站时间，博客版权许可，动画效果。这里参照的主要是NexT官网文档。 NexT官网文档 另外支付宝打赏功能似乎有些bug，参照下面的帖子做了修改。 hexo的next主题打赏 接着后面增加了评论功能（就在多说倒闭后不久）。使用的是Gitment。 感谢大神。 Gitment：使用 GitHub Issues 搭建评论系统Hexo博客框架下Gitment取代多说评论 效果： 然而不满足的我顺带增加了留言板（hhh），并在留言板上加上了音乐播放器。 参考博客： 给Hexo添加留言板 知乎提问·给Hexo添加留言板？ Hexo博客中插入音乐 hexo添加音乐、high一下及一些坑 怎样用七牛空间获得音乐外链 效果： 顺带一提，挖到了篇好文章。 hexo的next主题个性化教程：打造炫酷网站 现在又增加了字数统计、访客统计、Fork me on github等（参照的是上文和官方文档）。 效果如下： 还有添加日历的功能。但是似乎没见到用在NexT上的。先存下地址，后面有空研究一下。 Hexo主题中添加日历云功能 Hexo日历插件 接下来多实现一个功能，在留言板中增加一个访客地理分布功能。 基于以下的网址： revolvermaps 只需复制代码，放在markdown文档中就OK。 效果： 不过后面发现有些问题在本地预览能够显示，但是在github上部署完后，网页无法显示。目前就先去掉了。等后期再研究。]]></content>
      <categories>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017第十五届esri用户大会观感（二）——大会整体]]></title>
    <url>%2F2017%2F10%2F31%2F2017%E7%AC%AC%E5%8D%81%E4%BA%94%E5%B1%8Aesri%E7%94%A8%E6%88%B7%E5%A4%A7%E4%BC%9A%E8%A7%82%E6%84%9F%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E5%A4%A7%E4%BC%9A%E6%95%B4%E4%BD%93%2F</url>
    <content type="text"><![CDATA[上文初步介绍了大会的盛况以及笔者负责的Web App Builder for ArcGIS，本篇来讲讲大会整体观感。总体来说，我觉得本次大会的两大亮点可能不在产品技术本身。而是此次宣布的两大关于产品销售的消息。第一是ArcGIS个人版使用许可推出（960元/年，详情请点击），第二是扶持初创GIS企业的计划（该计划主要面向经营线下或线上的软件产品和服务，员工不超过50人，年营业额不超过一千万人民币的初创企业，提供ArcGIS软件三年的免费使用权）。当然技术上也有些亮点，我会就我的认识来谈一谈。 先谈谈两大亮点，而后聊些具体的技术和应用上看到的。 1 ArcGIS个人版使用许可先来看看960元/年提供了啥。 ArcGIS Pro，ArcMap，ArcCatalog的桌面高级版许可+桌面版中包括的所有高级分析功能拓展模块。直到工作流管理模块来说，已经涵盖了ArcGIS系列桌面的所有软件和功能，非开发人员需要用的ArcGIS桌面已经全部包括，另外传统的ArcGIS系列只留了ArcMap和ArcCatalog，某种程度来说，esri公司确实在推动ArcGIS Pro替代传统桌面。当然我个人觉得更关键的应该是”ArcGIS Online Level 2 Named User account and 100 Service Credits”。 不知大家有没有注意到前文提到的关于Web App Builder for ArcGIS许可方面的内容，明确要求Level 2成员。ArcGIS Online Level 2成员账户事实上具备了可以使用esri多个app的许可。同时应当拥有一个主页，也就是可以当做公有portal的url地址。 产品的简介如下。事实上ArcGIS Desktop大家还是比较熟悉的，Pro则是新一代桌面，而且是为Web GIS平台打造的桌面应用，它本身可以直连portal，同时又具备多种Web GIS相关关联的功能，二三维一体化可以去参考我前面的博客。其他新特性就暂且不提了。ArcGIS Online是公有云平台，事实上就是公有的和在线版本的portal。 附赠的还有一堆视频和文档。 总的来说，价格相当超值，而且符合esri公司在云计算时代的产品定位规划。 2 关于初创企业支持计划这个，毕竟笔者非企业人士就不瞎逼逼了，个人感觉还是蛮超值的，不过跟当年esri刚进中国在打通高校是一个道理。esri如今能在全世界包括中国占据这么大地理信息市场份额一大原因是因为他们在进入市场时送给各大高校那些软件造就的。现在相信对企业来说也是差不多的。 3 所见所闻由于企业主办的会议，比普通学术会议来说，我个人感觉是见到更多的落地项目，也见到了更多接地气的应用，当然逼格依旧在。印象比较深的包括今年获评esri用户最佳应用奖之一的“智慧宁波时空信息云平台”。当然原因主要还是笔者最近有些宁波这块研究的任务。基于portal for ArcGIS搭建的云平台，在辅助智慧城市建设方面，可以说走出了坚定的一步。ps，宁波市规划局没记错的话，似乎和百度慧眼也有合作，应该算是国内规划信息化走得很前端。 还有是江苏省的互联网+国土资源服务平台。 接下来聊一聊web平台中的另外两架马车（ArcGIS API for Javascript和ArcGIS API for Python）。JS API演示demo十分丰富，最主要是基于4.5版本的API，听到了两个比较大的亮点。 一个是点云数据支持渲染（在线地址）。 激光雷达和倾斜摄影的技术使得三维建模越来越快速。而JS 4.5 API是esri最主要做三维web的版本。 另外一个应该是号称可以渲染大数据的JS API，不过后面据闻需要ArcGIS Enterprise 10.6版本发布的服务支持（在线地址）。 ArcGIS API for Python的话，这次看到的是基于scikit-learning和esri存储的landsat数据实现遥感分类的相关样例。关于landsat存储的可以见虾神博客。此外值得推荐的是另外一个web appLandsat Explorer web app，来源于2017 esri 全球用户大会。 很多demo可以在体验中心查看。 另外ArcGIS runtime SDK现在可以实现.Net跨平台开发，以及三维支持。当然还有Geoevent，GeoAnalytics Server，矢量切片，无人机等多种技术。 很凑巧，今天esri官方放出了主题演示的一些视频。详情请点击 记录些观感。欢迎交流。]]></content>
      <categories>
        <category>GIS</category>
      </categories>
      <tags>
        <tag>ArcGIS</tag>
        <tag>ArcGIS Pro</tag>
        <tag>ArcGIS Online</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017第十五届esri用户大会观感（一）——WAB敏捷开发]]></title>
    <url>%2F2017%2F10%2F30%2F2017%E7%AC%AC%E5%8D%81%E4%BA%94%E5%B1%8Aesri%E7%94%A8%E6%88%B7%E5%A4%A7%E4%BC%9A%E8%A7%82%E6%84%9F%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94WAB%E6%95%8F%E6%8D%B7%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[2017年10月24日和25日，在北京国际会议中心召开了第十五届esri用户大会。本次会议的主题围绕着slogan的理念提出的”Applying The Science of Where”，一共有6大主题报告，6大技术论坛，10大行业论坛，业界称为GIS届的饕餮盛宴。从Enabling a Smarter World到The Science of Where,大概是走过风风雨雨，看过世界万物最终又返璞归真回到原点去回答，我是谁？我从哪里来？我到哪里去？ 空间贯穿着我们的一生，从子宫到坟墓，人依托着空间而存在，而地理与GIS正是用来理解我们所生存的地球空间的语言。 笔者很荣幸地成为了本届esri用户大会体验区志愿者之一，所负责的产品是esri推出的0代码敏捷开发的Web App Builder for ArcGIS。所以从观感来讲，就先来介绍下Web App Builder for ArcGIS产品吧。 1 Web App Builder for ArcGIS简介ArcGIS WebApp Builder 是Esri在2014年4月份推出的一种直观的所见即所得式 (WYSIWYG) 应用程序，可用于构建 2D 和 3D web 应用程序，而无需编写一行代码。它所包括的强大工具可用来配置功能完备的 HTML 应用程序。添加地图和工具时，您可以在应用程序中看到这些地图和工具并立即使用。 主要功能可通过 Web AppBuilder for ArcGIS 进行以下操作： 创建能在所有设备上运行的 HTML/JavaScript 应用程序。 使用即用型微件构建所需应用程序。 使用可配置的主题自定义应用程序的外观。 在线托管应用程序或在自己的服务器上运行应用程序。 创建自定义应用程序模板。 详情请点击 当然这里有许可相关限制。 2 Web App Builder for ArcGIS在线版敏捷开发正如前面所说，对于许可的限制，事实上目前Web App Builder for ArcGIS必须有ArcGIS Online或者是Portal的支持。而简单地说Web App Builder for ArcGIS有两种版本，一种是在线版本，内置在ArcGIS Online或者Portal中，只需有账户就可以快速搭建，另一种则是独立的开发者版本，需要部署。那我们就先来介绍第一种方式。 第一种方式对于许可有限制，事实上作为试用和测试的话，我们只需要注册一个账户，试用ArcGIS即可享受21天的ArcGIS Online组织2级成员的许可，当然这个也支持ArcGIS Pro等。 至于如何申请请点击：ArcGIS及Online使用许可申请。 接下来进入正题。进行快速的敏捷开发。这是笔者试用账户下托管的数据。我利用ArcGIS Pro发布了一份2016年1月1日到1月7日全国各个县级市空气质量的数据。并做了一张简易热力图。 接着点击“创建”→“使用Web App Builder” 设置好标题、标签和摘要即可创建（也可以选择3D，这里就选择2D）。 这里可以选择任意一个主题。也可以设置颜色以及布局 接着点击”地图”，选择所需要的数据和底图。这里选择了做好的”Urban Air”。此外还可以选择地图初始化的范围等。 接下来就可以点击”微件”，配置webgis地图的基础功能了。这里最新版提供了超过33个微件，满足webgis地图应用基本需求。 该主题里画框的”微件”分别对应的位置如上。 微件列表。 微件内部配置 最终页面。 最后还可以修改下”属性”，即可。 接下来可以选择保存，或者预览，或者启动。保存即保存在自己的账户项目内，预览可以观察不同尺寸的效果（web app是自适应的html5），启动的话直接访问该应用。 保存完毕即可查看。 也可以下载下来进行部署。下图为访问的页面。 如上是在线版本的敏捷开发。 3 Web App Builder for ArcGIS开发者版本部署前文提到，Web App Builder for ArcGIS有独立的开发者版本，可以下载下来自己部署。不过依旧需要有ArcGIS Online或者portal账户来支撑。 Web App Builder for ArcGIS开发者版本 开发者版本下载下来的文件夹结构。 只需要点击startup.bat运行，即可启动。cmd运行后，在浏览器中输入http://localhost:3345或https://localhsot:3346访问。 与在线版不同的就是，接着就需要你填入ArcGIS Online或者portal的组织url了。同时还需要有个应用ID。 试用账户组织的url可以直接点击组织查询（这个为自己申请试用时设置的）。 所以这里需要先从ArcGIS Online先行创建app，传递ID。首先点击”添加项目”→”应用程序”。 选择”Web制图”→”添加”。笔者测试过，这里必须设置为https://localhost:3346/webappbuilder。 点击”设置”→”注册” 必须做个重定向，将回调地址设为本地（除了localhost还可以设置为计算机名），注册。 即可看到ID。 运行startup.bat，访问https://localhost:3346。 接下来步骤与前面在线版本差异不大。当然也有一个公用的可以使用的portal: http://www.arcgisonline.cn/portal。也可以通过这个进行配置。相关教程在第一篇参考博客中。 整体来说，在用户大会体验区开放的接近一天半的时间内，还是有大量的人对零代码构建web app的工具产生了浓厚兴趣，整体来说大家都在询问的无非是：如何部署在自己的服务器上以及如何进行二次开发，而不仅仅是基于现有的框架。 解答如下：部署在自己服务器上的话必须购买portal for ArcGIS。而由于该工具是以敏捷开发为目的，封装得较好，所以二次开发支持的无非是主题配置和微件使用（具体可见文末的几篇博客）。当然也有大神对源码进行了改动，使其可以不依赖于portal for ArcGIS，这里就不提了（因为某种程度上失去了敏捷开发的特性，当然改完之后的倒是很完美）。 WAB敏捷开发目前来看已经是portal for ArcGIS以及新一代ArcGIS平台web开发的三架马车（另两架分别是ArcGIS API for Python，ArcGIS API for Javascript）了，当然个人理解，仅从我们体验区布设来看。 WAB的敏捷开发实现了一个快速开发的途径，某种程度上也降低了无代码人员开发的难度，本身也具有不错的平台扩展性，演示的demo中有集成了百度Echarts的可视化，当然基于portal for ArcGIS或者ArcGIS Online的这个特性确实在彰显esri公司在云计算时代的应对之策。 最后的最后，我个人觉得该产品可以类比Tableau Public和power BI。当然相对而言，WAB确实更偏向于GIS，而后两者则纯粹考虑可视化。 相关教程学习资料与博客: Web Appbuilder For ArcGIS# 正式版使用教程 配置与使用 webAppbuilder微件使用教程1 快速入门 webAppbuilder微件使用教程2 常用微件介绍 webAppbuilder微件使用教程3 地理处理微件 Webappbuilder开发快速预览 Webappbuilder自定义widget模板 WebAppBuilder自定义主题 ArcGIS WebApp builder 教程（一）简介 ArcGIS WebApp builder 教程（二）入门 ArcGIS Web Appbuilder代码改动为不需要protal步骤初探 Web App Builder For ArcGIS 安装部署使用 ArcGIS WebApp Builder 使用指南]]></content>
      <categories>
        <category>GIS</category>
      </categories>
      <tags>
        <tag>ArcGIS</tag>
        <tag>Web App Builder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WRFDA与WRFDA-4DVAR的编译安装]]></title>
    <url>%2F2017%2F10%2F30%2FWRFDA%E4%B8%8EWRFDA-4DVAR%E7%9A%84%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[前面介绍过WRFDA的安装教程了。这次主要是跟着前面的WRF安装程序再次安装下来。参考文档 1 WRFDA（运行三维变分）编译安装事实上在安装完WRF后，再安装WRFDA应该是比较简单的。这里遇到了一个问题。就是hdf5库的安装。按教程似乎必须安装hdf5库。但是前文安装WRF的时候，netcdf安装并没有基于hdf5库编译安装。所以现在再安装的话，也无法编译成功WRFDA(我尝试了挺多次，前面在WRF编译完成后，再安装hdf5，再编译无法成功)，这一点可以在网上找一些教程，网上大多教程都是先安装hdf5和zlib然后再装的netcdf。这一方面我也发了邮件向官方求助，目前还没收到回复，但是先按照如上的安装程序走下来看看。 在编译安装完WRF后，其实WRFDA只需要再设置NETCDF（在不需要hdf5安装的前提下）的环境变量就可以安装，当然需要做辐射传输模型同化的，则可以考虑，WRFDA自带的是CRTM，如果需要用RTTOV的需要在编译前安装并且设置环境变量（我前面的安装教程已经交代过设置环境变量的内容，RTTOV)。这里就不安装了，直接下载WRFDA的源码编译安装。 事实上前文安装WRF的时候已经设置过NETCDF的环境变量，此外前文设置的环境变量也请一一设置。所以这里所需的操作如下。 gunzip WRFDA_V3.9.1.tar.gz tar -xf WRFDA_V3.9.1.tar.gz ./configure wrfda 确实显示了hdf5没有设置环境变量。所以编译出来应该是无法使用这个数据格式同化的。这里依旧选择34。接下来可以开始编译。 ./compile all_wrfvar &gt;&amp; compile.out 接着等待编译完成。用如下的命令查看生成的exe。 ls -l var/build/*exe var/obsproc/src/obsproc.exe 2 WRFDA-4DVAR编译安装WRFDA-4DVAR就是运行四维变分程序同化的模块。至于这个的要求也在之前写WRFDA安装的文章里有介绍。可以翻到前面的博客查看。要安装WRFDA-4DVAR，必须先安装WRFPLUS。 gunzip WRFPLUSV3.9.1.tar.gz tar -xf WRFPLUSV3.9.1.tar cd WRFPLUSV3 ./configure wrfplus 用下面的命令查询生成的exe。 ls -ls main/*.exe 接着设置环境变量。 export WRFPLUS_DIR=/home/Build_WRF/WRFPLUSV3 生成编译的文件。 ./configure 4dvar 开始编译 ./compile all_wrfvar &gt;&amp; compile.out ls -ls var/build/*.exe var/obsproc/*.exe 编译出现跟上面3DVAR相同的44个exe即成功。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>WRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WRF主程序与WPS的编译与安装]]></title>
    <url>%2F2017%2F10%2F13%2FWRF%E4%B8%BB%E7%A8%8B%E5%BA%8F%E4%B8%8EWPS%E7%9A%84%E7%BC%96%E8%AF%91%E4%B8%8E%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[最近重新把WRF学习提上事宜，所以开始重新鼓捣WRF。由于WRF-DA的运行需要依赖WPS程序，这里就填下之前两篇WRF-DA模块编译中挖的坑。即WRF主程序的编译与安装。如果不太清楚的WRF与WRF-DA相关的可以去翻下前两篇博客。 博客地址 最近本来准备换VitualBox来玩虚拟机，结果不小心玩脱了，把之前编译安装好WRF-DA模块的Ubuntu系统删除了，所以只好重头再来了。所以这回我是从WRF主程序等一一安装完，最后再来安装WRF-DA模块。不过这一次按照网上原来的教程装了好多次都失败了，不得已之下，我去请教了WRF官方邮箱。他们给我提供了一份官方安装教程，结果一次成功。这里介绍下这份教程的过程。 WRF官方在线安装教程 1 系统环境测试首先对编译需要的gfortran,cpp,gcc检查是否安装，版本是否匹配。 which gfortran which cpp which gcc gfortran --version gcc --version g++ --version 能显示路径说明已安装，版本检查也未出现。可以发现gfortran并未安装。 apt install gfortran 接下来在安装WRF的文件夹下创建两个文件夹。一个是Build_WRF，一个是TESTS。然后下载Fortran and C Tests Tar File文件，并放入TESTS文件夹下，对编译器做测试。解压完毕。 一共有7个测试。首先是对Fortran和C的编译器做测试。 gfortran TEST_1_fortran_only_fixed.f ./a.out gfortran TEST_2_fortran_only_free.f90 ./a.out gcc TEST_3_c_only.c ./a.out gcc -c -m64 TEST_4_fortran+c_c.c gfortran -c -m64 TEST_4_fortran+c_f.f90 gfortran -m64 TEST_4_fortran+c_f.o TEST_4_fortran+c_c.o ./a.out 接下来测试下csh，perl，sh是否可行。 ./TEST_csh.csh ./TEST_perl.pl ./TEST_sh.sh 可以发现csh测试不通过。解决方案为安装tcsh。 apt install tcsh 安装完，测试通过。 2 安装依赖库首先在Build_WRF文件夹下面创建一个LIBRARIES的文件夹。然后下载所需的依赖库。 mpich-3.0.4netcdf-4.1.3Jasper-1.900.1libpng-1.2.50zlib-1.2.7 把这些压缩包全部放到LIBRARIES下面。 接着设置环境变量开始安装。 1 netcdf安装这里用的是4.1.3版本的netcdf，这个版本还没有把netcdf-fortran和netcdf-c拆开。比较新的版本已经把二者拆开了，新版本则必须两个都安装。 exprot DIR=/home/Build_WRF/LIBRARIES export CC=gcc export CXX=g++ export FC=gfortran export FCFLAGS=-m64 export F77=gfortran export FFLAGS=-m64 tar zxvf netcdf-4.1.3.tar.gz cd netcdf-4.1.3 ./configure --prefix=$DIR/netcdf --disable-dap \ --disable-netcdf-4 --disable-shared make make install export PATH=$DIR/netcdf/bin:$PATH export NETCDF=$DIR/netcdf 虽然网上有很多教程要求先安装zllib和hdf5后安装netcdf，但是我决定按官方教程走走看。 2 mpich安装如果不需要并行运算，可以不安装这个库。这里还是安装一下。 tar xzvf mpich-3.0.4.tar.gz cd mpich-3.0.4 ./configure --prefix=$DIR/mpich make make install export PATH=$DIR/mpich/bin:$PATH 3 zlib安装export LDFLAGS=-L$DIR/grib2/lib export CPPFLAGS=-I$DIR/grib2/include tar xzvf zlib-1.2.7.tar.gz cd zlib-1.2.7 ./configure --prefix=$DIR/grib2 make make install 4 libpng安装tar xzvf libpng-1.2.50.tar.gz cd libpng-1.2.50 ./configure --prefix=$DIR/grib2 make make install 5 jasper安装tar xzvf jasper-1.900.1.tar.gz cd jasper-1.900.1 ./configure --prefix=$DIR/grib2 make make install 3 依赖库兼容性测试接下来对安装完的依赖库兼容性做测试。测试文件 1 Fortran+C+NetCDFtar -xf Fortran_C_NETCDF_MPI_tests.tar cp ${NETCDF}/include/netcdf.inc . gfortran -c 01_fortran+c+netcdf_f.f gcc -c 01_fortran+c+netcdf_c.c gfortran 01_fortran+c+netcdf_f.o 01_fortran+c+netcdf_c.o \ -L${NETCDF}/lib -lnetcdff -lnetcdf ./a.out 2 Fortran+C+NetCDF+MPIcp ${NETCDF}/include/netcdf.inc . mpif90 -c 02_fortran+c+netcdf+mpi_f.f mpicc -c 02_fortran+c+netcdf+mpi_c.c mpif90 02_fortran+c+netcdf+mpi_f.o \ 02_fortran+c+netcdf+mpi_c.o \ -L${NETCDF}/lib -lnetcdff -lnetcdf mpirun ./a.out 4 编译WRF下载WRF的源码，放在Build_WRF里面。WRF3.9.1 gunzip WRFV3.9.1.1.TAR.gz tar -xf WRFV3.9.1.1.TAR cd WRFV3 ./configure 需要安装m4。 apt-get install m4 接着configure一下，出现如下界面。 选择34和1。 ./compile em_real &gt;&amp; log.compile 接下来只要等待编译完成了。用下面的语句检查是否生成exe。 ls -ls main/*.exe 5 编译WPS接下来就是编译WPS。 gunzip WPSV3.9.1.TAR.gz tar -xf WPSV3.9.1.TAR cd WPS ./clean export JASPERLIB=$DIR/grib2/lib export JASPERINC=$DIR/grib2/include ./configure 出现如下界面 选择3，然后运行如下命令编译。 WRF_DIR = ../WRFV3 ./compile &gt;&amp; log.compile 上述的exe出现且不为红色可以初步认为编译安装成功。如何运行WRF后面再介绍。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>WRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware Workstation下安装Ubuntu 64位系统]]></title>
    <url>%2F2017%2F10%2F11%2FVMware%20Workstation%E4%B8%8B%E5%AE%89%E8%A3%85Ubuntu%2064%E4%BD%8D%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[虽然笔者开始接触Linux有一段时间了，但是安装Ubuntu 64位系统并自己分区貌似也是才第一次。这里记录下安装过程。首先打开VMware Workstation。点击文件。 选择自定义。 这里直接点下一步。 选择稍后安装操作系统。 选择Linux，Ubuntu64位。并命名。因为这个虚拟机需要运行WRF之类的程序，选择了双核，当然目的也是实验看看。内存也调大了一些，4G。 网络设置默认，直接下一步。 如图，下一步。 虚拟磁盘类型也是默认，下一步。 创建新磁盘，下一步。 设置80G空间，并存储为单个文件。下一步。 OK，完毕。 存储位置自选。 接着右击设置。在CD/DVD里设置找到提前下载好的iso系统镜像。 接着启动虚拟机就开始安装程序了。 这个不勾选，继续。 点击”Something else”，自己分区，如果选择上面的选项则是推荐分区（看个人需求）。 我的分区如下。 各个分区的意思呢，可以查看这篇博客。安装Ubuntu Linux系统时硬盘分区最合理的方法 简单说至少需要四个分区（下面的第四个看需求）。 目录：/；大小：10G-20G；格式：ext4，描述：根目录。 目录：swap；大小：&lt;2048M；格式：swap；描述：交换空间。 目录：/boot；大小：200M左右；格式：ext4； 描述：Linux的内核及引导系统程序所需要的文件，比如 vmlinuz initrd.img文件都位于这个目录中。在一般情况下，GRUB或LILO系统引导管理器也位于这个目录；启动撞在文件存放位置，如kernels，initrd，grub。 目录：/tmp；大小：5G左右；格式：ext4；描述：系统的临时文件，一般系统重启不会被保存。（建立服务器需要？）。 目录：/home；大小：尽量大些；格式：ext4；描述：用户工作目录；个人配置文件，如个人环境变量等；所有账号分配一个工作目录。分完之后，接下来基本是一路默认了。 这里设置中文。 设置超级用户和登录密码。 正式开始安装。 安装完毕重启。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Vmware Workstation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用统计学与R语言实现学习笔记后记]]></title>
    <url>%2F2017%2F10%2F10%2F%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8ER%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%90%8E%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1 后记应用统计学与R语言实现学习笔记这一系列博客断断续续写了5个月左右。现在终于算是基本完成了。我个人比较强迫症，比较喜欢一个系列更完再更其他的。所以中间有一些不错的内容想写到博客里都没动笔。后面会继续填坑。另外之后遇到的跟应用统计学与R语言实现相关的内容会以番外篇形式发布。 当时想到写这个东西，主要是自己选了门应用统计学的公选课，个人觉得不能浪费了这门课，而且其实我们在做一些研究的时候，其实都用了很多新的、高大上的所谓的新方法，并且不断在追逐所谓的Big data，但是回过头来想想，最基础的统计学理论可能才是我们需要补课的地方（不得不说这门课挺对我胃口，去年暑假花了一部分时间在啃贾俊平的统计学，刚好是这门课的参考教材）。这个年代，用个tensorflow的包，import一下，训练个模型出来就能说自己做的是深度学习。个人意见，也对也不对。IT技术飞速发展，大大降低了程序猿的门槛，但是现在的情况更应当说是程序猿的行当易学难精了。扯得有点远，总之我认为返璞归真地去学一学高数、概率论、统计学、线性代数可能比一上来就开始各种机器学习什么的要强得多。 这份笔记的定位，就是一份笔记，某些程度上就是课程老师给我们的ppt，我对理论部分做了整理。所以要归功于我的任课老师王老师。我不求大家从头到尾看完这份笔记，因为理论很枯燥，但是当需要用些什么内容的时候，可以想起这份笔记，供大家查找和参考。我的笔记并不像《深入浅出统计学》那样直白而又易懂的语言，尽管中间有一定的尝试，所以不可能看完我的这个系列博客就能对统计学的基本内容完全融会贯通，如果你希望在统计学上有所建树，需要大家自己去补课。另外我这部分更多针对于应用，而且基于我自己本身地学背景，我讲的例子也都跟尽量跟地学、生态相关。所以其他专业的同学会觉得一些例子苦涩难懂是比较正常的（在此向其他专业同学说声不好意思，你们的批评我虚心接受，但是你们这方面的建议我坚决不改，傲娇脸）。 好，讲了这么多。这个系列我其实是作为我自己的一个开源项目做的，我希望大家有什么意见可以一起来帮忙修改完善这个项目。如果你觉得还不错，也不要吝啬你的star。我博客里提到的很多代码之类的也都在这个项目里面开源了。就请大家批评指正。 Note-of-Applied-Statistics-with-R 2 基于gitbook的电子书生成教程 Modern book format and toolchain using Git and Markdown 这是 gitbook 项目主页上对 gitbook 的定义。gitbook 首先是一个软件，正如上面定义的那样，它使用 Git 和 Markdown 来编排书本。 gitbook官网 也可以使用gitbook editor。gitbook可以与github关联，直接将仓库的markdown文档发布成电子书。为了方便管理，选择在github上搭建电子书整体内容，然后push到github上，同时同步到gitbook中。首先用github登录gitbook。接着在github上创建一个新的仓库。只保留markdown文档和文件夹。gitbook的关键是需要SUMMARY的markdown文档，这个文件是用来组织书的框架。如下图。 README的前言其实就是上面的后记，想说的话大体相同。只不过时间先后问题导致成了前言和后记。其他的是链接各章节。接着看一下github上仓库项目结构（初步构成）。 因为在线渲染电子书速度较慢，我们可以在本地进行渲染和修改。目前只需将仓库先克隆到本地。接着安装gitbook（基于node.js）。因为gitbook是基于node.js的，先查看是否安装了node.js。 node -v npm -v 接着输入命令，安装gitbook。 npm install gitbook-cli -g 接下来在github上先安装gitbook的拓展应用。并选择应用的仓库范围（可以选择所有仓库，也可以只选择对应的仓库） 这样在gitbook上创建新书的时候，可以选择github的模板，直接导入书籍的仓库，并且后面可以自动同步。 在gitbook的setting中设置，默认生成pdf，mobi，epub的电子书供下载，选择MIT许可证。 gitbook可以通过book.json这个文件来控制生成电子书的一些具体定制化的需求。我的设置如下，因为用到了流程图和大量数学公式，就多加了katex和mermaid的插件。 首先通过命令行，定位到克隆到本地书籍的路径。 gitbook install 先安装插件。接着渲染一下。 gitbook build 最后本地服务器运行。 gitbook serve 在浏览器网址输入localhost:4000。即可查看。 执行gitbook build的时候可能会有各种报错，根据报错信息一个一个修改。目前发现似乎gitbook不太支持mathjax。而且公式里面不能有中文及中文标点符号，而且原来在博客上，两个$是表示数学符号，但是不是自己占一行。四个 $是表示独立的公式，必须另起一行。但是katex只认四个$。所以进行了一番较多修改，流程图目前也一直无法显示。mermaid跟我博客的流程图插件也不相同。我用的是flowchart，但是安装了似乎也不显示。最后就先用截图来表示了。全部搞定后直接push到github上。 发现gitbook上没有完全同步。可以从setting里面设置。 OK，大工告成，接下来只需等它在线渲染成功就可以了。PDF版本。 每每看到封面的熊本分分钟出戏。。。 在线网址访问网址：应用统计学与R语言实现学习笔记 有兴趣的同学可以下载这本电子书，也可以在评论留邮箱，可以发送给大家。 参考博客： 使用GitBook平台发布电子书 【Gitbook】实用配置及插件介绍 GitBook使用教程 GitBook 制作 Kindle 电子书详细教程（可视化版）]]></content>
      <categories>
        <category>Statistics &amp; Model</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用统计学与R语言实现学习笔记（十四）——案例与实践]]></title>
    <url>%2F2017%2F10%2F08%2F%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8ER%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[Chapter 14 Case and Practice本篇是第十四章，内容是案例与实践。这里其实是对我公选课的作业做了个汇总。 1 描述性统计与抽样分布1.一种袋装食品用生产线自动装填，每袋重量大约为50g，但由于某些原因，每袋重量不会恰好是50g。下面是随机抽取的100袋食品，测得的重量数据见附录。（1）构建这些数据的频数分布表。（2）绘制频数分布的直方图。（3）说明数据分布的特征。 2.甲乙两个班各有40名学生，期末统计学考试成绩的分布见附录。（1）根据上面的数据，画出两个班考试成绩的复合柱形图、环形图和图饼图。（2）比较两个班考试成绩分布的特点。（3）画出雷达图，比较两个班考试成绩的分布是否相似。 3.随机抽取25个网络用户，得到他们的年龄数据（单位：周岁）见附录。（1）计算众数、中位数。（2）根据定义公式计算四分位数。（3）计算平均数和标准差。（4）计算偏态系数和峰态系数。（5）对网民年龄的分布特征进行综合分析。 4.某银行为缩短顾客到银行办理业务等待的时间，准备采用两种排队方式进行试验：一种是所有顾客都进入一个等待队列；另一种是顾客在三个业务窗口处列队三排等待。为比较哪种排队方式使顾客等待的时间更短，两种排队方式各随机抽取的9名顾客，得到第一中排队方式的平均等待时间为7.2分钟，标准差为，1.97分钟，第二种排队方式的等待时间（单位：min）见附录。（１）画出第二种排队方式等待时间的茎叶图。（２）计算第二种排队方式等待时间的平均数和标准差。（３）比较两种排队方式等待时间的离散程度。（４）如果让你选择一种排队方式，你会选择哪一种？试说明理由。 5.从均值为200、标准差为50的总体中，抽取n=100的简单随机样本，用样本均值`x估计总体均值。a)描述重复抽样的样本均值的抽样分布。b)不重复抽样，总体单位数分别为10000、1000时的样本均值的抽样分布。 2 参数估计与假设检验1.某大学为了解学生每天上网的时间，在全校7500名学生中采取不重复抽样方法随机抽取36人，调查他们每天上网的时间（单位：小时） ，得到的数据见附录。求该校大学生平均上网时间的置信区间，置信概率分别为90%、95%和99%。 2.假定两个总体的标准差分别为：$\sigma_1$=12，$\sigma_2$=15，若要求误差范围不超过5，相应的置信水平为95%，假定$n_1=n_2$，估计两个总体均值之差$m_1-m_2$时所需的样本容量为多大？ 3.经验表明，一个矩形的宽与长之比等于0.618的时候会给人们比较良好的感觉。某工艺品工厂生产的矩形工艺品框架的宽与长要求也按这一比例设计，假定其总体服从正态分布，现随机抽取了20个框架测得比值见附录。在显著性水平 ＝0.05时，能否认为该厂生产的工艺品框架宽与长的平均比例为0.618？。 4.一家大型超市连锁店上个月接到许多消费者投诉某种品牌炸土豆片中60克一袋的那种土豆片的重量不符。店方猜想引起这些投诉的原因是运输过程中沉积在食品袋底部的土豆片碎屑，但为了使顾客们对花钱买到的土豆片感到物有所值，店方仍然决定对来自于一家最大的供应商的下一批袋装炸土豆片的平均重量（克）进行检验，假设陈述如下： $H_0: \mu\le 60$ $H_1:\mu&lt;60$如果有证据可以拒绝原假设，店方就拒收这批炸土豆片并向供应商提出投诉。（1）与这一假设检验问题相关联的第一类错误是什么？（2）与这一假设检验问题相关联的第二类错误是什么？（3）你认为连锁店的顾客们会将哪类错误看得较为严重？而供应商会将哪类错误看得较为严重？ 3 方差分析与回归分析1.某家电制造公司准备购进一批5#电池，现有A、B、C三个电池生产企业愿意供货，为比较它们生产的电池质量，从每个企业各随机抽取5只电池，经试验得其寿命（单位：h）数据见附录。试分析三个企业生产的电池的平均寿命之间有无显著差异（$\alpha=0.05$）。如果有差异，用LSD方法检验哪些企业之间有差异？ 2.一家超市连锁店的老板进行一项研究，确定超市所在的位置和竞争者的数量对销售额是否有显著影响。获得的月销售额数据（单位：万元）见附录。取显著性水平$\alpha=0.01$，检验：（1）竞争者的数量对销售额是否有显著影响。（2）超市的位置对销售额是否有显著影响。（3）竞争者的数量和超市的位置对销售额是否有交互影响。 3.附录中有随机抽取的15家大型商场销售的同类产品的有关数据（单位：元）。（1）计算y与$x_1$ 、y与$x_2$之间的相关系数，是否有证据表明销售价格与购进价格、销售价格与销售费用之间存在线性关系？（2）根据上述结果，你认为用购进价格和销售费用来预测销售价格是否有用？（3）用Excel进行回归，并检验模型的线性关系是否显著（$\alpha=0.05$）。（4）解释判定系数$R^2$，所得结论与问题（2）中是否一致?（5）计算$x_1$与$x_2$之间的相关系数，所得结果意味着什么？（6）模型中是否存在多重共线性？你对模型有何建议？ 4.附录中有32名美士足球运动员的rating及其他相关信息。请建立一个回归模型以预测一位美士足球运动员的rating。提交报告包括：使用什么方法建立的模型，该方法的运行结果，最终模型的解释（拟合程度、预测误差）。 这一份作业汇总从最原始的描述统计、参数估计、假设检验到基础的方差分析与回归分析均有了。根据这里的习题即可对前面的内容再次熟悉。这里就不多说了，我有一份比较完整的文档针对这份内容。这里先给出节选部分的截图。具体地址再给出。]]></content>
      <categories>
        <category>Statistics &amp; Model</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用统计学与R语言实现学习笔记（十三）——因子分析]]></title>
    <url>%2F2017%2F10%2F06%2F%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8ER%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Chapter 13 Factor Analysis本篇是第十三章，内容是因子分析。 1 因子分析概念因子分析是一种数据简化的技术。它通过研究众多变量之间的内部依赖关系，探求观测数据中的基本结构，并用少数几个假想变量来表示其基本的数据结构。这几个假想变量能够反映原来众多变量的主要信息。原始的变量是可观测的显在变量，而假想变量是不可观测的潜在变量，称为因子。即一种用来在众多变量中辨别、分析和归结出变量间的相互关系并用简单的变量（因子）来描述这种关系的数据分析方法。寻求基本结构 通过因子分析，找出几个较少的有实际意义的因子，反映出原来数据的基本结构。 通常找出的这组观察不到的因子概括了原始的变量的大多数信息。 数据简化 强相关问题会对分析带来困难。 通过因子分析，可以用所找出的少数几个因子代替原来的变量做回归分析、聚类分析、判别分析等。 因子分析的用途 产生新的、更少的变量以便为后续的回归和其他分析做基础。 识别概念或产品的基本感知和特性。 改善市场研究领域多元测量的结构与方法。 2 因子分析模型数学模型设$X_i(i=1,2,\cdots,p)$p个变量，如果表示为：$$ X_i=\mu_i+a_{i1}F_1+\cdots+a_{im}F_m+\varepsilon_i(m\le p) $$或$$ \begin {bmatrix} X_1\\X_2\\\vdots\\X_p \end {bmatrix}=\begin {bmatrix} \mu_1\\\mu_2\\\vdots\\\mu_p \end {bmatrix}+\begin {bmatrix} \alpha_{11}&amp;\alpha_{12}&amp;\cdots&amp;\alpha_{1m}\\\alpha_{21}&amp;\alpha_{22}&amp;\cdots&amp;\alpha_{2m}\\\vdots&amp;\vdots&amp;&amp;\vdots\\\alpha_{p1}&amp;\alpha_{p2}&amp;\cdots&amp;\alpha_{pm} \end {bmatrix}\begin {bmatrix} F_1\\F_2\\\vdots\\F_m \end {bmatrix}+\begin{bmatrix} \varepsilon_1\ \varepsilon_2\\\vdots\ \varepsilon_p \end {bmatrix} $$或$$ X-\mu=AF+\varepsilon $$$F_1,F_2,\cdots,F_m$称为公共因子,是不可观测的变量，它们的系数称为因子载荷。$\varepsilon_i$是特殊因子，是不可能被前m个公共因子包含的部分。并且满足：$cov(F,\varepsilon)=0$，即$F,\varepsilon$不相关；$$ D(F)=\begin {bmatrix} 1 &amp; &amp; &amp; \\&amp;1&amp;&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;1 \end {bmatrix}=I $$即$F_1,F_2,\cdots,F_m$互不相关，方差为1。$$ D(\varepsilon)=\begin {bmatrix}\sigma_1^2&amp;&amp;&amp;\\&amp;\sigma_w^2&amp;&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\sigma_p^2\end {bmatrix} $$即$\varepsilon_i\sim N(0,\sigma_i^2)$互不相关，方差不一定相等。用矩阵的方式表达$$ X-\mu=AF+\varepsilon $$ $$ E(F)=0 $$ $$ E(\varepsilon)=0 $$ $$ Var(F)=1 $$$$ cov(F,\varepsilon)=E(F\varepsilon’)=\begin {pmatrix}E(F_1\varepsilon_1)&amp;E(F_1\varepsilon_2)&amp;\cdots&amp;E(F_1\varepsilon_p)\\E(F_2\varepsilon_1)&amp;E(F_2\varepsilon_2)&amp;\cdots&amp;E(F_2\varepsilon_p)\\\vdots&amp;\vdots&amp;&amp;\vdots\\E(F_p\varepsilon_1)&amp;E(F_p\varepsilon_2)&amp;\cdots&amp;E(F_p\varepsilon_p) \end {pmatrix}=0 $$$$ Var(\varepsilon)=diag(\sigma_1^2,\sigma_2^2,\cdots,\sigma_p^2) $$因子分析模型的性质 1、原始变量X的协方差矩阵的分解$$ \Sigma_x=AA’+D $$A是因子模型的系数$$ Var(\varepsilon)=D=diag(\sigma_1^2,\sigma_2^2,\cdots,\sigma_p^2) $$D的主对角线上的元素值越小，则公共因子共享的成分越多。2、模型不受计量单位的影响。3、因子载荷不是惟一的：设T为一个p×p的正交矩阵，令A=AT， F=T’F也是一个满足因子模型条件的因子载荷。 因子载荷矩阵中的统计特征 因子载荷$a_{ij}$是第i个变量与第j个公共因子的相关系数。 变量$X_i$的共同度是因子载荷矩阵的第i行的元素的平方和。记为$h_i^2=\sum_{i=1}^ma_{ij}^2$所有的公共因子和特殊因子对变量$X_i$的贡献为1。如果$\sum_{i=1}^ma_{ij}^2$非常靠近1， $\sigma_i^2$非常小，则因子分析的效果好，从原变量空间到公共因子空间的转化性质好。 因子载荷矩阵中各列元素的平方和$S_j=\sum_{i=1}^pa_{ij}^2$称为$F_j(j=1,2,\cdots,m)$对所有的$X_i$的方差贡献和。衡量$F_j$的相对重要性。 3 因子载荷矩阵的估计方法 主成分分析法设随机向量$x=(x_1,x_2,\cdots,x_p)’$的均值为$\mu$，协方差为$\Sigma$，$\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_p\ge0$为$\Sigma$的特征根，$u_1,u_2,\cdots,u_p$为对应的标准化特征向量，则：$$ \Sigma=U\begin {bmatrix}\lambda_1&amp;&amp;&amp;\\&amp;\lambda_2&amp;&amp;\\&amp;&amp;\cdots&amp;\\&amp;&amp;&amp;\lambda_p \end {bmatrix}U’=AA’+D=\begin {bmatrix}\sqrt\lambda_1u_1&amp;\sqrt\lambda_2u_2&amp;\cdots&amp;\sqrt\lambda_pu_p \end {bmatrix}\begin {bmatrix}\sqrt\lambda_1u_1’\\\sqrt\lambda_2u_2’\\\vdots\\\sqrt\lambda_pu_p’ \end {bmatrix}+D $$上式给出的Σ表达式是精确的，然而，它实际上是毫无价值的，因为我们的目的是寻求用少数几个公共因子解释，故略去后面的p-m项的贡献。上式有一个假定：模型中的特殊因子是不重要的，因而从Σ的分解中忽略了特殊因子的方差。确定因子个数（特征根大于1所对应的特征向量；碎石原则：把特征根从大到小排列，把特征根减小速度变缓的特征根都删掉）。 主因子法主因子方法是对主成分方法的修正，假定我们首先对变量进行标准化变换。则$$ R=AA’+D\\R^{\ast}=AA’=R-D $$称$R^{\ast}$为约相关矩阵，$R^{\ast}$对角线上的元素是$h_i^2$，而不是1。$$ R^{\ast}=R-\hat D=\begin {bmatrix}\hat h_1^2&amp;r_{12}&amp;\cdots&amp;r_{1p}\\r_{21}&amp;\hat h_2^2&amp;\cdots&amp;r_{2p}\\\vdots&amp;\vdots&amp;&amp;\vdots\\r_{p1}&amp;r_{p2}&amp;\cdots&amp;\hat h_p^2 \end {bmatrix} $$直接求$R^{\ast}$的前p个特征根和对应的正交特征向量。得如下的矩阵：$$A=\begin {bmatrix}\sqrt{\lambda_1^{\ast}}u_1^{\ast}&amp;\sqrt{\lambda_2^{\ast}}u_2^{\ast}&amp;\cdots&amp;\sqrt{\lambda_p^{\ast}}u_p^{\ast} \end {bmatrix}$$$R^{\ast}$特征根：$\lambda_1^{\ast}\ge\lambda_2^{\ast}\ge\cdots\ge\lambda_p^{\ast}\ge0$，正交特征向量：$u_1^{\ast},u_2^{\ast},\cdots,u_p^{\ast}$当特殊因子$\varepsilon_i$的方差已知：$$ R^{\ast}=R-\begin {bmatrix}\sigma_1^2&amp;&amp;&amp;\\&amp;\sigma_2^2&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\sigma_p^2 \end {bmatrix}=\begin {bmatrix}\sqrt{\lambda_1^{\ast}}u_1^{\ast}&amp;\sqrt{\lambda_2^{\ast}}u_2^{\ast}&amp;\cdots&amp;\sqrt{\lambda_p^{\ast}}u_p^{\ast} \end {bmatrix}\begin {bmatrix}\sqrt{\lambda_1^{\ast}}u_1^{‘\ast}\\\sqrt{\lambda_2^{\ast}}u_2^{‘\ast}\\\vdots\\\sqrt{\lambda_p^{\ast}}u_p^{‘\ast} \end {bmatrix} $$$$ A=\begin {bmatrix}\sqrt{\lambda_1^{\ast}}u_1^{\ast}&amp;\sqrt{\lambda_2^{\ast}}u_2^{\ast}&amp;\cdots&amp;\sqrt{\lambda_m^{\ast}}u_m^{\ast}\end {bmatrix} $$$$ D=\begin {pmatrix}1-\hat h_1^2&amp;&amp;0\\&amp;\ddots&amp;\\0&amp;&amp;1-\hat h_p^2 \end {pmatrix} $$在实际的应用中，个性方差矩阵一般都是未知的，可以通过一组样本来估计。估计的方法有如下几种：首先，求$h_i^2$的初始估计值，构造出R*1、取$h_i^2=1$，在这个情况下主因子解与主成分解等价；2、取$h_i^2=R_i^2$，$R_i^2$为$x_i$与其他所有的原始变量$x_j$的复相关系数的平方，即$x_i$对其余的p-1个$x_j$的回归方程的判定系数，这是因为xi与公共因子的关系是通过其余的p-1个$x_j$的线性组合联系起来的；3、取$\hat h_i^2=max\left|r_{ij}\right|(j\neq i)$，这意味着取$x_i$与其余的$x_j$的简单相关系数的绝对值最大者；4、取$h_i^2=\frac{1}{p-1}\sum_{j=1,j\neq i}^pr_{ij}$，其中要求该值为正数。5、取$h_i^2=1/r^{ii}$，其中$r^{ii}$是$R^{-1}$的对角元。 极大似然估计法如果假定公共因子F和特殊因子$\varepsilon$服从正态分布，那么可以得到因子载荷和特殊因子方差的极大似然估计。设$x_1,x_2,\cdots,x_n$为来自正态总体$N_p(\mu,\Sigma)$的随机样本。$\Sigma=AA’+\Sigma_{\varepsilon}$$$\begin {aligned} L(\hat \mu,\hat A,\hat D)&amp;=f(X)=f(X_1)\cdot f(X_2)\cdots f(X_n)\\&amp;=\prod_{i=1}^n(2\pi)^{-p/2}\left |\Sigma\right|^{1/2}exp\begin {bmatrix}-\frac{1}{2}(x_i-\mu)’\Sigma^{-1}(x_i-\mu) \end {bmatrix}\\&amp;=\begin {bmatrix}(2\pi)^p\left |\Sigma \right |^{-n/2} \end {bmatrix}exp\begin {bmatrix}-\frac{1}{2}\sum_{i=1}^n(X_i-\mu)’\Sigma^{-1}(X_i-\mu) \end {bmatrix} \end {aligned} $$用数值极大化的方法可以得到极大似然估计。 4 因子旋转（正交变换）旋转因子的目的因子分析的目的不仅仅是要找出公共因子以及对变量进行分组，更重要的是要知道每个公共因子的意义，以便进行进一步的分析。如果每个公共因子的含义不清，则不便于进行实际背景的解释。初始因子的综合性太强，难以找出因子的实际意义。由于因子载荷阵是不唯一的，所以可以对因子载荷阵进行旋转，使因子载荷阵的结构简化，使其每列或行的元素平方值向0和1两极分化。旋转方法设$\Gamma$正交矩阵，做正交变换$B=A\Gamma$。 变换后各变量的共同度不会发生变化。 变换后各因子的贡献会发生变化。 三种主要的正交旋转法 方差最大法方差最大法从简化因子载荷矩阵的每一列出发，使和每个因子有关的载荷的平方的方差最大。当只有少数几个变量在某个因子上有较高的载荷时，对因子的解释最简单。 方差最大的直观意义是希望通过因子旋转后，使每个因子上的载荷尽量拉开距离，一部分的载荷趋于±1，另一部分趋于0。$$ A=\begin {bmatrix} a_{11}&amp;a_{12}\\a_{21}&amp;a_{22}\\\vdots&amp;\vdots\\a_{p1}&amp;a_{p2} \end {bmatrix} $$$$ X_1=a_{11}F_1+a_{12}F_2\\X_2=a_{21}F_1+a_{22}F_2\\\cdots\\X_p=a_{p1}F_1+a_{p2}F_2 $$设旋转矩阵：$ T=\begin {pmatrix}\cos\varphi&amp;-\sin\varphi\\\sin\varphi&amp;\cos\varphi \end {pmatrix} $则$$ \begin {aligned}B&amp;=AT=A\begin {pmatrix}\cos\varphi&amp;-\sin\varphi\\\sin\varphi&amp;\cos\varphi \end {pmatrix}\\&amp;=\begin {pmatrix}a_{11}\cos\varphi+a_{12}\sin\varphi&amp;-a_{11}\sin\varphi+a_{12}\cos\varphi\\\vdots&amp;\vdots\\a_{p1}\cos\varphi+a_{p2}\sin\varphi&amp;-a_{p1}\sin\varphi+a_{p2}\cos\varphi \end {pmatrix}\\&amp;=\begin {pmatrix}a_{11}^{\ast}&amp;a_{12}^{\ast}\\\vdots&amp;\vdots\\a_{p1}^{\ast}&amp;a_{p2}^{\ast} \end {pmatrix} \end {aligned} $$令$ d_{ij}=\frac{a_{ij}^{\ast}}{h_i},i=1,2,\cdots,p;j=1,2 $$ \bar d_j=\frac{1}{p}\sum_{i=1}^pd_{ij}^2 $（这是列和）简化准则为：$ V(\theta)=\sum_{j=1}^m\sum_{i=1}^p(d_{ij}^2-\bar d_j)^2=max $即：$ V_1+V_2+V_3+\cdots+V_m=max $令$ \frac{\partial V}{\partial \theta}=0 $，则可以解出$ \theta_0 $旋转矩阵为：$ T=\begin {pmatrix}\cos\theta_0&amp;-\sin\theta_0\\\sin\theta_0&amp;\cos\theta_0 \end {pmatrix} $ 四次方最大法四次方最大旋转是从简化载荷矩阵的行出发，通过旋转初始因子，使每个变量只在一个因子上有较高的载荷，而在其它的因子上尽可能低的载荷。 如果每个变量只在一个因子上有非零的载荷，这时的因子解释是最简单的。四次方最大法通过使因子载荷矩阵中每一行的因子载荷平方的方差达到最大。简化准则为：$ Q=\sum_{i=1}^p\sum_{j=1}^m(b_{ij}^2-\frac{1}{m})^2=max $$$ \begin {aligned} Q&amp;=\sum_{i=1}^p\sum_{j=1}^m(b_{ij}^2-\frac{1}{m})^2=\sum_{i=1}^p\sum_{j=1}^m(b_{ij}^4-2\frac{1}{m}b_{ij}^2+\frac{1}{m^2})\\&amp;=\sum_{i=1}^p\sum_{j=1}^m(b_{ij}^4-2\sum_{i=1}^p\sum_{j=1}^m\frac{1}{m}b_{ij}^2+\sum_{i=1}^p\sum_{j=1}^m\frac{1}{m^2})\\&amp;=\sum_{i=1}^p\sum_{j=1}^m(b_{ij}^4-2\sum_{i=1}^p\sum_{j=1}^m\frac{1}{m}b_{ij}^2+\sum_{i=1}^p\sum_{j=1}^m\frac{1}{m^2})\\&amp;=\sum_{i=1}^p\sum_{j=1}^m(b_{ij}^4-2+\frac{p}{m}) \end {aligned} $$最终的简化准则为：$ Q=\sum_{i=1}^p\sum_{j=1}^mb_{ij}^4=MAX $ 等量最大法等量最大法把四次方最大法和方差最大法结合起来求Q和V的加权平均最大。最终的简化准则为：$ E=\sum_{i=1}^p\sum_{j=1}^mb_{ij}^4-\gamma\sum_{j=1}^m(\sum_{i=1}^pb_{ij}^2)^2/p=MAX $权数$\gamma$等于$m/2$，因子数有关。 5 因子得分当解决了用一组公共因子的线性组合来表示一组观测变量后，有时我们需要使用这些因子做其他的研究。比如把得到的因子作为自变量来做回归分析，对样本进行分类或评价，这就需要我们对公共因子进行测度，即给出公共因子的值。因子得分因子分析的数学模型：$$ \begin {bmatrix}X_1\\X_2\\\vdots\\X_p \end {bmatrix}=\begin {bmatrix}\alpha_{11}&amp;\alpha_{12}&amp;\cdots&amp;\alpha_{1m}\\\alpha_{21}&amp;\alpha_{22}&amp;\cdots&amp;\alpha_{2m}\\\vdots&amp;\vdots&amp;&amp;\vdots\\\alpha_{p1}&amp;\alpha_{p2}&amp;\cdots&amp;\alpha_{pm} \end {bmatrix}\begin {bmatrix}F_1\\F_2\\\vdots\\F_p \end {bmatrix} $$原变量被表示为公共因子的线性组合，当载荷矩阵旋转之后，公共因子可以做出解释，通常的情况下，我们还想反过来把公共因子表示为原变量的线性组合。因子得分函数：$F_j=\beta_{j1}X_1+\cdots+\beta_{jp}X_p, j=1,\cdots,m$。可见，要求得每个因子的得分，必须求得分函数的系数，而由于p&gt;m，所以不能得到精确的得分，只能通过估计。巴特莱特因子得分(加权最小二乘法）把$x_i-\mu_i$看作因变量；把因子载荷矩阵$$ \begin {bmatrix}\alpha_{11}&amp;\alpha_{12}&amp;\cdots&amp;\alpha_{1m}\\\alpha_{21}&amp;\alpha_{22}&amp;\cdots&amp;\alpha_{2m}\\\vdots&amp;\vdots&amp;&amp;\vdots\\\alpha_{p1}&amp;\alpha_{p2}&amp;\cdots&amp;\alpha_{pm} \end {bmatrix} $$看成自变量的观测；把某个个案的得分$F_j$看作最小二乘法需要求的系数。$$ \begin {cases}x_{i1}-\mu_1=a_{11}f_1+a_{12}f_2+\cdots+a_{1m}f_m+\varepsilon_1\\x_{i2}-\mu_2=a_{21}f_1+a_{22}f_2+\cdots+a_{2m}f_m+\varepsilon_2\\\cdots\\x_{ip}-\mu_p=a_{p1}f_1+a_{p2}f_2+\cdots+a_{pm}f_m+\varepsilon_p \end {cases} $$由于特殊因子的方差相异，所以用加权最小二乘法求得分，每个个案作一次，要求出所有样品的得分，需要作n次。$$ \sum_{j=1}^p[(x_i-\mu_i)-(a_{i1}\hat f_1+a_{i2}\hat f_2+\cdots+a_{im}\hat f_m)]^2/\sigma_i^2 $$使上式最小的$\hat f_1,\cdots,\hat f_m$是相应个案的因子得分。回归方法$$ \begin {bmatrix}X_1\\X_2\\\vdots\\X_n \end {bmatrix}=\begin {bmatrix}\alpha_{11}&amp;\alpha_{12}&amp;\cdots&amp;\alpha_{1m}\\\alpha_{21}&amp;\alpha_{22}&amp;\cdots&amp;\alpha_{2m}\\\vdots&amp;\vdots&amp;&amp;\vdots\\\alpha_{p1}&amp;\alpha_{p2}&amp;\cdots&amp;\alpha_{pm} \end {bmatrix}\begin {bmatrix}F_1\\F_2\\\vdots\\F_p \end {bmatrix}+\begin {bmatrix}\varepsilon_1\\\varepsilon_2\\\vdots\\\varepsilon_n \end {bmatrix} $$$$ \hat F_j=b_{j1}X_1+\cdots+b_{jp}X_p, j=1,\cdots,m $$$$ \begin {bmatrix}b_{11}&amp;b_{12}&amp;\cdots&amp;b_{1p}\\b_{21}&amp;b_{22}&amp;\cdots&amp;b_{2p}\\\vdots&amp;\vdots&amp;&amp;\vdots\\b_{m1}&amp;b_{m2}&amp;\cdots&amp;b_{mp} \end {bmatrix}=\begin {bmatrix}b_1\\b_2\\\vdots\\b_m \end {bmatrix} $$$$ \begin {aligned} \alpha_{ij}=\gamma_{x_iF_j}=E(X_i,F_j)&amp;=E[X_i(b_{j1}X_1+\cdots+b_{jp}X_p)]\\&amp;=b_{j1}\gamma_{i1}+\cdots+b_{jp}\gamma_{ip}=\begin {bmatrix}\gamma_{i1}&amp;\gamma_{i2}&amp;\cdots&amp;\gamma_{ip} \end {bmatrix}\begin {bmatrix}b_{j1}\\b_{j2}\\\vdots\\b_{jp} \end {bmatrix} \end {aligned}$$则，我们有如下的方程组：$$ \begin {bmatrix}\gamma_{11}&amp;\gamma_{12}&amp;\cdots&amp;\gamma_{1p}\\\gamma_{21}&amp;\gamma_{22}&amp;\cdots&amp;\gamma_{2p}\\\vdots&amp;\vdots&amp;&amp;\vdots\\\gamma_{p1}&amp;\gamma_{p2}&amp;\cdots&amp;\gamma_{pp} \end {bmatrix}\begin {bmatrix}b_{j1}\\b_{j2}\\\vdots\\b_{jp} \end {bmatrix}=\begin {bmatrix}a_{1j}\\a_{2j}\\\vdots\\a_{pj} \end {bmatrix},j=1,2,\cdots,m $$$\begin {bmatrix}\gamma_{11}&amp;\gamma_{12}&amp;\cdots&amp;\gamma_{1p}\\\gamma_{21}&amp;\gamma_{22}&amp;\cdots&amp;\gamma_{2p}\\\vdots&amp;\vdots&amp;&amp;\vdots\\\gamma_{p1}&amp;\gamma_{p2}&amp;\cdots&amp;\gamma_{pp} \end {bmatrix}$为原始变量的相关系数；$\begin {bmatrix}b_{j1}\\b_{j2}\\\vdots\\b_{jp} \end {bmatrix}$为第j个因子得分函数的系数；$\begin {bmatrix}a_{1j}\\a_{2j}\\\vdots\\a_{pj} \end {bmatrix}$为载荷矩阵的第 j列注：共需要解m次才能解出所有的得分函数的系数。 6 因子分析步骤 选择分析的变量用定性分析和定量分析的方法选择变量，因子分析的前提条件是观测变量间有较强的相关性，因为如果变量之间无相关性或相关性较小的话，他们不会有共享因子，所以原始变量间应该有较强的相关性。 计算所选原始变量的相关系数矩阵相关系数矩阵描述了原始变量之间的相关关系。可以帮助判断原始变量之间是否存在相关关系，这对因子分析是非常重要的，因为如果所选变量之间无关系，做因子分析是不恰当的。并且相关系数矩阵是估计因子结构的基础。 提取公共因子这一步要确定因子求解的方法和因子的个数。需要根据研究者的设计方案或有关的经验或知识事先确定。因子个数的确定可以根据因子方差的大小，只取方差大于1(或特征值大于1)的那些因子，因为方差小于1的因子其贡献可能很小。或者按照因子的累计方差贡献率来确定，一般认为要达到60％才能符合要求。 因子旋转通过坐标变换使每个原始变量在尽可能少的因子之间有密切的关系，这样因子的实际意义更容易解释,也更容易为每个潜在因子赋予有实际意义的名字。 计算因子得分求出各样本的因子得分，有了因子得分值，则可以在许多分析中使用这些因子，例如以因子的得分做聚类分析的变量，做回归分析中的回归因子。 注 因子分析是十分主观的，在许多出版的资料中，因子分析模型都用少数可命名因子提供了合理解释。实际上，绝大多数因子分析并没有产生如此明确的结果。不幸的是，评价因子分析质量的法则尚未很好量化，质量问题只好依赖一个“哇！”准则如果在仔细检查因子分析的时候，研究人员能够喊出“哇，我明白这些因子”的时候，就可认为是成功地运用了因子分析方法。 主成分分析与因子分析主成分分析与因子分析有所不同，主成分分析仅仅是变量变换。 主成分分析：原始变量的线性组合表示新的综合变量，即主成分。 因子分析：潜在的假想变量和随机影响变量的线性组合表示原始变量。因子模型除了公共因子外还有特殊因子。公共因子只解释了原来变量的部分方差，而全部主成分解释了原来变量的全部方差。 主成分和公共因子的位置不同。因子分析也有因子载荷（ factor loading）的概念，代表了因子和原先变量的相关系数。但是在因子分析公式中的因子载荷位置和主成分分析不同。在数学模型上，因子分析和主成分分析也有不少区别。而且因子分析的计算也复杂得多。根据因子分析模型的特点，它还多一道程序：因子旋转（ factor rotation）；这个步骤可以使结果更好。旋转后的公共因子一般没有主成分那么综合，公共因子往往可以找到实际意义，而主成分常找不到实际的含义。可以看出，因子分析和主成分分析都依赖于原始变量，也只能反映原始变量的信息。所以原始变量的选择很重要。在得到分析的结果时，并不一定会都得到如我们例子那样清楚的结果。这与问题的性质，选取的原始变量以及数据的质量等都有关系。如果原始变量本质上独立，就很难把很多独立变量用少数综合的变量概括，降维就可能失败。数据越相关，降维效果就越好。可用如下方法进行变量间的相关性检验： KMO样本测度： KMO在0.9以上，非常适合； 0.8-0.9，很适合； 0.7-0.8，适合； 0.6-0.7，不太适合；0.5-0.6；很勉强； 0.5以下，不适合； 巴特莱特球体检验： H0：相关系数矩阵R为单位阵I。拒绝时H0可作因子分析 7 因子分析的R语言实现R语言做因子分析这里主要介绍三个函数，一个是自带的factanal函数。 factanal(x,factors,data=NULL,covmat=NUL,n.obs=NA,subset,na.action,start=NULL,score=c(&quot;none&quot;,&quot;regression&quot;,&quot;Bartlett&quot;),rotation=&quot;varimax&quot;,control=NULL,…) x是公式或者用于因子分析的数据，可以是矩阵（每一行为一个样本）或数据框；factors表示要生成的因子个数；data指定数据集，当x为公式的时候使用；covmat是样本的协方差矩阵或者相关系数矩阵，使用这个参数的时候x可以忽略；scores表示计算因子得分的方法；rotation表示因子旋转的方法，默认为”varimax”，最大方差旋转。这里近介绍几个常用的几个参数，其他参数说明可查询R语言官方帮助。另外，这个函数事实上仅支持用极大似然估计方法做因子分析。第二个函数就是自编函数实现的主成分分析方法做因子分析（具体函数代码后面给出）。 factor.analysis(x,m) x为相关系数矩阵，m为因子个数。第三个函数是psych包里的fa函数。 fa（r，nfactors=，n.obs=，rotate=，scores=，fm） r是相关系数矩阵或原始数据矩阵；nfactors设定提取的因子数（默认为1）；n.obs是观测数（输入相关系数矩阵时需要填写）；rotate设定放置的方法（默认互变异数最小法）；scores设定是否计算因子得分（默认不计算）；fm设定因子化方法（默认极小残差法）。用上一章提供的数据再进行因子分析。比较不同函数结果的差异。基于factnal函数，3个因子。 基于自编函数，3个因子。 基于fa函数，3个因子。]]></content>
      <categories>
        <category>Statistics &amp; Model</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用统计学与R语言实现学习笔记（十二）——主成分分析]]></title>
    <url>%2F2017%2F09%2F22%2F%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8ER%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Chapter 12 Priciple Component Analysis本篇是第十二章，内容是主成分分析。 1 主成分分析基本思想依旧从问题开始本篇的介绍。地理学和生态学研究里经常遇到的问题就是，影响变量非常之多，而且地球表层地理生态环境现象无法使用控制变量的方式进行实验。同时影响变量非常多，经常出现变量冗余、冗杂的现象，同时多元分布数据本身对人类的认知就是一种挑战。这里举个栗子：比如在研究城市经济发展的时候，我们会考虑到的因素会包括第一产业、第二产业、第三产业占比，城市人口，城市地理位置，城市气候适宜度，政策扶持等等很多因子，但是这里有很多因子存在共线性的情况，也就是变量冗余冗杂。用矛盾论的话说，要抓住主要矛盾，那么如何在多元分布数据中分离出主要的因子，这就是本篇的主角主成分分析（Priciple Component Analysis，PCA）。 所以它的基本思想是。 在社会经济的研究中，为了全面系统的分析和研究问题，必须考虑许多经济指标，这些指标能从不同的侧面反映我们所研究的对象的特征，但在某种程度上存在信息的重叠，具有一定的相关性。这种信息的重叠有时甚至会抹杀事物的真正特征与内在规律。主成分分析是利用降维的思想， 在力求数据信息丢失最少的原则下，对高维的变量空间降维，即在众多变量中找出少数几个综合指标（原始变量的线性组合），并且这几个综合指标将尽可能多地保留原来指标变异方面的信息，且这些综合指标互不相关。这些综合指标就称为主成分。主成分的数目少于原始变量的数目。在一个低维空间识辨系统要比在一个高维空间容易得多。因此，更容易抓住主要矛盾，揭示事物内部变量之间的规律性，使问题得到简化，提高分析效率。指标间具有相关性是做主成分分析的前提。主成分分析是一种数学变换方法，它把给定的一组变量通过线性变换转换为一组不相关的变量。在这种变换中，保持变量的总方差不变，同时，使第一主成分具有最大方差，第二主成分具有次大方差，依此类推。主成分与原始变量间的关系（1）每一个主成分是原始变量的线性组合。（2）主成分的数目少于原始变量的数目。（3）主成分保留了原始变量的大多数变异信息。（4）各主成分间互不相关。 2 几何解释与数学模型2.1 几何解释假定只有二维，即只有两个变量，由横坐标和纵坐标所代表；每个观测值都有相应于这两个坐标轴的坐标值。如果这些数据形成一个椭圆形状的点阵（这在二维正态的假定下是可能的）该椭圆有一个长轴和一个短轴。在短轴方向上数据变化较少。在极端的情况，短轴如退化成一点，长轴的方向可以完全解释这些点的变化，由二维到一维的降维就自然完成了。 由图可以看出这些样本点无论是沿着$x_l$轴方向或$x_2$轴方向都具有较大的离散性，其离散的程度可以分别用观测变量$x_l$的方差和$x_2$的方差定量地表示。显然，如果只考虑$x_1$和$x_2$中的任何一个，那么包含在原始数据中的经济信息将会有较大的损失。当坐标轴和椭圆的长短轴平行，那么代表长轴的变量就描述了数据的主要变化，而代表短轴的变量就描述了数据的次要变化。但是，坐标轴通常并不和椭圆的长短轴平行。因此，需要寻找椭圆的长短轴，并进行变换，使得新变量和椭圆的长短轴平行。如果长轴变量代表了数据包含的大部分信息，就用该变量代替原先的两个变量（舍去次要的一维），降维就完成了。椭圆的长短轴相差得越大，降维也越有道理。 2.2 数学模型如果我们将xl轴和x2轴先平移，再同时按逆时针方向旋转$\theta$角度，得到新坐标轴Fl和F2。Fl和F2是两个新变量。根据旋转变换的公式：$$ \begin{cases} y_1=x_1\cos\theta+x_2\sin\theta\\y_2=-x_1\sin\theta+x_2\cos\theta \end{cases} $$$$ \begin{pmatrix} y_1\\y_2 \end{pmatrix}=\begin{pmatrix} \cos\theta &amp; \sin\theta\\-sin\theta &amp; \cos\theta \end{pmatrix} \begin{pmatrix} x_1\\x_2 \end{pmatrix}=U’x $$$U’$为旋转变换矩阵，它是正交矩阵，即有$U’=U^{-1},U’U^{-1}=I$。旋转变换的目的是为了使得n个样品点在$F_l$轴方向上的离散程度最大，即$F_l$的方差最大。变量$F_l$代表了原始数据的绝大部分信息，在研究某经济问题时，即使不考虑变量$F_2$也无损大局。经过上述旋转变换原始数据的大部分信息集中到$F_l$轴上，对数据中包含的信息起到了浓缩作用。$F_l$， $F_2$除了可以对包含在$X_l$，$ X_2$中的信息起着浓缩作用之外，还具有不相关的性质，这就使得在研究复杂的问题时避免了信息重叠所带来的虚假性。二维平面上的个点的方差大部分都归结在$F_l$轴上，而$F_2$轴上的方差很小。 $F_l$和$F_2$称为原始变量，$x_1$和$x_2$的综合变量。 简化了系统结构，抓住了主要矛盾。多维情形多维变量的情况和二维类似。正如二维椭圆有两个主轴，三维椭球有三个主轴一样，有几个变量，就有几个主轴。和二维情况类似，高维椭球的主轴也是互相垂直的。首先把高维椭球的主轴找出来，再用代表大多数数据信息的最长的几个轴作为新变量。这些互相正交的新变量是原先变量的线性组合，叫做主成分(principal component)。假设我们所讨论的实际问题中，有p个指标，我们把这p个指标看作p个随机变量，记为$X_1， X_2， \cdots，X_p$，主成分分析就是要把这个p指标的问题，转变为讨论p个指标的线性组合的问题，而这些新的指标$F_1， F_2， \cdots， F_k(k\le p)$，按照保留主要信息量的原则充分反映原指标的信息，并且相互独立。这种由讨论多个指标降为少数几个综合指标的过程在数学上就叫做降维。主成分分析通常的做法是，寻求原指标的线性组合Fi。$$ F_1=u_{11}X_1+u_{21}X_2+\cdots+u_{p1}X_p\\F_2=u_{12}X_1+u_{22}X_2+\cdots+u_{p2}X_p\\\cdots\\F_p=u_{1p}X_1+u_{2p}X_2+\cdots+u_{pp}X_p $$满足条件每个主成分的系数平方和为1。即$$ u_{1i}^2+u_{2i}^2+\cdots+u_{pi}^2=1 $$主成分之间相互独立，即无重叠的信息。即$$ Cov(F_i，F_j）=0，i\neq j,i,j=1,2，\cdots，p $$主成分的方差依次递减，重要性依次递减，即$$ Var(F_1)\ge Var(F_2)\ge \cdots \ge Var(F_p) $$ 3 主成分的推导两个线性代数的结论1、若A是p阶实对称矩阵，则一定可以找到正交阵U，使$$ U^{-1}AU=\begin{bmatrix} \lambda_1 &amp; 0 &amp; \cdots &amp; 0\\0 &amp; \lambda_2 &amp; \cdots &amp; 0\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\0 &amp; 0 &amp; \cdots &amp; \lambda_p \end{bmatrix}_{p\times p} $$其中$\lambda_i，i=1,2,\cdots,p $是A的特征根。2、若上述矩阵的特征根所对应的单位特征向量为$u_1,\cdots,u_p$。令$$ U=(u_1,\cdots,u_p)=\begin {bmatrix} u_{11} &amp; u_{12} &amp; \cdots &amp; u_{1p}\\u_{21} &amp; u_{22} &amp; \cdots &amp; u_{2p}\\\vdots &amp; \vdots &amp; &amp; \vdots\\u_{p1} &amp; u_{p2} &amp; \cdots &amp; u_{pp} \end {bmatrix} $$则实对称阵A属于不同特征根所对应的特征向量是正交的，即有$U’U=UU’=I$第一主成分设X的协方差阵为$$ \Sigma_x=\begin {bmatrix} \sigma_{11} &amp; \sigma_{12} &amp; \cdots &amp; \sigma_{1p}\\\sigma_{21} &amp; \sigma_{22} &amp; \cdots &amp; \sigma_{2p}\\\vdots &amp; \vdots &amp; &amp; \vdots\\\sigma_{p1} &amp; \sigma_{p2} &amp; \cdots &amp; \sigma_{pp} \end {bmatrix} $$由于$\Sigma_x$为非负定的对称阵，必存在正交阵U，使得：$$ U’\Sigma_x U=\begin {bmatrix} \lambda_1 &amp; &amp; 0\\&amp; \ddots &amp;\\0 &amp; &amp; \lambda_p \end {bmatrix} $$其中$\lambda_1，\lambda_2，\cdots，\lambda_p$为$\Sigma_x$的特征根。不妨假设$\lambda_1\ge \lambda_2\ge \cdots \ge \lambda_p$。而U恰好是由特征根相对应的特征向量所组成的正交阵。$$ U=(u_1,\cdots,u_p)=\begin {bmatrix} u_{11} &amp; u_{12} &amp; \cdots &amp; u_{1p}\\u_{21} &amp; u_{22} &amp; \cdots &amp; u_{2p}\\\vdots &amp; \vdots &amp; &amp; \vdots\\u_{p1} &amp; u_{p2} &amp; \cdots &amp; u_{pp} \end {bmatrix} $$$$ U_i=(u_{1i},u_{2i},\cdots,u_{pi})’ i=1,2,\cdots,p $$设有p维正交向量$a_1=(a_{11},a_{21},\cdots,a_{p1})’$，$F_1=a_{11}X_1+\cdots+a_{p1}X_p=a’X$$$ V(F_1)=a_1’\Sigma a_1==a_1’U\begin {bmatrix} \lambda_1 &amp; &amp; &amp;\\&amp; \lambda_2 &amp; &amp;\\&amp; &amp; \cdots &amp; &amp;\\&amp; &amp; &amp; \lambda_p &amp;\end {bmatrix}U’a_1 $$当且仅当$a_1=u_1$时，即$F_1=u_{11}X_1+\cdots+u_{p1}X_p$时，有最大的方差$\lambda_1$。$Var(F_1)=U_1’\Sigma_x U_1=\lambda_1$。如果第一主成分的信息不够，则需要寻找第二主成分。第二主成分在约束条件$cov(F_1,F_2)=0$下，寻找第二主成分，取线性变换$$ F_2=u_{12}X_1+\cdots+u_{p2}X_p $$的方差次大$$ cov(F_1,F_2)=cov(u_1’x,u_2’x)=u_2’\Sigma u_1=\lambda_1 u_2’u_1=0 $$$$ Var(F_2)=U_2’\Sigma_x U_2=\lambda_2 $$类推$$ F_1=u_{11}X_1+u_{21}X_2+\cdots+u_{p1}X_p\\F_2=u_{12}X_1+u_{22}X_2+\cdots+u_{p2}X_p\\\cdots\\F_p=u_{1p}X_1+u_{2p}X_2+\cdots+u_{pp}X_p$$写成矩阵形式：$$ F=U’X $$$$ U=(u_1,\cdots,u_p)=\begin {bmatrix} u_{11} &amp; u_{12} &amp; \cdots &amp; u_{1p}\\u_{21} &amp; u_{22} &amp; \cdots &amp; u_{2p}\\\vdots &amp; \vdots &amp; &amp; \vdots\\u_{p1} &amp; u_{p2} &amp; \cdots &amp; u_{pp} \end {bmatrix} $$$$ X=(X_1,X_2,\cdots,X_p)’ $$ 4 主成分的性质1、均值 $E(U’x)=U’\mu$2、方差为所有特征根之和$$ \sum_{i=1}^pVar(F_i)=\lambda_1+\lambda_2+\cdots+\lambda_p=\sigma_1^2+\sigma_2^2+\cdots+\sigma_p^2 $$说明主成分分析把p个随机变量的总方差分解成为p个不相关的随机变量的方差之和。协方差矩阵$\Sigma$的对角线上的元素之和等于特征根之和。3、精度分析1）贡献率：第i个主成分的方差在全部方差中所占比重$\lambda_i/\sum_{i=1}^p\lambda_i$，称为贡献率，体现这个主成分的综合能力的大小，即反映原来p个指标的信息的多少。2）累积贡献率：前k个主成分共有多大的综合能力，用这个k个主成分的方差和在全部方差中所占比重$$ \sum_{i=1}^k\lambda_i/\sum_{i=1}^p\lambda_i $$来描述，称为累积贡献率。我们进行主成分分析的目的之一是希望用尽可能少的主成分$F_1，F_2，\cdots，F_k(k\le p)$代替原来的p个指标。到底应该选择多少个主成分，在实际工作中，所采用主成分个数的多少取决于能够反映原来变量85%以上的信息量为依据，即当累积贡献率≥85%时的主成分的个数就足够了。最常见的情况是主成分为2到3个。4、载荷矩阵$$ \begin {bmatrix} u_{11} &amp; u_{12} &amp; \cdots &amp; u_{1m}\\u_{21} &amp; u_{22} &amp; \cdots &amp; u_{2m}\\\vdots &amp; \vdots &amp; &amp; \vdots\\u_{p1} &amp; u_{p2} &amp; \cdots &amp; u_{pm} \end {bmatrix} $$原始变量与主成分之间的相关系数$$ F_j=u_{1j}x_1+u_{2j}x_2+\cdots+u_{pj}x_p j=1,2,\cdots,m,m\le p $$$$ F=U’X UF=X $$$$ \begin {bmatrix} x_1\\x_2\\\vdots\\x_p \end {bmatrix}=\begin {bmatrix} u_{11} &amp; u_{12} &amp; \cdots &amp; u_{1p}\\u_{21} &amp; u_{22} &amp; \cdots &amp; u_{2p}\\\vdots &amp; \vdots &amp; &amp; \vdots\\u_{p1} &amp; u_{p2} &amp; \cdots &amp; u_{pp} \end {bmatrix}\begin {bmatrix} F_1\\F_2\\\vdots\\F_p \end {bmatrix} $$$$ Cov(x_i,F_j)=Cov(u_{i1}F_1+u_{i2}F_2+\cdots+u_{ip}F_p,F_j)=u_{ij}\lambda_j $$$$ \rho(x_i,F_j)=\frac{u_{ij}\lambda_j}{\sigma_i\sqrt{\lambda_j}}=\frac{u_{ij}\sqrt{\lambda_j}}{\sigma_i} $$可见，$x_i$和$F_j$的相关的密切程度取决于对应线性组合系数的大小。该相关系数又叫因子负荷量。在解释主成分的成因或是第i个变量对第k个主成分的重要性时，应当根据因子负荷量而不是变换系数u.原始变量被主成分的提取率主成分的贡献率和累计贡献率度量了$F_1，F_2，\cdots，F_m$分别从原始变量$X_1, X_2, \cdots, X_P$中提取了多少信息。那么$X_1, X_2, \cdots, X_P$各有多少信息分别被$F_1，F_2，\cdots，F_m$提取？这可以用$F_1，F_2，\cdots，F_m$分别与$X_1, X_2, \cdots, X_P$的相关系数的平方来衡量。$$ Var(x_i)=Var(u_{i1}F_1+u_{i2}F_2+\cdots+u_{ip}F_p) $$则$$ u_{i1}^2\lambda_1+u_{i2}^2\lambda_2+\cdots+u_{im}^2\lambda_m+\cdots+u_{ip}^2\lambda_p=\sigma_i^2 $$$u_{ij}^2\lambda_j$是$F_j$能说明的第i 原始变量的方差。$u_{ij}^2\lambda_j/\sigma_i^2$是$F_j$提取的第i原始变量信息的比重。如果我们仅仅提出了m个主成分，则第i原始变量信息的被提取率为：$$ \Omega_i=\sum_{j=1}^m\lambda_ju_{ij}^2/\sigma_i^2=\sum_{j=1}^m\rho_{ij}^2 $$公共成分定义：如果一个主成分仅仅对某一个原始变量有作用，则称为特殊成分。如果一个主成分对所有的原始变量都起作用，则称为公共成分。 5 主成分分析的步骤第一步：由X的协方差阵或相关系数阵Σ，求出其特征根，即解方程，可得特征根。第二步：求出特征根所对应的特征向量$U_1,U_2,\cdots,U_p$，$$ U_i=(u_{1i},u_{2i},\cdots,u_{pi})’ $$第三步：计算累积贡献率，给出恰当的主成分个数。$$ F_i=U_i’X，i=1,2,\cdots,k(k\le p) $$第四步：计算所选出的k个主成分的得分。将原始数据的中心化值:$$ X_i^{\ast}=X_i-\bar X=(x_{1i}-\bar x_1,x_{2i}-\bar x_2,\cdots,x_{pi}-\bar x_p)’ $$代入前k个主成分的表达式，分别计算出各单位k个主成分的得分，并按得分值的大小排队。 基于协方差矩阵 在实际问题中， X的协方差通常是未知的，样品有$$ X_1=(x_{1l},x_{2l},\cdots,x_{pl})’(l=1,2,\cdots,n) $$$$ \hat\Sigma_x=(\frac{1}{n-1}\sum_{l=1}^n(x_{ij}-\bar x_i)(x_{jl}-\bar x_j))_{p\times p} $$ 基于相关系数矩阵 如果变量有不同的量纲， 变量水平差异很大，应该基于相关系数矩阵进行主成分分析。不同的是计算得分时应采用标准化后的数据。 6 主成分的应用与回归1、主成分分析能降低所研究的数据空间的维数。即用研究m维的Y空间代替p维的X空间(m＜p)，而低维的Y空间代替高维的x空间所损失的信息很少。即使只有一个主成分$Y_1$(即m＝1)时，这个$Y_1$仍是使用全部X变量(p个)得到的。在所选的前m个主成分中，如果某个Xi的系数全部近似于零的话，就可以把这个Xi删除，这也是一种删除多余变量的方法。2、多维数据的一种图形表示方法。多元统计研究的问题大都多于3个变量，要把研究的问题用图形表示出来是不可能的。然而，经过主成分分析后，我们可以选取前两个主成分或其中某两个主成分，根据主成分的得分，画出n个样品在二维平面上的分布情况，由图形可直观地看出各样品在主分量中的地位。3、用主成分分析法构造回归模型。即把各主成分作为新自变量代替原来的自变量做回归分析。主成分回归方法$$ F_1=u_{11}X_1+u_{21}X_2+\cdots+u_{p1}X_p\\F_2=u_{12}X_1+u_{22}X_2+\cdots+u_{p2}X_p\\\cdots\\F_p=u_{1p}X_1+u_{2p}X_2+\cdots+u_{pp}X_p$$ $$ Y_i^{\ast}=\gamma_1F_{11}+\gamma_2F_{12}+\cdots+\gamma_mF_{1m}+\varepsilon_i\\\sum_{i=1}^n[Y_i^{\ast}-\gamma_1F_{11}-\gamma_2F_{12}-\cdots-\gamma_mF_{1m}]^2=min $$ 原始数据观测矩阵 $$ X_0=\begin {bmatrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p}\\x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p}\\\vdots &amp; \vdots &amp; &amp; \vdots\\x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np} \end {bmatrix} $$ 主成分系数矩阵 $$ U=(u_1,\cdots,u_p)=\begin {bmatrix} u_{11} &amp; u_{12} &amp; \cdots &amp; u_{1p}\\u_{21} &amp; u_{22} &amp; \cdots &amp; u_{2p}\\\vdots &amp; \vdots &amp; &amp; \vdots\\u_{p1} &amp; u_{p2} &amp; \cdots &amp; u_{pp} \end {bmatrix} $$ 主成分得分矩阵 $$ \begin {bmatrix} F_{11} &amp; F_{12} &amp; \cdots &amp; F_{1p}\\F_{21} &amp; F_{22} &amp; \cdots &amp; F_{2p}\\\vdots &amp; \vdots &amp; &amp; \vdots\\F_{n1} &amp; F_{n2} &amp; \cdots &amp; F_{np} \end {bmatrix}\\F=X_0U $$ 主成分分析的一些注意事项主成分分析依赖于原始变量，也只能反映原始变量的信息。所以原始变量的选择很重要。如果原始变量本质上独立，那么降维就可能失败，这是因为很难把很多独立变量用少数综合的变量概括。数据越相关，降维效果就越好。分析结果并不一定会有清楚的解释。这与问题的性质，选取的原始变量以及数据的质量等都有关系。基于相关系数矩阵还是基于协方差矩阵做主成分分析？有时基于相关系数矩阵和基于协方差矩阵求出的主成分会有很大不同，且两者之间不存在简单的线性关系。一般而言，当分析中所选择的经济变量具有不同的量纲，变量水平差异很大，应考虑将数据标准化，选择基于相关系数矩阵的主成分分析。对同度量或是取值范围在同量级的数据，选择基于协方差矩阵的主成分分析。选择几个主成分？主成分分析的目的是简化变量，一般情况下主成分的个数应该小于原始变量的个数。关于保留几个主成分，应该权衡主成分个数和保留的信息。如何解释主成分所包含的经济意义？主成分分析不要求数据来自于正态总体。一般认为当原始数据大部分变量的相关系数都小于0.3时，运用主成分分析的效果不显著。 7 主成分分析的R语言实现主成分分析的函数本篇介绍的主要有两个。一个是princomp，一个是psych里的principal。 princomp(x,cor=FALSE,scores=TRUE) x为主成分分析数据集，cor=TRUE和FALSE分别代表是基于相关系数矩阵计算还是协方差矩阵计算。scores则代表是否存储主成分得分。 principal(x,nfactors=2,rotate=&quot;varimax&quot;,scores=T,covar=F) x为主成分分析数据集，nfactors为主成分个数，rotate表示旋转方式（一般选方差最大，保证互不相关），scores则代表是否存储主成分得分，covar=TRUE和FALSE分别代表是基于协方差矩阵计算还是相关系数矩阵计算。这回用的数据是2006年城市统计年鉴285个地级市的经济人口数据，探究gdp与人口之间的关系。先做一个相关系数可视化。发现人口因子之间相互影响因子很高。 于是先对人口的几个因子进行降维和主成分分析，中途发现第三产业从业人数（third)加入会使得系数矩阵不正定，后面就删除了第三产业从业人数(third)。分别用不同方式进行主成分分析结果。princomp结果（基于协方差矩阵）碎石图 结果 主成分得分图 princomp结果（基于相关系数矩阵）碎石图 结果 主成分得分图 principal结果碎石图 因子关系图 主成分得分图 碎石图表示的是曲线与纵坐标1交点的横坐标即为主成分个数，而主成分得分荷图是将原始数据的坐标映射在主成分分析的坐标上，事实上可以根据主成分得分在不同象限对原始数据进行分类，在本篇的样例数据里其实就是可以通过人口生成的几个主成分对中国地级市进行分类，可以区分出是在第一主成分得分高，第二主成分得分低的城市，亦或是其他排列组合的分类结果。关于这种可视化图具体如何解释。可以参照如下的文章。 http://www.cnblogs.com/SCUJIN/p/5965946.html]]></content>
      <categories>
        <category>Statistics &amp; Model</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用统计学与R语言实现学习笔记（十一）——判别分析]]></title>
    <url>%2F2017%2F09%2F11%2F%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8ER%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Chapter 11 Discriminant Analysis笔者最近任务繁重，断更了一顿时间，最近会开始慢慢把这个系列写完。本篇是第十一章，内容是判别分析。 1 判别分析应用判别分析（Discriminant Analysis）——判别分析的目的是对已知分类的数据建立由数值指标构成的分类规则，然后把这样的规则应用到未知分类的样本中去分类，以识别未知样本所属的类别。判别分析是多元数据分析的重要方法之一。通常解决被解释变量是非数值变量，解释变量是数值变量的情形。事实上地学领域应用判别分析最多的是在哪里呢？其实是遥感影像的地物分类，通常遥感导论中无论Erdas或者ENVI在做完监督分类之后，其实就是用标注的样本去训练判别函数，然后用判别函数完成整幅影像的判别分析，就可以分出不同的地物类型，这种方法就是我们最普遍使用的极大似然法。而这里的被解释变量就是地物类型，解释变量（多元）就是遥感影像不同波段的DN值，或者是辐射率。 聚类分析和判别分析差异在聚类分析中，人们一般事先并不知道应该分成几类及哪几类，全根据数据确定。在判别分析中，至少有一个已经明确知道类别的“训练样本”，并利用该样本来建立判别准则，并通过预测变量来为未知类别的观测值进行判别。通常实际问题中，可以先聚类以得知类型,再进行判别。用机器学习的话来说，聚类分析是非监督学习，判别分析属于监督学习。 判别分析的数据结构 individuals $X_1$ $X_2$ $\cdots$ $X_l$ $\dots$ $X_p$ Y 1 28 1.0 $\cdots$ 114 $\cdots$ 0.15 1 2 29 2.0 $\cdots$ 117 $\cdots$ 0.20 1 $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ i $x_{i1}$ $x_{i2}$ $\cdots$ $x_{il}$ $\cdots$ $x_{ip}$ 2 $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ 47 15 8 $\cdots$ 64 $\cdots$ 0.51 2 48 16 7.5 $\cdots$ 65 $\cdots$ 0.50 3 $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ n $x_{n1}$ $x_{n2}$ $\cdots$ $x_{nl}$ $\cdots$ $x_{np}$ 3 对比聚类分析的数据结构，事实上就是多了最后一列的Y。 个体由$X_1,X_2,\cdots,X_p$变量描述。 有分类变量$Y$明确对个体分类。 问题：建立$Y与X_1,X_2,\cdots,X_p$变量间关系的函数。根据函数将新个体进行分类。 误判率误判率的高低有下面两个因素决定： 主观因素：分界线的位置要正确。 客观因素：均值，方差——通过选择指标来控制：一般来说，维度高一点，可以使分辨率高一些，但在许多情况下，指标太多，不仅不能提高分辨率，还增加计算量（需要丰富的实际经验和试算）；在做判别分析前，要做假设检验。在两个总体的均值有显著差异的情况下，再做判别分析。 判别分析的假设 每一个判别变量（解释变量）不能是其他判别变量的线性组合——不符合该假设的话，无法估计判别函数，变量间高度相关或一变量与其他变量的线性组合高度相关时，参数估计的标准误差将很大。 判别变量之间具有多元正态分布——可精确的计算显著性检验值和归属概率。 如要采用线性判别函数，还要求各组协方差距阵相等——线性判别函数使用起来最方便、在实际中使用最广。 2 判别分析方法2.1 距离判别法两总体情况假设有两个总体$G_1$和$G_2$，如果能够定义点x到它们的距离d(x,$G_1$)和d(x,$G_2$)，则可用如下规则进行判别： 如果d(x,$G_1$) &lt; d(x,$G_2$)则$x\in G_1$ 如果d(x,$G_2$) &lt; d(x,$G_1$)则$x\in G_2$ 如果d(x,$G_1$) = d(x,$G_2$)则待判。 距离常选用马氏距离——假设$\mu_1,\mu_2,\Sigma_1,\Sigma_2$分别为$G_1和G_2$的均值向量和协方差阵，则点$x$到$G_i$的马氏距离为$$ d^2(x,G_i)=(x-μ_i)’(\Sigma_i)^{-1}(x-μ_i) $$马氏距离的好处是可以克服变量之间的相关性干扰，并且消除各变量量纲的影响。 $\Sigma_1=\Sigma_2=\Sigma$定义：$$ \begin{aligned}d^2(x,G_1)-d^2(x,G_2) &amp; =(x-\mu_1)’\Sigma^{-1}(x-\mu_1)-(x-\mu_2)’\Sigma^{-1}(x-\mu_2) \ &amp;=-2[x-(\mu_1+\mu_2)/2]’\Sigma^{-1}(\mu_1-\mu_2) \end{aligned} $$令：$ \bar\mu=(\mu_1+\mu_2)/2, \alpha=\Sigma^{-1}(\mu_1-\mu_2),W(x)=(x-\bar\mu)’\alpha=\alpha’(x-\bar\mu) $判别规则：如果W(x)&gt;0，d(x,$G_1$) &lt; d(x,$G_2$)则$x\in G_1$如果W(x) d(x,$G_2$)则$x\in G_2$如果W(x)=0，d(x,$G_1$) = d(x,$G_2$)则待判。称W(x)为判别函数(discriminant function)，α为判别系数。当$\mu_1,\mu_2,\Sigma$未知时，可通过样本来估计。$ x_1^{(i)},\cdots,x_{n_i}^{(i)} $为来自$G_i$的样本(i=1,2)。$$ \hat\mu^{(i)}=\frac{1}{n_i}\sum_{k=1}^{n_2}x_k^{(i)}=\bar x^{(i)},\hat \Sigma=\frac{1}{n_1+n_2-2}(S_1+S_2), $$$$ S_i=\sum_{t=1}^{n_i}(x_t^{(i)}-\bar x^{(i)})(x_t^{(i)}-\bar x^{(i)})’,\bar x=\frac{1}{2}(\bar x^{(1)}+\bar x^{(2)}) $$判别函数为$W(x)=(x-\bar x)’\Sigma^{-1}(\bar x^{(1)}-\bar x^{(2)})$ $\Sigma_1 \neq \Sigma_2$判别函数为二次函数$W(x)=d^2(x,G_2)-d^2(x,G_1)=(x-\mu_2)’\Sigma_2^{-1}(x-\mu_2)-(x-\mu_1)’\Sigma_2^{-1}(x-\mu_1)$按照距离最近原则，判别准则为：如果W(x)&gt;0即d(x,$G_1$) &lt; d(x,$G_2$)则$x\in G_1$如果W(x) d(x,$G_2$)则$x\in G_2$如果W(x)=0即d(x,$G_1$) = d(x,$G_2$)则待判。 多总体情况 多总体情况：协方差相同假设有k个总体$G_1,G_2,\cdots,G_k$，它们的均值向量分别为$\mu_1,\mu_2,\cdots,\mu_k$,协方差阵为$\Sigma$，类似于两总体的讨论，判别函数为：$W_{ij}(x)=[x-(\mu_1+\mu_2)/2]’\Sigma^{-1}(\mu_i-\mu_j),i,j=1,\cdots,k$判别规则：如果存在i，对所有j≠i，有$W_{ij}(x)&gt;0$，则$x\in G_i$，否则待判。如果服从多元正态分布，且各组协方差相同$\begin{aligned} d^2(x,G_i)&amp; =(x-\mu_i)’\Sigma^{-1}(x-\mu_i) \ &amp; =x’\Sigma^{-1}x-2(x’\Sigma^{-1}\mu_i-\mu_i’\Sigma^{-1}\mu_i/2) \ &amp; =x’\Sigma^{-1}x-f_i(x)\end{aligned}$在所有的$f_i(x)$中，哪个$f_i(x)$的值大，x到相应的组i的马氏距离小，判$x\in G_i$ 多总体情况：协方差不等假设有k个总体$G_1,G_2,\cdots,G_k$，它们的均值向量分别为$\mu_1,\mu_2,\cdots,\mu_k$,协方差阵为$\Sigma_1,\Sigma_2,\cdots,\Sigma_k$，类似于两总体的讨论，判别函数为：$W(x)=(x-\mu_j)’\Sigma_j^{-1}(x-\mu_j)-(x-\mu_i)’\Sigma_i^{-1}(x-\mu_i),i,j=1,\cdots,k$判别规则：如果存在i，对所有j≠i，有$W_{ij}(x)&gt;0$，则$x\in G_i$，否则待判。如果总体均值、协方差未知，用样本均值、协方差估计。 若总体不服从正态分布，直接从马氏距离来做判别分析，失去了概率意义，仅仅是一直观的经验判断而已，可能偏误较大。 2.2 Fisher判别法Fisher判别法的思想就是投影，将k组p维数据投影到某一个方向，使得它们的投影组与组之间尽可能的分开。考虑只有两个(预测)变量的判别问题。假定只有两类。数据中的每个观测值是二维空间的一个点。这里只有两种已知类型的训练样本。一 类 有 38 个 点 ( 用“o”表示)，另一类有44个点(用“*”表示)。按原来变量(横坐标和纵坐标)，很难将这两种点分开。 但是沿着图上的虚线方向朝和这个虚线垂直的一条直线进行投影会使得这两类分得最清楚。可以看出，如果向其他方向投影，判别效果不会比这个好。有了投影之后，再用前面讲到的距离远近的方法得到判别准则。这种先投影的判别方法就是Fisher判别法。Fisher判别法 不要求总体分布类型； 工作原理就是对原数据系统进行坐标变换，寻求能够将总体尽可能分开的方向； a为$R^p$中的任一向量，点x在以a为法方向的投影为a’x; 各组数据的投影为：$$ G_i:a’x_1^{(i)}\cdots a’x_n^{(i)},i=1,2,\cdots,k$$ 这些数据正好组成一元方差分析的数据。 将$G_m$组中数据投影的均值记为$ a’\bar x^{(m)} $,有：$$ a’\bar x^{(m)}=\frac{1}{n_m}\sum_{i=1}^{n_m}a’\bar x_i^{(m)} ,m=1,\cdots,k$$记k组数据投影的总均值为$a’\bar x$，有：$$ a’\bar x=\frac{1}{n}\sum_{m=1}^{k}\sum_{i=1}^{n_m}a’\bar x_i^{(m)} $$组间离差平方和为：$$ SSG=\sum_{m=1}^{k}n_m(a’\bar x^{(m)}-a’\bar x)^2=a’[\sum_{m=1}^{k}n_m(\bar x^{(m)}-\bar x)(\bar x^{(m)}-\bar x)’]a=a’Ba; $$$$ B=\sum_{m=1}^{k}n_m[(\bar x^{(m)}-\bar x)(\bar x^{(m)}-\bar x)’] $$组内离差平方和为：$$ SSE=\sum_{m=1}^k \sum_{i=1}^{n_m}(a’\bar x^{(m)}-a’\bar x)^2=a’[\sum_{m=1}^k \sum_{i=1}^{n_m}(\bar x^{(m)}-\bar x)(\bar x^{(m)}-\bar x)’]a=a’Ea $$$$ E=\sum_{m=1}^k \sum_{i=1}^{n_m}(\bar x^{(m)}-\bar x)(\bar x^{(m)}-\bar x)’ $$如果K组有显著差异，则$$ F=\frac{SSG/(k-1)}{SSE/(n-k)}=\frac{n-k}{k-1}\frac{a’Ba}{a’Ea} $$F应充分大，即希望找到a使得SSG尽可能大而SSE尽可能小。$$ \Delta (a)=\frac{a’Ba}{a’Ea}\rightarrow max $$使$\frac{a’Ba}{a’Ea}$最大的值为方程$\left|B-\lambda E\right|=0$的最大特征根$\lambda_1$。记方程$\left|B-\lambda E\right|=0$的全部特征根为$\lambda_1\ge \cdots \ge \lambda_r \gt 0$，相应的特征向量为$v_1,\cdots,v_r$。$\Delta (a)$的大小可以估计判别函数$y_i (x)=v_i’x(=a’x)$的效果。记$p_i$为判别能力（效率），有：$$ p_i=\frac{\lambda_i}{\sum_{h=1}^r \lambda_h} $$在有些问题中，仅用一个线性判别函数不能很好区别各个总体，可取第二个、第三个，以此类推。 m个判别函数的判别能力定义为：$$ \sum_{i=1}^mp_i=\frac{\sum_{i=1}^m\lambda_i}{\sum_{h=1}^r \lambda_h} $$据此来确定选择多少判别函数。判别准则选择i使得：$$ v_1’(x-\mu_i)+\cdots+v_m’(x-\mu_i) $$的值最小Fisher判别法实质 选几个新的综合性指标，代替原来的p个指标。 构成新的综合性指标的条件：（1）不同类的均值差距尽可能大；（2）各类中的方差尽可能小。 Fisher判别法的依据不是x属于哪个总体的概率的大小，而是类别之间具有最大的可分性，也没有考虑错判带来的损失大小（错报台风登陆vs.漏报台风登陆）。 2.3 Bayes判别法 不用判别式，而是比较新给样品属于各个总体的条件概率p(g|x)，$g=1,\cdots,k$的大小，将新样品判归为来自条件概率最大的总体。 先给出 k 个总体的先验概率$q_1,\cdots,q_k$（实践中通常把频率作为先验概率）。如各总体密度为${f_k(x)}$，则后验概率为($g=1,\cdots,k$):$P(g|x)=q_gf_g(x)/\Sigma_i q_if_i(x)$。 当且仅当$P(h|x)= max_gP(g|x)$，判x来自第h总体。 也可以用使错判的损失最小的准则来判别。 设($D_1,D_2,\cdots,D_K$)是$R_p$的一个完备的划分，当样品x属于$D_i$,就判x来自$G_i$。 记$p(j|i), c(j|i)$分别为来自i总体的个体被错判到第j总体的概率和损失。定义平均错判损失(ECM: expected cost of misclassification)为$ECM(D)=\Sigma_{i=1\cdots k}q_i[\Sigma_{j=1\cdots k}p(j|i)c(j|i)]$ Bayes判别法就是要选择划分D使得ECM(D)最小。 3 建立判别函数的方法与多元回归类似，变量选择的好坏直接影响判别分析的效果。常遇问题：（1）忽略最主要的指标；（2）引入太多指标，计算量既大又干扰分析。 全模型法(SPSS系统默认方法） 前向选择法从没有变量的模型开始 每一部逐步把对判别函数贡献最大的变量加入模型，直到模型外没有一个变量符合条件为止。当希望有较多变量进入判别函数时，选用此方法（在Syntax中实现）。选择使威尔克斯统计量最小且显著的变量加入。 后向选择法从包含用户指定的所有变量的模型开始。每一部逐步把对判别函数贡献最小的变量从模型中剔除出去，直到留在模型中的变量都符合条件为止。当希望判别函数含有较少变量时，选用此方法。选择使威尔克斯统计量最大且不显著的变量剔除。 逐步选择法前向选择和后向选择的结合。从没有变量的模型开始。每一部逐步把对判别函数贡献最大的变量加入模型，同时，对模型中的变量进行检验，把不符合条件的变量从模型中删除。是一种较好的方法。选择使威尔克斯统计量最小且显著的变量加入。选择使威尔克斯统计量最大且不显著的变量剔除。 4 判别分析的步骤及注意事项判别分析的步骤 第1步：确定研究的问题与目的判别分析适合将个体归类的问题，特别适合一个定性的被解释变量和多个定量的解释变量的情形。 第2步：判别分析研究设计解释变量与被解释变量的选择：被解释变量的组数可以是两个或更多，但必须互斥和完备。样本容量：判别分析对样本量与预测变量的比率敏感。总样本量：建议比率为每个解释变量20个观测，最小的总样本量为每个变量5个观测。最小的组的大小必须超过解释变量的个数，建议每组至少有20个观测，还要注意组的相对大小（大的组有不相称的高的分类机会）。样本分割：需要将样本分割为两个子样本，一个用于估计判别函数，另一个用于验证。随机分组，最常见的是随机分为两半。通常各组比率相同。 第3步：判别分析的假定多元正态性，如不满足建议使用Logistic回归。Box’s Test 检验各组协方差阵是否相等，不等的协方差矩阵可能会负面影响分类过程，观测会被“过度归类”到大的协方差阵组中。解释变量的多重共线性。 第4步：估计判别模型和评估整体拟和统计显著性： Wilks’ Lambda， Hotelling迹和Pillai评估判别函数的判别效力的显著性。评估整体拟和：计算每个观测的判别Z得分，检验各组在判别Z得分上的差异，评估组，关系的预测精度。 第5步：结果的解释解释判别分析中每个解释变量的相对重要性。标准化判别权重（判别系数）：如存在多重共线性时不合适，可能不稳定。判别载荷，又称结构相关系数，是每个解释变量与判别函数的简单相关系数，也可能不稳定。偏F值。能力指数：当保留多个判别函数时。 第6步：结果的验证分隔样本或交叉验证法。 判别分析注意事项 解释变量（判别变量）必须是可测量的。 每一个判别变量不能是其它判别变量的线性组合（不能提供新的信息，无法估计判别函数）。 判别变量不能高度相关，否则导致估计的标准误差很大。 训练样本中必须包含所有要判别的类型，分类必须清楚（在判别分析前最好应当做假设检验，确定各个类的有关变量的均值是显著不同的）。 要选择好可能用于判别的预测变量。判别分析是为了正确地分类，但同时也要注意使用尽可能少的预测变量来达到这个目的。使用较少的变量意味着节省资源和易于对结果作解释。 检验结果(在SPSS选项中选择Wilks’ Lambda、Rao’s V、 The Squared Mahalanobis Distance或The Sum of Unexplained Variations等检验的计算机输出)，以确定是否分类结果仅由于随机因素。 对于多个判别函数，要弄清各自的重要性。 注意训练样本的正确和错误分类率。研究被误分类的观测值，看是否能找出原因。 5 R语言中判别分析实现正如上文提到的，我们以一个简单的地物分类的例子来进行实践。原始的遥感影像如图所示(高分一号卫星16 m数据）。 高分一号卫星有四个波段，分别显示如下： 我们随机在区域内生成了55个样本点，并根据目视解译做了分类，由于所处研究区位于新城且仅作测试，用地类型仅选择了两类：建设用地/不透水面和植被。前面已经用4，3，2显示了原始影像，红色部分即为植被。植被为类型1，建设用地/不透水面为类型2。 另外我们随机在区域内还生成了10个样本点作为验证点。 接着下来我们读取数据并且利用三种不同的判别分析方法进行判别分析地物类别。判别分析可以自己通过dist函数计算距离得到。现在已经有对应的包可以直接分析。这里推荐两个包（WMDB和MASS）。WMDB可以实现加权马氏距离判别分析和Bayes判别分析，MASS可以实现Fisher判别分析。距离判别分析的函数为wmd。具体参数如下： wmd(Trnx,TrnG,Tweight=NULL,Tstx=NULL,var.equal=F) Trnx是训练样本数据。TrnG为分类结果，Tweight为指定权重，可以根据主成分贡献计算或者取相等（原始的判别分析法），Tstx为待测样本数据，var.equal指定协方差矩阵是否相等。Fisher判别分析的函数为lda。具体参数如下： lda(formula,data,……,subset,na.action) formula形如groups~x1+x2+……的形式，data为数据集，subset指定训练样本，na.action指定有缺失值处理方式。Bayes判别分析的函数为dbayes。具体参数如下： dbayes(Trnx,TrnG,p=rep(1,length(levels(TrnG))),Tstx=NULL,var.equal=F) Trnx是训练样本数据。TrnG为分类结果，p为指定先验概率的向量，Tstx为待测样本数据，var.equal指定协方差矩阵是否相等。接下来就是基于高分影像的四个波段进行训练和判别分析。距离判别分析结果。 Fisher判别分析结果。 列联表分析及判别准确率。 Bayes判别分析结果。 从样本数据来看，Fisher结果是最好的。接下来即按照训练好的判别规则进行分类。这里发现WMDB包的两个函数并没有提供预测功能，这里选用了另一个包klaR来做贝叶斯分类（朴素贝叶斯）。分类结果对比： 为了验证准确率，这里利用随机生成的10个验证点进行精度验证。 由于选取验证点较少，准确率都达到了100%。从实际影像对比来看，似乎Bayes方法将更多细小的植被提取出来了，但是也有一部分道路错分。Fisher方法少提取了一部分，但错分的部分几乎没有。这部分的代码和数据后面会一起放出来。]]></content>
      <categories>
        <category>Statistics &amp; Model</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用统计学与R语言实现学习笔记（十）——聚类分析]]></title>
    <url>%2F2017%2F06%2F21%2F%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8ER%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Chapter 10 Cluster Analysis本篇是第十章，内容是聚类分析。由于之后的几章是典型的分析方法。而且在14章的案例里面可能不会体现，所以内容里会渗透较多的R语言操作。 1 多元分布基本概念在研究实际问题的时候，我们经常遇到的是多变量的问题，由于指标间相互不独立，单独割裂开来分别研究分析，不能从整体上把握所研究问题的实质。所以我们必须对多元变量及其分布进行统计和分析，在地学领域这种问题比比皆是，这里就不展开阐述了，接下来是一堆纯数学概念，数学恐惧者慎入，这部分的重点应该是关于协方差矩阵。一般来说，假设所研究的问题有p个指标，进行了n次独立观测，得到了np个数据。那么对于单次独立观测，我们定义随机向量（Random Vector）为：$$ x=(x_1,x_2,\cdots,x_p)’ $$其概率分布函数定义为$$ F(a_1,a_2,\cdots,a_p)=P(x_1\le a_1,x_2\le a_2,\cdots,x_p\le a_p) $$分布函数的性质 非降的右连续函数 分布函数的取值范围为[0，1]，即$$ 0\le F(a_1,a_2,\cdots,a_p)\le 1 $$ 分布函数当变量取值为无穷大时，函数值收敛到1，即$$ F(\infty,\infty,\cdots,\infty)=1 $$ 多元概率密度函数随机向量$x=(x_1,x_2,\cdots,x_p)’$的分布函数可以表示为$$F(a_1,a_2,\cdots,a_p)=P(x_1\le a_1,x_2\le a_2,\cdots,x_p\le a_p)=\int_{-\infty}^{a_1}\cdots\int_{-\infty}^{a_p}f(x_1,x_2,\cdots,x_p)dx_1\cdots dx_p $$则称$ x=(x_1,x_2,\cdots,x_p)’ $为连续型随机变量，称$f(x_1,x_2,\cdots,x_p)$为其多元概率密度函数。若$F(a_1,a_2,\cdots,a_p)$在点$(x_1,x_2,\cdots,x_p)$连续，则$f(x_1,x_2,\cdots,x_p)=\frac{\partial^p}{\partial x_1\partial x_2\cdots\partial x_p}F(x_1,x_2,\cdots,x_p)$且有$1\ge F(x_1,x_2,\cdots,x_p)\ge 0$，$\int_{-\infty}^{a_1}\cdots\int_{-\infty}^{a_p}f(x_1,x_2,\cdots,x_p)dx_1\cdots dx_p=1$。数学期望定义$$ \begin {bmatrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1q}\\x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2q}\\\vdots &amp; \vdots &amp; &amp; \vdots\\x_{p1} &amp; x_{p2} &amp; \cdots &amp; x_{pq} \end {bmatrix} $$是由随机变量构成的随机矩阵，定义X的数学期望为$$ \begin {bmatrix} E(x_{11}) &amp; E(x_{12}) &amp; \cdots &amp; E(x_{1q})\\E(x_{21}) &amp; E(x_{22}) &amp; \cdots &amp; E(x_{2q})\\\vdots &amp; \vdots &amp; &amp; \vdots\\E(x_{p1}) &amp; E(x_{p2}) &amp; \cdots &amp; E(x_{pq}) \end {bmatrix} $$特别当q=1时，便可得到随机向量$(x_1,x_2,\cdots,x_p)’$的数学期望为$E(x)=(E(x_1),E(x_2),\cdots,E(x_p))’$协方差矩阵设$x=(x_1,x_2,\cdots,x_p)’$和$y=(y_1,y_2,\cdots,y_q)’$分别为p维和q维随机向量，则其协方差矩阵为$$ E\begin {bmatrix} \begin {pmatrix} x-E(x_1) \ x-E(x_2)\\\vdots\\x-E(x_p) \end{pmatrix} (y-E(y_1),(y-E(y_2),\cdots,(y-E(y_q) \end {bmatrix} $$ $$ =\begin {pmatrix} cov(x_1,y_1) &amp; cov(x_1,y_2) &amp; \cdots &amp; cov(x_1,y_q)\\cov(x_2,y_1) &amp; cov(x_2,y_2) &amp; \cdots &amp; cov(x_2,y_q)\\\vdots &amp; \vdots &amp; &amp; \vdots\\cov(x_p,y_1) &amp; cov(x_p,y_2) &amp; \cdots &amp; cov(x_p,y_q) \end {pmatrix}=cov(X,Y) $$$x=(x_1,x_2,\cdots,x_p)’$的协方差矩阵为$$\Sigma=Var(x)=\begin {pmatrix} var(x_1) &amp; cov(x_1,x_2) &amp; \cdots &amp; cov(x_1,x_p)\\cov(x_2,x_1) &amp; var(x_2) &amp; \cdots &amp; cov(x_2,x_p)\\\vdots &amp; \vdots &amp; &amp; \vdots\\cov(x_p,x_1) &amp; cov(x_p,x_2) &amp; \cdots &amp; var(x_p) \end {pmatrix} $$随机向量X的协方差矩阵Σ是非负定矩阵。设A是常数矩阵，b为常数向量，则$$ Var(AX+b)=AV(X)A’=A\Sigma A’ $$若$(x_1,x_2,\cdots,x_p)’$的分量相互独立，则协方差矩阵除主对角线上的元素外均为零，即$$\Sigma=Var(x)=\begin {pmatrix} var(x_1) &amp; 0 &amp; \cdots &amp; 0\\0 &amp; var(x_2) &amp; \cdots &amp; 0\\\vdots &amp; \vdots &amp; &amp; \vdots\\0 &amp; 0 &amp; \cdots &amp; var(x_p) \end {pmatrix} $$相关系数矩阵若$x=(x_1,x_2,\cdots,x_p)’$和$y=(y_1,y_2,\cdots,y_q)’$分别是p维和q维随机向量，则其相关系数矩阵为$$ \rho (x,y)=\begin {pmatrix} \rho (x_1,y_1) &amp; \rho (x_1,y_2) &amp; \cdots &amp; \rho (x_1,y_q)\\\rho (x_2,y_1) &amp; \rho (x_2,y_2) &amp; \cdots &amp; \rho (x_2,y_q)\\\vdots &amp; \vdots &amp; &amp; \vdots\\\rho (x_p,y_1) &amp; \rho (x_p,y_2) &amp; \cdots &amp; \rho (x_p,y_q) \end {pmatrix} $$若$ \rho (x,y)=0 $，两随机向量相互独立。 2 数据的变换处理数据变换是将原始数据矩阵中的每个元素按照某种特定的运算把它变成为一个新值，而且数值的变化不依赖于原始数据集合中其它数据的新值。事实上多元数据的变换处理通常是为了消除不同量纲的差异。较常用的数据变换如下： 中心化变换中心化变换是一种坐标轴平移处理方法，它是先求出每个变量的样本平均值，再从原始数据中减去该变量的均值，就得到中心化变换后的数据。设原始观测数据矩阵为：$$ X= \begin {bmatrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1q}\\x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2q}\\\vdots &amp; \vdots &amp; &amp; \vdots\\x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nq} \end {bmatrix} $$$$ x_{ij}^*=x_{ij}-\bar x_j (i=1,2,\cdots,n;j=1,2,\cdots,p) $$中心化变换的结果是使每列数据之和均为0，即每个变量的均值为0，而且每列数据的平方和是该列变量样本方差的(n-1)倍，任何不同两列数据之交叉乘积是这两列变量样本协方差的(n-1)倍，所以这是一种很方便地计算方差与协方差的变换。 极差规格化变换极差规格化变换是从数据矩阵的每一个变量中找出其最大值和最小值，这两者之差称为极差，然后从每个变量的每个原始数据中减去该变量中的最小值，再除以极差，就得到规格化数据。$$ x_{ij}^*=\frac{x_{ij}-min_{k=1,2,\cdots,n} (x_{kj})}{R_j} (i=1,2,\cdots,n;j=1,2,\cdots,p) $$ 极差：$$ R_j= max_{i=1,2,\cdots,n} (x_{ij})-min_{i=1,2,\cdots,n} (x_{ij})，0\le x_{ij}^*\le 1 $$经过极差规格化变换后，数据矩阵中每列即每个变量的最大数值为1，最小数值为0，其余数据取值均在0和1之间；并且变换后的数据都不再具有量纲，便于不同的变量之间的比较。 标准化变换标准化变换首先对每个变量进行中心化变换，然后用该变量的标准差进行标准化。$$ x_{ij}^*=\frac{(x_{ij}-\bar x)}{S_j} (i=1,2,\cdots,n;j=1,2,\cdots,p) $$$$ S_j=\sqrt{\frac{1}{n-1}\sum_{i=1}^n(x_{ij}-\bar x_j)^2} $$经过标准化变换处理后，数据矩阵中每列数据即每个变量的平均值为0，方差为1，且不再具有量纲，便于不同变量之间的比较。变换后，数据矩阵中任何两列数据乘积之和是所对应的两个变量相关系数的（ n-1）倍，所以这是一种很方便地计算相关矩阵的变换。 对数变换对数变换是将各个原始数据取对数，将原始数据的对数值作为变换后的新值。$$ x_{ij}^*=log(x_{ij}) $$ 3 聚类分析聚类分析是一种分类技术。与多元分析的其他方法相比，该方法较为粗糙，理论上还不完善，但应用方面取得了很大成功。与回归分析、判别分析一起被称为多元分析的三大方法。聚类的目的——根据已知数据（ 一批观察个体的许多观测指标） ， 按照一定的数学公式计算各观察个体或变量（指标）之间亲疏关系的统计量（距离或相关系数等）。 根据某种准则（ 最短距离法、最长距离法、中间距离法、重心法等），使同一类内的差别较小，而类与类之间的差别较大，最终将观察个体或变量分为若干类。聚类的种类——根据分类的方法可将聚类分析分为：系统聚类、快速聚类、有序聚类。根据分类的对象可将聚类分析分为：Q型——样品聚类clustering for individuals；R型——指标聚类clustering for variables。数据结构 individuals $ X_1 $ $ X_2 $ $\cdots$ $ X_l $ $\cdots$ $ X_p $ 1 28 1.0 $\cdots$ 114 $ \cdots$ 0.15 2 29 2.0 $\cdots$ 117 $\cdots$ 0.20 $ \cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ i $x_{i1}$ $x_{i2}$ $\cdots$ $x_{il}$ $\cdots$ $x_{ip}$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ 47 15 8 $\cdots$ 64 $\cdots$ 0.51 48 16 7.5 $\cdots$ 65 $\cdots$ 0.50 $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ $\cdots$ n $x_{n1}$ $x_{n2}$ $\cdots$ $x_{nl}$ $\cdots$ $x_{np}$ 4 样品间亲疏程度的测度样品间亲疏程度的测度研究样品或变量的亲疏程度的数量指标有两种，一种叫相似系数，性质越接近的变量或样品，它们的相似系数越接近于1，而彼此无关的变量或样品它们的相似系数则越接近于0，相似的为一类，不相似的为不同类；另一种叫距离，它是将每一个样品看作p维空间的一个点，并用某种度量测量点与点之间的距离，距离较近的归为一类，距离较远的点属于不同的类。变量之间的聚类即R型聚类分析，常用相似系数来测度变量之间的亲疏程度。而样品之间的聚类即Q型聚类分析，则常用距离来测度样品之间的亲疏程度。距离假使每个样品有p个变量，则每个样品都可以看成p维空间中的一个点， n个样品就是p维空间中的n个点，则第i样品与第j样品之间的距离记为$d_{ij}$。定义距离的准则定义距离要求满足第i个和第j个样品之间的距离如下四个条件（距离可以自己定义，只要满足距离的这四个条件） $d_{ij}\ge 0$对一切的i和j成立; $d_{ij}=0$当且仅当$x_i=x_j$成立; $d_{ij}=d_{ji}$对一切的i和j成立; $d_{ij}\le d_{ik}+d_{kj}$对于一切的i和j成立。常用距离 明氏距离(Minkowski)设$x_i=(x_{i1},x_{i1},\cdots,x_{ip})’$和$x_j=(x_{j1},x_{j1},\cdots,x_{jp})’$是第i和j个样品的观测值，则二者之间的距离。明氏距离：$$ d_{ij}=(\sum_{k=1}^p \left |x_{ik}-x_{jk}|^q\right)^{\frac{1}{q}} $$欧氏距离：$$ d_{ij}=\sqrt{\sum_{k=1}^p(x_{ik}-x_{jk})^2} $$绝对距离：$$ d_{ij}=\sum_{k=1}^p|x_{ik}-x_{jk}| $$Chebychev距离：$$ d_{ij}=max_{k=1}^p|x_{ik}-x_{jk}| $$明氏距离主要有以下两个缺点明氏距离的值与各指标的量纲有关，而各指标计量单位的选择有一定的人为性和随意性，各变量计量单位的不同不仅使此距离的实际意义难以说清，而且，任何一个变量计量单位的改变都会使此距离的数值改变从而使该距离的数值依赖于各变量计量单位的选择。明氏距离的定义没有考虑各个变量之间的相关性和重要性。实际上，明氏距离是把各个变量都同等看待，将两个样品在各个变量上的离差简单地进行了综合。 兰氏距离(Lance &amp; Williams)这是兰思和维廉姆斯(Lance &amp; Williams)所给定的一种距离，其计算公式为：$$ d_{ij}(L)=\sum_{k=1}^p\frac{|x_{ik}-x_{jk}|}{x_{ik}+x_{jk}} $$这是一个自身标准化的量，由于它对大的奇异值不敏感，这样使得它特别适合于高度偏倚的数据。虽然这个距离有助于克服明氏距离的第一个缺点，但它也没有考虑指标之间的相关性。 马氏距离(Mahalanobis)这是印度著名统计学家马哈拉诺比斯(P.C． Mahalanobis)所定义的一种距离，其计算公式为：$$ d_{ij}^2=(x_i-x_j)’\Sigma^{-1}(x_i-x_j) $$Σ表示观测变量之间的协方差短阵。在实践应用中，若总体协方差矩阵Σ未知，则可用样本协方差矩阵作为估计代替计算。马氏距离又称为广义欧氏距离。显然，马氏距离与上述各种距离的主要不同就是马氏距离考虑了观测变量之间的相关性。如果假定各变量之间相互独立，即观测变量的协方差矩阵是对角矩阵，则马氏距离就退化为用各个观测指标的标准差的倒数作为权数进行加权的欧氏距离。因此，马氏距离不仅考虑了观测变量之间的相关性，而且也考虑到了各个观测指标取值的差异程度。 斜交空间距离由于各变量之间往往存在着不同的相关关系，用正交空间的距离来计算样本间的距离易变形，所以可以采用斜交空间距离。$$ d_{ij}=[\frac{1}{p^2}\sum_{h=1}^p\sum_{k=1}^p(x_{ih}-x_{jh})(x_{ik}-x_{jk})r_{hk}]^{\frac{1}{2}} $$当各变量之间不相关时，斜交空间退化为欧氏距离。 配合距离$$ X_1=(V,Q,S,T,K) X_2=(V,M,S,F,K)$$$$ d_{12}=\frac{m_1}{m_1+m_2}=\frac{不配合数}{配合数+不配合数}=\frac{2}{2+3}=\frac{2}{5} $$适用于分类变量，尤其是名义尺度变量。相似系数研究样品间的关系常用距离，研究指标间的关系常用相似系数。相似系数常用的有夹角余弦和相关系数。 夹角余弦(Cosine)夹角余弦是从向量集合的角度所定义的一种测度变量之间亲疏程度的相似系数。设在n维空间的向量。$$ x_i=(x_{1i},x_{2i},\cdots,x_{ni})’，x_j=(x_{1j},x_{2j},\cdots,x_{nj})’ $$$$ c_{ij}=\cos \alpha_{ij}=\frac{\sum_{k=1}^nx_{ki}x_{kj}}{\sqrt{\sum_{k=1}^nx_{ki}^2\sum_{k=1}^nx_{kj}^2}}，d_{ij}^2=1-c_{ij}^2$$ 相关系数是将数据标准化后的夹角余弦$$ x_i=(x_{1i},x_{2i},\cdots,x_{ni})’，x_j=(x_{1j},x_{2j},\cdots,x_{nj})’ $$$$ c_{ij}=\frac{\sum_{k=1}^n(x_{ki}-\bar x_i)(x_{kj}-\bar x_j)}{\sqrt{\sum_{k=1}^n(x_{ki}-\bar x_i)^2\sum_{k=1}^n(x_{kj}-\bar x_j)^2}} $$距离和相似系数选择的原则一般说来，同一批数据采用不同的亲疏测度指标，会得到不同的分类结果。产生不同结果的原因，主要是由于不同的亲疏测度指标所衡量的亲疏程度的实际意义不同，也就是说，不同的亲疏测度指标代表了不同意义上的亲疏程度。因此我们在进行聚类分析时，应注意亲疏测度指标的选择。通常，选择亲疏测度指标时，应注意遵循的基本原则主要有：所选择的亲疏测度指标在实际应用中应有明确的意义。如在经济变量分析中，常用相关系数表示经济变量之间的亲疏程度。适当地考虑计算工作量的大小。如对大样本的聚类问题，不适宜选择斜交空间距离，因采用该距离处理时，计算工作量太大。亲疏测度指标的选择要综合考虑已对样本观测数据实施了的变换方法和将要采用的聚类分析方法。如在标准化变换之下，夹角余弦实际上就是相关系数；又如若在进行聚类分析之前已经对变量的相关性作了处理，则通常就可采用欧氏距离，而不必选用斜交空间距离。此外，所选择的亲疏测度指标，还须和所选用的聚类分析方法一致。如聚类方法若选用离差平方和法，则距离只能选用欧氏距离。样品间或变量间亲疏测度指标的选择是一个比较复杂且带主观性的问题，我们应根据研究对象的特点作具体分析，以选择出合适的亲疏测度指标。实践中，在开始进行聚类分析时，不妨试探性地多选择几个亲疏测度指标，分别进行聚类，然后对聚类分析的结果进行对比分析，以确定出合适的亲疏测度指标。 5 类与类之间的距离 最短距离法(Nearest Neighbor) 最长距离法(Furthest Neighbor) 重心法(Centroid method) 类平均法(average linkage)——组间连接法(Between-groups Linkage)和组内连接法(Within-groups Linkage) Ward离差平方和法(Ward’s minimumvariance method) 6 系统聚类(hierarchical clustering method)系统聚类的步骤 （1） 开始将n个样品各作为一类。（2） 根据样品的特征，选择合适的距离公式，计算n个样品两两之间的距离，构成距离矩阵。（3） 选择距离矩阵中最小的非对角线元素$d_{pq}$，将相应的两类$G_p$和$G_q$合并为一新类$G_r$={$G_p, G_q$}。（4） 利用递推公式计算新类与当前各类的距离。 分别删除原矩阵的第p，q行和第p，q列，并新增一行和一列添上的结果，产生新的距离矩阵。（5） 再合并、计算，直至只有一类为止。（6） 画聚类图，解释。 最短距离法定义距离：$D_{pq}$=$Min${$d_{ij}：x_i\in G_p， x_j\in G_q$}假设第p类和第q类合并成第r类，第r类与其它各旧类的距离按最短距离法为：递推公式：$D_{rl}$=$Min${$D_{pl}，D_{ql}$} $l\neq p,q$ 最长距离法定义距离：$D_{pq}$=$Max${$d_{ij}：x_i\in G_p， x_j\in G_q$}假设第p类和第q类合并成第r类，第r类与其它各旧类的距离按最长距离法为：递推公式：$D_{rl}$=$Max${$D_{pl}，D_{ql}$} $l\neq p,q$ 中间距离法递推公式：$$ D_{lr}^2=\frac{1}{2}D_{lp}^2+\frac{1}{2}D_{lq}^2-\frac{1}{4}D_{pq}^2 $$$$ D_{kr}^2=\frac{1}{2}D_{kp}^2+\frac{1}{2}D_{kq}^2+\beta D_{pq}^2，-\frac{1}{4}\lt \beta \lt 0 $$ 可变方法如果让中间距离法的递推公式前两项的系数也依赖于β，则递推公式为：$$ D_{kr}^2=\frac{1-\beta}{2}(D_{kp}^2+D_{kq}^2)+\beta D_{pq}^2，\beta \lt 1 $$用上式作为递推公式的系统聚类法称为可变法。 重心法假设第p类和第q类合并成第r类，第r类与其它各旧类的距离按重心法为：$$ D_{rl}^2=\frac{n_p}{n_r}D_{pl}^2+\frac{n_q}{n_r}D_{ql}^2-\frac{n_pn_q}{n_r^2}D_{pq}^2 $$ 类平均方法类平均法定义类间的距离是两类间样品的距离的平均数，递推公式：$$ D_{rk}^2=\frac{n_pD_{kp}^2+n_qD_{kq}^2}{n_p+n_q} $$ 可变类平均法类平均法的递推公式中，没有反映$G_p$类和$G_q$类的距离有多大，进一步将其改进，加入$D_{pq}^2$，并给定系数β&lt;1， 则类平均法的递推公式改为：$$ D_{rl}^2=(1-\beta)\frac{n_pD_{pl}^2+n_qD_{ql}^2}{n_p+n_q}+\beta D_{pq}^2 $$用此递推公式进行聚类就是可变类平均法。递推公式由p类和q类与l类的距离的加权平均数、p类和q类的距离两项的加权和构成，β的大小根据哪项更重要而定。 离差平方和法类似于方差分析的想法，如果类分得恰当，同类内的样品之间的离差平方和应较小，而类间的离差平方和应当较大。当k固定时，选择使SST达到最小的分类。分类可能指数级增长，寻找最优难以完成。离差平方和法的思路：先让n个样品各自成一类，然后缩小一类，每缩小一类离差平方和就要增大，选择使SST增加最小的两类合并，直到所有的样品归为一类为止（局部最优）。定义距离为离差平方和的增量：$$ D_{pq}=S_r^2-S_p^2-S_q^2 $$其中 是由$G_p$和$G_q$合并成的$G_r$类的类内离差平方和。可以证明离差平方和的聚类公式为。递推公式：$$ D_{rk}^2=\frac{n_k+n_p}{n_r+n_k}D_{pk}^2+\frac{n_k+n_q}{n_r+n_k}D_{qk}^2-\frac{n_k}{n_k+n_r}D_{pq}^2 $$以上聚类方法的计算步骤完全相同，仅类与类之间距离的定义不同。 Lance和Williams于1967年将其统一为：$$ D_{MJ}^2=\alpha_KD_{KJ}^2+\alpha_LD_{KJ}^2+\beta D_{KL}^2+\gamma|D_{KJ}^2-D_{KJ}^2| $$ 确定类的个数从系统聚类的计算机结果可以得到任何可能数量的类。但是，聚类的目的是要使各类之间的距离尽可能地远，而类中点的距离尽可能的近， 并且分类结果还要有令人信服的解释。往往做系统聚类的时候，大部分情况下我们都是依靠人的主观判断确定最后分类的个数。这里给出了一些统计方法来确定类的个数。 给定阈值通过观测聚类图， 给出一个合适的阈值T。要求类与类之间的距离要超过T值。 例如我们给定T=0.35， 当聚类时， 类间的距离已经超过了0.35， 则聚类结束。 统计量R²总离差平方和的分解$$ \begin{bmatrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p}\\x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p}\\\vdots &amp; \vdots &amp; &amp; \vdots\\x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np} \end{bmatrix} $$ $$ 总离差平方和=(x_{11}-\bar x_1)^2+\cdots+(x_{n1}-\bar x_1)^2+\cdots+(x_{1p}-\bar x_p)^2+\cdots+(x_{np}-\bar x_p)^2 $$ 如果这些样本被分为两类$$ \begin{bmatrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p}\\x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p}\\\vdots &amp; \vdots &amp; &amp; \vdots\\x_{n_11} &amp; x_{n_12} &amp; \cdots &amp; x_{n_1q} \end{bmatrix} \begin{bmatrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p}\\x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p}\\\vdots &amp; \vdots &amp; &amp; \vdots\\x_{n_21} &amp; x_{n_22} &amp; \cdots &amp; x_{n_2p} \end{bmatrix} $$$$ 一组离差平方和=(x_{11}-\bar x_1^{(1)})^2+\cdots+(x_{n_11}-\bar x_1^{(1)})^2+\cdots+(x_{1p}-\bar x_p^{(1)})^2+\cdots+(x_{n_1p}-\bar x_p^{(1)})^2 $$$$ 二组离差平方和=(x_{11}-\bar x_1^{(2)})^2+\cdots+(x_{n_11}-\bar x_1^{(2)})^2+\cdots+(x_{1p}-\bar x_p^{(2)})^2+\cdots+(x_{n_1p}-\bar x_p^{(2)})^2 $$可以证明：总离差平方和＝组内离差平方和＋组间离差平方和。令T为总离差平方和，令$P_G$为分为G类的组内离差平方和。统计量$$ R^2=1-\frac{P_G}{T} $$其中T是数据的总离差平方和， 是组内离差平方和。R²比较大，说明分G个类时组内离差平方和比较小，也就是说分G类是合适的。但是，分类越多，每个类的组内离差平方和就越小，R²也就越大；所以我们只能取合适的G，使得R²足够大，而G本身很小，随着G的增加，R²的增幅不大。比如，假定分4类时，R²=0.8；下一次合并分三类时，下降了许多，R²=0.32，则分4类是合适的。 伪F统计量伪F统计量的定义为$$ F=\frac{(T-P_G)/(G-1)}{P_G/(n-G)} $$伪F统计量用于评价聚为G类的效果。如果聚类的效果好，类间的离差平方和相对于类内的离差平方和大，所以应该取伪F统计量较大而类数较小的聚类水平。 伪t²统计量伪t²统计量的定义为$$ t^2=\frac{B_{KL}}{(W_K+W_L)/(N_K+N_L-2)} $$其中$W_L$和$W_K$分别是类内离差平方和， $W_M$是将K和L合并为第M类的离差平方和，$B_{KL}=W_M-W_K-W_L$为合并导致的类内离差平方和的增量。用它评价合并第K和L类的效果，伪t²统计量大说明不应该合并这两类，应该取合并前的水平。 7 快速聚类(k-means clustering method)系统聚类法的缺陷——系统聚类法是一种比较常用的聚类方法。然而当样本点数量十分庞大时，则是一件非常繁重的工作，聚类的计算速度也比较慢。比如在市场抽样调查中，有4万人就其对衣着的偏好作了回答，希望能迅速将他们分为几类。这时， 用系统聚类法计算的工作量极大，作出的树状图也十分复杂，不便于分析。quick cluster method， k-means method也叫动态聚类、逐步聚类、迭代聚类、k-均值聚类，快速聚类适用于大型数据。快速聚类（K-means）流程图 用一个简单的例子来说明快速聚类法的工作过程。例如我们要把图中的点分成两类。 快速聚类的步骤：1、随机选取两个点$x_1^{(1)}$和$x_2^{(1)}$作为聚核。2、对于任何点$x_k$，分别计算$d(x_k,x_1^{(1)})$和$d(x_k,x_2^{(1)})$3、若$d(x_k,x_1^{(1)})\lt d(x_k,x_2^{(1)})$，则将$x_k$划为第一类，否则划给第二类。于是得图（b）的两个类。4、分别计算两个类的重心，则得$x_1^{(2)}$和$x_2^{(2)}$，以其为新的聚核，对空间中的点进行重新分类，得到新分类。 选择凝聚点初始凝聚点（聚类种子、initial cluster seeds/clustercenters）就是一批有代表性的点，是欲形成类的中心。初始凝聚点的选择直接决定初始分类，对分类结果也有很大的影响，由于凝聚点的不同选择，其最终分类结果也将出现不同，故选择时要慎重。通常选择初始凝聚点的方法有：人为选择，当人们对所欲分类的问题有一定了解时，根据经验，预先确定分类个数和初始分类，并从每一类中选择一个有代表性的样品作为凝聚点。将数据人为地分为A类，计算每一类的重心，就将这些重心作为凝聚点。用密度法选择凝聚点。以某个正数d为半径，以每个样品为球心，落在这个球内的样品数（不包括作为球心的样品）就叫做这个样品的密度。计算所有样品点的密度后，首先选择密度最大的样品作为第一凝聚点，并且人为地确定一个正数D（一般D＞d，常取D＝2d）。然后选出次大密度的样品点，若它与第一个凝聚点的距离大于D，则将其作为第二个凝聚点；否则舍去这点，再选密度次于它的样品。这样，按密度大小依次考查，直至全部样品考查完毕为止．此方法中，d要给的合适，太大了使凝聚点个数太少，太小了使凝聚点个数太多。人为地选择一正数d，首先以所有样品的均值作为第一凝聚点。然后依次考察每个样品，若某样品与已选定的凝聚点的距离均大于d，该样品作为新的凝聚点，否则考察下一个样品。随机地选择，如果对样品的性质毫无所知，可采用随机数表来选择，打算分几类就选几个凝聚点。或者就用前A个样品作为凝聚点（假设分A类）。这方法一般不提倡使用。 衡量聚类结果的合理性指标设$P_i^n$表示在第n次聚类后得到的第i类集合，$i=1,2,3,\cdots,k,A_i^{(n)}$为第n次聚类所得到的聚核。定义：$$ u_n\triangleq \sum_{i=1}^k\sum_{x\in P_i^n}d^2(x,A_i^{(n)}) $$为所有K个类中所有元素与其重心的距离的平方和。若分类不合理时，$u_n$会很大，随着分类的过程，逐渐下降并趋于稳定。算法终止的标准定义算法终止的标准是：$$ \frac{|u_{n+1}-u_n|}{u_{n+1}}\le \varepsilon $$ε是事前给定的一个充分小量。快速聚类步骤第一，选择若干个观测值点为“凝聚点”；第二，通过分配每个“凝聚点”最近的类来形成临时分类。每一次对一个观测值点进行归类，“凝聚点”更新为这一类目前的均值；所有的观测值点分配完后，这些类的“凝聚点”用临时类的均值代替；该步骤可以一直进行直到“凝聚点”的改变很小或为零时止；第三，最终的分类由分配每一个观测到最近的“凝聚点”而形成。 8 有序聚类有序样本聚类法 有序样本聚类法又称为最优分段法。该方法是由费歇在1958年提出的。它主要适用于样本由一个变量描述，或者将多变量综合成为一个变量来分析的情况。对于有序样本聚类，实际上是需要找出一些分点，将它们划分为几个分段，每个分段看作一类，这样的分类又称分割。分点位置不同得到的分割不同，有序样本聚类是要找到一个分割使得各段内部样本差异很小，而各段之间样本的差异很大。有序样本聚类法常常被用于系统的评估问题，被用来对样本点进行分类划级。这种行政上的规定往往是不客观、不合理的。合理的分类应该把发展情况最近似的地区划入同一类。这就是有序样本聚类的工作思路。系统聚类开始n个样品各自自成一类，然后逐步并类，直至所有的样品被聚为一类为止。而有序聚类则相反，开始所有的样品为一类，然后分为二类、三类等，直到分成n类。每次分类都要求产生的离差平方和最小。 有序样本聚类算法步骤设有序样品$x_{(1)},x_{(2)},\cdots,x_{(n)}$。它们可以是从小到大排列，也可以是按时间的先后排列。 定义类的直径；设某类G中包含的样品有{$ x_{(i)},x_{(i+1)},\cdots,x_{(j)} $} $(j\gt i)$该类的均值向量为$$ \bar X_G=\frac{1}{j-i+1}\sum_{i=1}^jx_{(t)} $$用D(i,j)表示这一类的直径，常用的直径有欧氏距离：$$ D(i,j)=\sum_{t=i}^j(x_{(t)}-\bar X_G)’(x_{(t)}-\bar X_G) $$当是单变量时，也可以定义直径为：$$ D(i,j)=\sum_{t=i}^j|x_{(t)}-\tilde X_G|，其中\tilde X_G是中位数$$ 定义分类的损失函数L[p(n,k)]；用b(n,k)表示将n个有序的样品分为k类的某种分法($j_1$=1)：$$ \begin{array}{lcl}G_1=[j_1,j_1+1,\cdots,j_2-1]\\G_2=[j_2,j_2+1,\cdots,j_3-1]\\\cdots\qquad\cdots\\G_k=[j_k,j_k+1,\cdots,n] \end{array} $$定义这种分类法的损失函数为各类的直径之和。$$ L[b(n,k)]=\sum_{t=1}^kD(j_t,j_{t+1}-1) $$由损失函数的构造可以看出，损失函数是各类的直径之和。如果分类不好，则各类的直径之和大，否则比较小。当n和k固定时， L[b(n,k)]越小表示各类的离差平方和越小，分类是合理的。因此要寻找一种分法b(n,k)，使分类损失函数L[b(n,k)]达到最小。记该分法为p[n,k]。 L[p(n,k)]的递推公式；$$ \begin{cases}L[p(n,2)]=\min_{2\le j\le n}(D(1,j-1)+D(j,n))\\L[p(n,k)]=\min_{k\le j\le n}(L[p(j-1,k-1)]+D(j,n)) \end{cases} $$以上的两个公式的含义是，如果要找到n个样品分为k个类的最优分割，应建立在将j-1（j＝2,3,…,n)个样品分为k-1类的最优分割的基础上。 寻找最优解。 9 聚类分析的主要步骤 （1）选择变量和聚类分析的目的密切相关；在不同研究对象上的值有明显的差异；变量之间不能高度相关。（2）计算相似性相似性是聚类分析中的基本概念，它反映了研究对象之间的亲疏程度，聚类分析就是根据对象之间的相似性来分类的。（3）聚类选定了聚类的变量，计算出样品或指标之间的相似程度后，构成了一个相似程度的矩阵。这时主要涉及两个问题：选择聚类的方法和确定形成的类数。（4）聚类结果的解释和证实对聚类结果进行解释是希望对各个类的特征进行准确的描述，给每类起一个合适的名称。这一步可以借助各种描述性统计量进行分析，通常的做法是计算各类在各聚类变量上的均值，对均值进行比较，还可以解释各类差别的原因。（5）有关问题几种聚类方法获得的结果不一定相同，指标聚类采用相似系数，相似系数大或距离小则表示类间关系密切。为了统一，可采用以下公式变换：$$ d_{ij}^2=1-r_{ij}^2 $$（6）变量聚类分析对于变量聚类分析，聚类分析做完之后，各类中有较多的指标。 为了达到降维的目的， 需要在每类中选出一个代表指标。 具体做法是：假设某类中有k个指标， 首先分别计算类内指标之间的相关指数， 然后计算某个指标与类内其它指标之间相关指数的平均数， 即$$ \bar R_i^2=\frac{\sum_{i\neq j}r_{ij}}{k-1} $$取$\bar R_i^2$最大的$x_i$，作为该类的代表。 10 R语言中聚类分析实现R语言自带的聚类分析函数包括了hclust和k-means。所以本篇主要介绍这两个函数的使用。而首先hclust是基于距离进行的聚类分析，所以事实上在做层次聚类的时候，第一步是先计算距离。当然前期说明下，这里的样例数据是北京市12个大气污染监测站点在2017年6月7日和6月8日全天的PM2.5数据（数据来自笔者自己写的代码获取而得，调用了环境云的API），样例数据连同完整的代码会在笔记写完后统一给出。 环境云官网：http://www.envicloud.cn/ 数据： 12dist.pm25&lt;-dist(airnew[,-1],method=&apos;euclidean&apos;)heatmap(as.matrix(dist.pm25),labRow=stationname,labcol=F) 我们做的分析是对一天内24小时下12个站点的PM2.5聚类分析。所以这个问题的多元变量，是不同时间段的PM2.5值，前期已经把数据结构也已经成功做成矩阵形式，接下来就需要计算距离了。距离矩阵在R里面是比较好求取的。dist函数。dist函数的参数事实上有不少，但是其实一般重点用的就是输入矩阵的参数（代码中的airnew[,-1]，-1代表去掉第一列数据（站点名称）），还有计算距离的方式——method。这里选的是欧氏距离。这个参数的可选取值还包括maximum（最大距离）、manhattan（曼哈顿距离）、canberra（兰氏威廉姆斯距离）、binary（定性距离，其实就是配合距离）、minkowski（闵可夫斯基距离——明氏距离）。还有用得多些的参数——diag和upper。diag为TRUE的时候给出对角线上的距离。upper为TURE的时候给出上三角矩阵上的值。默认都是FALSE。函数计算完之后得到的是一个距离矩阵。我们用热力图的方式进行可视化，这就是上面的第二句代码。heatmap函数是个热图可视化函数，要求输入一个矩阵。labRow其实是输入列名，labcol是与labRow相关，用来映射输入的值的。结果如下图。 计算完矩阵，即可进行聚类分析了。hclust函数的必要参数与前面距离的参数类似——输入矩阵参数，方法参数（这里聚类的方法前面也有提到，这里就不赘述了，有兴趣的可以自己看官方帮助文档）。而聚类完的结果存储在model1里面，用plot即可画出聚类谱系图。事实上，plclust也是相同的作用，参数基本是统一的，labels填写我们聚类的变量。而聚类完的结果则可以用cutree来获得，输入的model1——聚类结果，k是要求的类数。 123model1=hclust(dist.pm25,method=&quot;ward&quot;)plot(model1,labels=stationname,hang=-1,las=1)plclust(model1,labels=stationname,hang=-1) 对聚类结果做个简单可视化。以0点和1点的PM2.5值分别为x和y轴，以聚类结果做划分。 12result=cutree(model1,k=3)plot(airnew[,2],airnew[,3],col=result,pch=as.integer(result)) 接下来是K-means的方法。函数并不复杂，输入数据框或者矩阵（做聚类的数据），center就是聚类数，nstart是迭代次数。迭代次数高，聚类可信度高些。后面的这个函数是聚类可视化的函数，是fpc包下面的，使用前请先确认是否安装。12kres&lt;-kmeans(airnew[,-1],centers=3,nstart=10)plotcluster(airnew[,-1],kres$cluster) 对比了二者的分类结果，是一致的。 聚类结束后，我们就这个数据和结果做些简单的分析。事实上作为地学人员，我们就简单地画个站点分布图来对应看看具体情况。从这张图来看，PM2.5的聚类结果显示了它具有很好的空间分异性。当然下面的图有点简陋，给出一个对比的，基于leaflet和R Notebook的交互式小地图（老规矩）。 st=>start: 选择凝聚点（聚类中心） e=>end: 分类结果 sub=>subroutine: 分类 cond=>condition: 分类结果是否合理？ op=>operation: 调整分类 st->sub->cond(yes)->e cond(no)->op(right)->sub{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);]]></content>
      <categories>
        <category>Statistics &amp; Model</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用统计学与R语言实现学习笔记（九）——线性回归]]></title>
    <url>%2F2017%2F06%2F13%2F%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8ER%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[Chapter 9 Linear Regression本篇是第九章，内容是回归分析（主要以线性回归为主）。回归分析是数理统计、数理分析中最基础（也可以说是最重要）的一个分析，所以这一章内容相对来说也较多。 1 变量间的关系 确定型关系vs不确定型关系函数关系——一一对应的确定型关系设有两个变量x和y，变量y随变量x一起变化， 并完全依赖于x，当变量x取某个数值时，y依确定的关系取相应的值，则称y是x的函数，记为y=f(x)，其中x称为自变量，y称为因变量各观测点落在一条线上。相关关系(correlation)——变量间关系不能用函数关系精确表达。一个变量的取值不能由另一个变量唯一确定。当变量x取某个值时， 变量y的取值可能有几个。各观测点分布在直线周围。 相关关系包括了线性相关（正相关、负相关）、非线性相关、完全相关（正相关、负相关）、不相关。 除了如上的图，可以看下面的链接——关于相同统计量不同数据的一篇外文。 https://www.autodeskresearch.com/publications/samestats 相关系数(correlation coefficient) 对变量之间关系密切程度的度量（只关心密切程度，无关因果关系）； 对两个变量之间线性相关程度的度量称为简单相关系数； 若相关系数是根据总体全部数据计算的，称为总体相关系数，记为ρ； 若是根据样本数据计算的，则称为样本相关系数，记为 r。 总体相关系数的计算公式：$$ \rho=\frac{\sigma_{xy}}{\sigma_x\sigma_y}=\frac{E[(X-E(X))(Y-E(Y))]}{\sqrt{E(X-E(X))^2}\sqrt{E(Y-E(Y))^2}} $$相关系数特点 无量纲(Unitfree)； ρ的取值范围是 [-1,1]； |ρ|=1，为完全相关（ρ=1为完全正相关；ρ=-1为完全负相关）； ρ=0，不存在线性相关关系； -1≤ρ&lt;0，为负相关，0&lt;ρ≤1，为正相关； |ρ|越趋于1表示线性关系越密切；|ρ|越趋于0表示线性关系越不密切； 若X与Y相互独立，则ρ=0，但ρ=0，X与Y不一定相互独立； 若ρ= 0，且X与Y服从正态分布，则X与Y相互独立。 样本相关系数计算公式：$$ r=\frac{\sum(x_i-\bar x)(y_i-\bar y)}{\sqrt{\sum(x_i-\bar x)^2\cdot\sum(y_i-\bar y)^2}}或r=\frac{n\sum x_iy_i-\sum x_i\sum y_i}{\sqrt{n\sum x_i^2-(\sum x_i)^2}\cdot\sqrt{n\sum x_i^2-(\sum x_i)^2}} $$样本相关系数特点 无量纲(Unitfree)； r的取值范围是 [-1,1]； |r|=1，为完全相关（r=1为完全正相关；r=-1为完全负相关）； r=0，不存在线性相关关系； -1≤r&lt;0为负相关，0&lt;r≤1为正相关； |r|越趋于1表示线性关系越密切；|r|越趋于0表示线性关系越不密切； 对变量之间关系密切程度的度量，只关心密切程度，无关因果关系。比如撑伞的人数和降雨量的相关系数非常高。但是我们不能说因为撑伞的人多了，所以降雨量大。 r的抽样分布r的抽样分布随总体相关系数和样本容量的大小而变化。当样本数据来自服从正态分布的总体时，随着n的增大，r的抽样分布趋于正态分布，尤其是在总体相关系数ρ很小或接近0时，趋于正态分布的趋势非常明显。而当ρ远离0时，除非n非常大，否则r的抽样分布呈现一定的偏态。当ρ为较大的正值时， r呈现左偏分布；当ρ为较小的负值时， r 呈现右偏分布。只有当ρ接近于0，而样本容量n很大时，才能认为r是接近于正态分布的随机变量。相关系数的显著性检验步骤检验两个变量之间是否存在线性相关关系，等价于对回归系数$β_1$的检验。采用R. A. Fisher提出的t检验。检验的步骤为： （1） 提出假设：$H_0：\rho=0；H_1：\rho \neq0$（2） 计算检验的统计量： $t=r\sqrt{\frac{n-2}{1-r^2}}\sim t(n-2)$（3） 确定显著性水平α，并作出决策。 若$|t|&gt;t_{α/2}$，拒绝$H_0$。 若$|t|&lt;t_{α/2}$，不能拒绝$H_0$。 2 回归分析和简单线性回归分析2.1 回归分析什么是回归分析(Regression)? 从一组样本数据出发，确定变量之间的数学关系式。对这些关系式的可信程度进行各种统计检验，并从影响某一特定变量的诸多变量中找出哪些变量的影响显著， 哪些不显著。利用所求的关系式，根据一个或几个变量的取值来预测或控制另一个特定变量的取值， 并给出这种预测或控制的精确程度。 回归分析与相关分析的区别 相关分析中，变量x变量y处于平等的地位；回归分析中，变量y称为因变量，处在被解释的地位，x称为自变量，用于预测因变量的变化；相关分析中所涉及的变量x和y都是随机变量；回归分析中，因变量y是随机变量，自变量x可以是随机变量，也可以是非随机的确定变量；相关分析主要是描述两个变量之间线性关系的密切程度；回归分析不仅可以揭示变量x对变量y的影响大小，还可以由回归方程进行预测和控制。 回归模型(regression model)——回答“变量之间是什么样的关系？”方程中运用1个数值型因变量(响应变量)作为被预测的变量；1个或多个数值型或分类型自变量 (解释变量)作为用于预测的变量。主要用于预测和估计。回归模型的类型包括一元回归模型（线性和非线性）和多元回归模型（线性和非线性）。接下来先从简单线性回归分析讲起。 2.2 简单线性回归分析简单线性回归(Simple Linear Regression)——涉及一个自变量的回归，因变量y与自变量x之间为线性关系。被预测或被解释的变量称为因变量(dependent variable)，用y表示；用来预测或用来解释因变量的一个或多个变量称为自变量(independent variable)，用x表示。因变量与自变量之间的关系用一个线性方程来表示。描述因变量y如何依赖于自变量x和误差项ε的方程称为回归模型(Regression Model，定义如前)。（1）简单线性回归模型的表示形式$$ y=\beta_0+\beta_1 x+\varepsilon $$y是x的线性函数(部分)加上误差项(residual/random error term)。线性部分反映了由于x的变化而引起的y的变化。误差项ε是随机变量。反映了除x和y之间的线性关系之外的随机因素对y的影响，是不能由x和y之间的线性关系所解释的变异性。$β_0$和$β_1$称为模型的参数(interception, slope)。（2）简单线性回归模型的基本假定误差项ε是一个期望值为0的随机变量，即E(ε)=0。对于一个给定的x值，y的期望值为$$ E(y)=\beta_0+\beta_1x $$对于所有的x值，ε的方差$σ^2$都相同；误差项ε是一个服从正态分布的随机变量，且相互独立。即ε~N(0,$σ^2$);独立性意味着对于一个特定的x值，它所对应的ε与其他x值所对应的ε不相关；对于一个特定的x值， 它所对应的y值与其他x所对应的y值也不相关。（3）简单线性回归方程(regression equation)描述y的平均值或期望值如何依赖于x的方程称为回归方程；简单线性回归方程的形式如下$$ E(y)=\beta_0+\beta_1x $$方程的图示是一条直线，也称为直线回归方程。$β_0$是回归直线在y轴上的截距(interception)，是当x=0时y的期望值。$β_1$是直线的斜率(slope)，称为回归系数，表示当x每变动一个单位时，y的平均变动值。（4）估计的回归方程(estimated regression equation)总体回归参数$β_0$和$β_1$是未知的，必须利用样本数据去估计。用样本统计量$b_0$和$b_1$代替回归方程中的未知参数$β_0$和$β_1$，就得到了估计的回归方程。简单线性回归中估计的回归方程为$$ \hat y=b_0+b_1x $$其中：$b_0$是估计的回归直线在y轴上的截距，$b_1$是直线的斜率，也表示x每变动一个单位时，y的平均变动值，$\hat y$表示一个给定的x的值对应的y的估计值。（5）最小二乘估计使因变量的观察值与估计值之间的离差平方和达到最小来求得$b_0$和$b_1$的方法。即$$ argmin \sum_{i=1}^n(y_i-\hat y_i)^2=\sum_{i=1}^n(y_i-b_0-b_ix_i)^2 $$用最小二乘法拟合的直线来代表x与y之间的关系与实际数据的误差平方和比其他任何直线都小。根据最小二乘法的要求，可得到如下的公式：$$ \begin{cases}b_1=\frac{n\sum_{i=1}^nx_iy_i-(\sum_{i=1}^nx_i)(\sum_{i=1}^ny_i)}{n\sum_{i=1}^nx_i^2-(\sum_{i=1}^nx_i)^2}\\b_0=\bar y-b_1\bar x\end{cases} $$最小二乘估计的性质 所有残差的和为0。所有残差的平方和最小； 回归直线经过变量X与Y的均值； 是$β_0和β_1$的无偏估计。 在r语言中，简单线性回归的代码如下：1modele&lt;-lm(e~a) （7）回归直线的拟合优度变差因变量 y 的取值是不同的， y 取值的这种波动称为变差。 变差来源于两个方面： 由于自变量 x 的取值不同造成的。 除 x 以外的其他因素(如x对y的非线性影响、测量误差等)的影响。对一个具体的观测值来说， 变差的大小可以通过该实际观测值与其均值之差$y-\bar y$来表示。 离差平方和的分解(三个平方和的关系与意义)$$ \sum_{i=1}^n(y_i-\bar y)^2=\sum_{i=1}^n(\hat y_i-\bar y)^2+\sum_{i=1}^n(y_i-\hat y)^2 $$从左至右分别为SST，SSR，SSE。所以就有SST=SSR+SSE。总平方和(SST)——反映因变量的 n 个观察值与其均值的总离差；回归平方和(SSR)——反映自变量 x 的变化对因变量 y 取值变化的影响，或者说，是由于x与y之间的线性关系引起的y的取值变化，也称为可解释的平方和；残差平方和(SSE)——反映除x以外的其他因素对y取值的影响，也称为不可解释的平方和或剩余平方和。 判定系数R²(coefficient of determination)回归平方和占总离差平方和的比例。$$R^2=\frac{SSR}{SST}=\frac{\sum_{i=1}^n(\hat y_i-\bar y)^2}{\sum_{i=1}^n(y_i-\bar y)^2}=1-\frac{\sum_{i=1}^n(y_i-\hat y)^2}{\sum_{i=1}^n(\hat y_i-\bar y)^2}$$ 反映回归直线的拟合程度； 取值范围在[0,1]之间； $R^2\rightarrow1$，说明回归方程拟合的越好；$R^2\rightarrow0$，说明回归方程拟合的越差； 对简单线性回归，判定系数等于相关系数的平方，r=(b1的符号)sqrt(R²)。 估计标准误差(standard error of estimate) 实际观察值与回归估计值离差平方和的均方根； 反映实际观察值在回归直线周围的分散状况； 对误差项ε的标准差σ的估计， 是在排除了x对y的线性影响后，y随机波动大小的一个估计量； 反映用估计的回归方程预测y时预测误差的大小。计算公式为$$ s=\sqrt{\frac{\sum_{i=1}^n(y_i-\hat y_i)^2}{n-2}}=\sqrt{\frac{SSE}{n-2}}=\sqrt{MSE} $$ 显著性检验 线性关系的显著性检验：检验自变量与因变量之间的线性关系是否显著，即检验x与y之间是否具有线性关系，或者说，检验自变量x对因变量y的影响是否显著； 回归系数的显著性检验：检验回归系数是否不等于0； 在简单线性回归中，线性关系的显著性检验等价于回归系数的显著性检验。线性关系的检验将回归均方(MSR)同残差均方(MSE)加以比较， 应用F检验来分析二者之间的差别是否显著。回归均方：回归平方和SSR除以相应的自由度(自变量的个数p)；残差均方：残差平方和SSE除以相应的自由度(n-p-1)。 提出假设：$H_0：\beta_1=0$ 线性关系不显著； 计算检验统计量F：$$ F=\frac{SSR/1}{SSE/(n-2)}=\frac{MSR}{MSE}\sim F(1,n-2) $$ 确定显著性水平α，并根据分子自由度1和分母自由度n-2找出临界值$F_\alpha$。 作出决策：$若F&gt;F_\alpha，拒绝H_0； 若F&lt;F_\alpha，不拒绝H_0$。回归系数的检验(检验步骤) 提出假设$$H_0:\beta_1=0(没有线性关系)$$$$H_1:\beta_1\neq 0(有线性关系)$$ 计算检验的统计量$$ t=\frac{b_1}{s_{b_1}}\sim t(n-2) $$ 确定显著性水平$\alpha$，并进行决策：$$ \left|t\right|\gt t_{\alpha/2}，拒绝H_0；\left|t\right|\lt t_{\alpha/2}，不拒绝H_0 $$显著性检验的几点注意显著性关系的结论不意味着因果关系。显著性关系的结论也不能推出线性关系的结论，仅能说在x的样本观测之范围内，x和y是相关的，而且一个线性关系只揭示了y的变异的主要部分。当样本容量很大时，对于小的b1值也能得到统计上是显著的结果。 3 利用回归方程进行估计和预测根据自变量x的取值估计或预测因变量y的取值。估计或预测的类型 （1）点估计：y的平均值的点估计，y的个别值的点估计；（2）区间估计：y的平均值的置信区间估计，y的个别值的预测区间估计。 （1）点估计对于自变量x的一个给定值$x_0$，根据回归方程得到因变量y的一个估计值$\hat y_0$。点估计值有y的平均值的点估计和y的个别值的点估计。在点估计条件下，平均值的点估计和个别值的的点估计是一样的，但在区间估计中则不同。 y的平均值的点估计利用估计的回归方程， 对于自变量 x 的一个给定值$x_0$，求出因变量y的平均值的一个估计值$E(y_0)$，就是平均值的点估计。y的个别值的点估计利用估计的回归方程，对于自变量x的一个给定值$x_0$，求出因变量y的一个个别值的估计值$\hat y_0$，就是个别值的点估计。 （2）区间估计点估计不能给出估计的精度， 点估计值与实际值之间是有误差的， 因此需要进行区间估计。对于自变量x的一个给定值$x_0$，根据回归方程得到因变量y的一个估计区间。区间估计有两种类型：置信区间估计(confidence interval estimate)和预测区间估计(prediction interval estimate)。置信区间估计利用估计的回归方程，对于自变量x的一个给定值$x_0$，求出因变量y的平均值的估计区间，这一估计区间称为置信区间(confidence interval)。$E(y_0)$在$1-\alpha$置信水平下的置信区间为:$$ \hat y_0\pm t_{\alpha/2}(n-2)s\sqrt{\frac{1}{n}+\frac{(x_0-\bar x)^2}{\sum_{i=1}^n(x_i-\bar x)^2}} $$式中s为估计标准误差。x=均值时能得到y的平均值的最精确估计。预测区间估计利用估计的回归方程,对于自变量x的一个给定值$x_0$,求出因变量y的一个个别值的估计区间，这一区间称为预测区间(prediction interval)。$E(y_0)$在$1-\alpha$置信水平下的预测区间为:$$ \hat y_0\pm t_{\alpha/2}(n-2)s\sqrt{1+\frac{1}{n}+\frac{(x_0-\bar x)^2}{\sum_{i=1}^n(x_i-\bar x)^2}} $$影响区间宽度的因素 置信水平(1-α)——区间宽度随置信水平的增大而增大； 数据的离散程度s——区间宽度随离散程度的增大而增大； 样本容量——区间宽度随样本容量的增大而减小； 用于预测的$x_p$与$\bar x$的差异程度，区间宽度随$x_p$与$\bar x$的差异程度的增大而增大。 其实在R语言里主要用predict.lm函数来进行区间估计。代码样例如下：1con&lt;-predict.lm(modele,h,interval="confidence",level=0.95) 其中interval控制是置信区间（参数填confidence）、预测区间（参数填prediction）或者是不做区间估计，level是置信水平，接着用R绘制一个简单的回归和置信区间的图，这里先给出如何绘制置信区间band的代码，完整代码还是老规矩，在这一部分笔记写完后给出。1polygon(c(h[,1], rev(h[,1])), c(con[,3], rev(con[,2])),border="red",lwd=1,lty = c("dashed", "solid")) 4 残差分析残差(residual)——因变量的观测值与根据估计的回归方程求出的预测值之差，用e表示。$$e_i=y_i-\hat y_i$$反映了用估计的回归方程去预测而引起的误差。残差检验的目的 检验线性的假设是否成立； 确定有关误差项ε的假定是否成立（正态分布；方差为常数；独立性）。 检测有影响的观测值。 残差图(residual plot) 表示残差的图形（关于x的残差图，关于y的残差图，标准化残差图）。 用直方图或正态概率图检验正态性。 标准化残差(standardized residual) 残差除以它的标准差后得到的数值。 计算公式为$$ z_{e_i}=\frac{e_i}{s_{e_i}}=\frac{y_i-\hat y_i}{s_{e_i}} $$ $e_i$是第i个残差的标准差， 其计算公式为$$ s_{e_i}=s_y\sqrt{1-h_i}=s_y\sqrt{1-(\frac{1}{n}+\frac{(x_i-\bar x)^2}{\sum(x_i-\bar x)^2})} $$ 标准化残差图用以直观地判断误差项服从正态分布这一假定是否成立。 若假定成立， 标准化残差的分布也应服从正态分布。 在标准化残差图中， 大约有95%的标准化残差在-2到+2之间。 变换数据变换的问题在前面第七章拟合优度检验提过，那么什么时候做变换?如果从散点图观察发现残差是自变量的函数，通过变换可能可以解决问题。做什么变换？观察残差与因变量观测值的均值的关系： 如果残差的标准差与因变量观测值的均值有线性关系，用log变换； 如果残差的方差与因变量观测值的均值有线性关系，用square root变换； 如果残差的标准差与因变量观测值的均值的平方有线性关系，用inverse变换； 如果残差的标准差与因变量观测值的均值的幂有线性关系，用power变换。 序列相关（自相关）当数据是按时间顺序采集的，有可能引起误差项之间的相关(Serial correlation,autocorrelation)。这里介绍一个相关的杜宾-瓦特森(Durbin-Watson)检验统计量：$$ d=\frac{\sum_{t=2}^n(e_t-e_{t-1})^2}{\sum_{t=1}^ne_t^2} $$是否遗漏了重要的对因变量有时序影响的自变量，有时可通过引入度量观测次数的自变量解决该问题。这部分属于时间序列分析的范畴，这里就不进一步阐述了。 在R语言中，线性回归方程残差图绘制非常简单。模型拟合过程会自动给出四个残差可视化相关的图。绘制方法如下：12layout(matrix(c(1,2,3,4),nrow=2,byrow=T))plot(modele) 结果如图 异常值(outlier)与识别如果某一个点与其他点所呈现的趋势不相吻合，这个点就有可能是异常点。 如果异常值是一个错误的数据， 比如记录错误造成的， 应该修正该数据， 以便改善回归的效果； 如果是由于模型的假定不合理， 使得标准化残差偏大， 应该考虑采用其他形式的模型，比如非线性模型； 如果完全是由于随机因素而造成的异常值， 则应该保留该数据。 在处理异常值时， 若一个异常值是一个有效的观测值， 不应轻易地将其从数据集中予以剔除。 异常值也可以通过标准化残差来识别； 如果某一个观测值所对应的标准化残差较大， 就可以识别为异常值； 一般情况下，当一个观测值所对应的标准化残差小于-2或大于+2时，就可以将其视为异常值。 有影响的观测值如果某一个或某一些观测值对回归的结果有强烈的影响，那么该观测值或这些观测值就是有影响的观测值。一个有影响的观测值可能是：一个异常值， 即有一个值远远偏离了散点图中的趋势线；对应一个远离自变量平均值的观测值；或者是这二者组合而形成的观测值。如果有影响的观测值是一个错误的数据，比如记录错误造成的， 应该修正该数据，以便改善回归的效果。如果有影响的观测值是一个有效的数据则应该保留它， 可以帮助我们分析模型的假定是否合理。杠杆率点(leverage point)如果自变量存在一个极端值， 该观测值则称为高杠杆率点(high leverage point)，在简单回归中，第i个观测值的杠杆率用$h_i$表示，其计算公式为：$$h_i=\frac{1}{n}+\frac{(x_i-\bar x)^2}{\sum(x_i-\bar x)^2}$$如果一个观测值的杠杆率$h_i&gt;n/6$，就可以将该观测值识别为有高杠杆率的点；一个有高杠杆率的观测值未必是一个有影响的观测值， 它可能对回归直线的斜率没有什么影响。 5 多元线性回归(multiple regression model)多元线性回归(multiple regression model) 一个因变量与两个及两个以上自变量的回归。 描述因变量y如何依赖于自变量$x_1,x_2,\cdots,x_p$和误差项$\varepsilon$的方程，称为多元回归模型。 涉及 p 个自变量的多元回归模型可表示为$$y=\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_px_p+\varepsilon$$ $\beta_0,\beta_1,\beta_2,\cdots,\beta_p$是参数。 $\varepsilon$是被称为误差项的随机变量。 y是$x_1,x_2,\cdots,x_p$的线性函数加上误差项$\varepsilon$。 $\varepsilon$包含在y里面但不能被p个自变量的线性关系所解释的变异性。 多元回归模型的基本假定 误差项ε是一个期望值为0的随机变量， 即E(ε)=0。 对于自变量$x_1,x_2,\cdots,x_p$的所有值，ε的方差$\sigma^2$都相同。 误差项ε是一个服从正态分布的随机变量，即$ε~N(0,\sigma^2)$，且相互独立。 多元回归方程(multiple regression equation)描述因变量y的平均值或期望值如何依赖于自变量$x_1,x_2,\cdots,x_p$的方程。多元线性回归方程的形式为$$E(y)=\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_px_p+\varepsilon$$ $\beta_1,\beta_2,\cdots,\beta_p$称为偏回归系数。 $\beta_i$表示假定其他变量不变，当$x_i$每变动一个单位时，y的平均变动值。 二元回归方程的几何表达——回归面。 估计的多元回归的方程(estimated multiple regression equation)用样本统计量$b_0,b_1,b_2,\cdots,b_p$估计回归方程中的参数$\beta_0,\beta_1,\beta_2,\cdots,\beta_p$时得到的方程。一般形式为。$$\hat y=b_0+b_1x_1+b_2x_2+\cdots+b_px_p$$参数的最小二乘法使因变量的观察值与估计值之间的离差平方和达到最小来求得$b_0,b_1,b_2,\cdots,b_p$，即：$$argmin Q(b_0,b_1,b_2,\cdots,b_p)=\sum_{i=1}^n(y_i-\hat y_i)^2=\sum_{i=1}^ne_i^2$$求解各回归参数的标准方程如下：$$ \begin{cases}\left. \frac{\partial Q}{\partial \beta_0} \right| _{\beta_0=b_0}=0\\\left. \frac{\partial Q}{\partial \beta_i} \right| _{\beta_i=b_i}=0(i=1,2,\cdots,p)\end{cases} $$多重判定系数(multiple coefficient of determination)回归平方和占总平方和的比例，计算公式为$$ R^2=\frac{\sum_{i=1}^n(\hat y_i-\bar y)^2}{\sum_{i=1}^n(y_i-\bar y)^2}=\frac{SSR}{SST}=1-\frac{SSE}{SST} $$因变量取值的变差中， 能被估计的多元回归方程所解释的比例。修正多重判定系数(adjusted multiple coefficient of determination)用样本容量n和自变量的个数p去修正$R^2$得到。计算公式为$$R_a^2=1-(1-R^2)\times\frac{n-1}{n-p-1}$$避免增加自变量而高估$R^2$，意义与$R^2$类似，数值小于$R^2$。估计标准误差s对误差项ε的标准差σ的一个估计值。衡量多元回归方程的拟合优度。计算公式为$$ s=\sqrt{\frac{\sum_{i=1}^n(y_i-\hat y_i)^2}{n-p-1}}=\sqrt{\frac{SSE}{n-p-1}}=\sqrt{MSE} $$线性关系检验检验因变量与所有自变量之间的线性关系是否显著，也被称为总体的显著性检验。检验方法是将回归均方和(MSR)同离差均方和(MSE)加以比较，应用F检验来分析二者之间的差别是否显著。 如果是显著的， 因变量与自变量之间存在线性关系； 如果不显著， 因变量与自变量之间不存在线性关系。（1）提出假设：$H_0：\beta_1=\beta_2=\cdots=\beta_p=0$ 线性关系不显著；$H_1：\beta_1,\beta_2,\cdots,\beta_p $ 至少有一个不等于0。（2）计算检验统计量F：$$ F=\frac{SSR/p}{SSE/(n–p-1)}=\frac{MSR}{MSE}\sim F(p,n-p-1) $$（3）确定显著性水平α，并根据分子自由度p和分母自由度n-p-1找出临界值$F_\alpha$。（4）作出决策：$若F&gt;F_\alpha，拒绝H_0$。 回归系数的检验(检验步骤) 线性关系检验通过后，对各个回归系数进行检验。 对每一个自变量单独应用 t 检验统计量进行检验。（1）提出假设$$H_0:\beta_i=0(自变量x_i与因变量y没有线性关系)$$$$H_1:\beta_i\neq 0(自变量x_i与因变量y有线性关系)$$（2）计算检验的统计量$$ t=\frac{b_i}{s_{b_i}}\sim t(n-p-1)，s_{b_i}=\frac{s}{\sqrt{\sum(x_i-\bar x)^2}}$$（3）确定显著性水平$\alpha$，并进行决策：$$ \left|t\right|\gt t_{\alpha/2}，拒绝H_0；\left|t\right|\lt t_{\alpha/2}，不拒绝H_0 $$ 回归系数的推断(置信区间)回归系数在(1-α)%置信水平下的置信区间为$$ b_i\pm t_{\alpha/2}(n-p-1)s_{b_i} $$回归系数的抽样标准差$$ s_{b_i}=\frac{s}{\sqrt{\sum(x_i-\bar x)^2}}$$ 6 多重共线性(multicollinearity)回归模型中两个或两个以上的自变量彼此相关。多重共线性带来的问题有：可能会使回归的结果造成混乱， 甚至会把分析引入歧途；可能对参数估计值的正负号产生影响， 特别是各回归系数的正负号有可能同我们预期的正负号相反。多重共线性的识别 检测多重共线性的最简单的一种办法是计算模型中各对自变量之间的相关系数， 并对各相关系数进行显著性检验；若有一个或多个相关系数显著， 就表示模型中所用的自变量之间相关，存在着多重共线性。 如果出现下列情况，暗示存在多重共线性：模型中各对自变量之间显著相关。当模型的线性关系检验(F检验)显著时，几乎所有回归系数的t检验却不显著。回归系数的正负号与预期的相反。 检测多重共线性(Variance Inflationary Factor)VIF (variance inflation factor) 用以测量如果自变量相关对估计的回归系数的变异程度的影响。$VIF_j$的定义:$$VIF_j=\frac{1}{1-R_j^2}$$$R_2^j$是第j个自变量对其它自变量进行回归的判定系数。VIF=1表示所对应自变量与其它自变量无线性关系。VIF值越大，多重共线性越严重。如果$VIF_j$&gt;5，$x_j$与其它自变量高度相关。多重共线性(问题的处理)将一个或多个相关的自变量从模型中剔除，使保留的自变量尽可能不相关。如果要在模型中保留所有的自变量，则应避免根据t统计量对单个参数进行检验，对因变量值的推断(估计或预测)的限定在自变量样本值的范围内。 7 定性自变量的回归虚拟变量(dummy variable)定性自变量————只有两个水平的定性自变量或有两个以上水平的定性自变量。虚拟变量——用数字代码表示的定性自变量。虚拟变量的取值为0，1。虚拟变量的个数当定性自变量只有两个水平时，可在回归中引入一个虚拟变量。一般而言，如果定性自变量有k个水平，需要在回归中模型中引进k-1个虚拟变量。当定性自变量只有两个水平并引进虚拟变量时，回归方程可写$E(y)=\beta_0+ \beta_1x$。当指定虚拟变量0，1时,$\beta_0$总是代表与虚拟变量值0所对应的那个分类变量水平的平均值；$\beta_1$总是代表与虚拟变量值1所对应的那个分类变量水平的平均响应与虚拟变量值0所对应的那个分类变量水平的平均值的差值，即平均值的差值=$(\beta_0+\beta_1)-\beta_0=\beta_1$当定性自变量超过两个水平（假定三个水平）并引进虚拟变量时，回归方程可写$E(y)=\beta_0+ \beta_1x_1+\beta_2x_2$。方差分析同样可以通过引入虚拟变量做回归分析。 8 非线性回归（1）二阶回归模型(Quadratic Regression Model)——当散点图如下所示，可考虑二次回归模型。$$y_i=\beta_0+\beta_1x_i+\beta_2x_i^2+\varepsilon$$ 二阶回归模型的显著性检验 总体显著性检验$$F test statistic=\frac{MSR}{MSE}$$ 二阶检验比较二阶模型$$ y=\beta_0+\beta_1x_1+\beta_2x^2+\varepsilon $$线性模型$$ y=\beta_0+\beta_1x_1+\varepsilon $$假设：$H_0:β_2=0 (没有二阶项)$$H_1:β_2\neq 0 (需要二阶项)$ （2）交互作用交互作用——两个自变量共同作用对因变量产生的潜在影响。 假设：$$ y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_1x_2+\varepsilon $$没有交互项, $x_1$对y的影响用$β_1$测量；有交互项,$x_1$对y的影响用$β_1+β_3x_2$测量。影响随$x_2$的改变而改变。 交互作用显著性检验 交互作用模型:$$ y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_1x_2+\varepsilon $$ 假设：$$ H_0：\beta_3=0 （x_1和x_2无交互作用）$$$$ H_1：\beta_3\neq 0 （x_1和x_2有交互作用）$$ （3）其他非线性回归因变量y与x之间不是线性关系，可通过变量代换转换成线性关系，用最小二乘法求出参数的估计值。但是并非所有的非线性模型都可以化为线性模型。 双曲线基本形式：$y=\frac{x}{\alpha x+\beta}$线性化方法：$令y’=\frac{1}{y}，x’=\frac{1}{x}，则有y’=\alpha+\beta x’$ 幂函数曲线基本形式：$y=\alpha x^\beta$线性化方法：$两端取对数得：lgy=lg\alpha+\beta lgx，y’=lgy，x’=lgx，则有y’=lg\alpha+\beta x’$ 对数曲线基本形式：$y=\alpha+\beta lnx$线性化方法：$x’=lnx，则有y’=\alpha+\beta x’$ 指数曲线基本形式：$y=\alpha^{\beta x}$线性化方法：$两端取对数得：lny=ln\alpha+\beta x，y’=lny，则有y’=ln\alpha+\beta x$ S型曲线基本形式：$y=\frac{1}{\alpha+\beta e^{-x}}$线性化方法：$令y’=1/y，x’=e^{-x}，则有y’=\alpha+\beta x’$ 9 建立回归模型得到描述因变量与一个或一个以上自变量之间关系的估计的回归方程。目的是建立一个基于最好自变量集合的模型。找到一个适合的描述变量关系之间关系的函数。选择模型应包含的变量。 俭约的模型–用尽可能少的变量来提供足够精度的预测。 将不重要的变量除去更容易对模型进行解释。 发生多重共线性的可能变小。 变量选择Variable Selection 有些变量的作用不是很大，SSE 不会随着变量个数的增加而增加，但MSE=SSE/(n-k-1) 有可能会随着变量个数的增加而增加。最小的MSE可作为最优变量选择的一个准则，但需考虑所有子集 (2^p个)。 检验增加变量是否适宜的F统计$$F=\frac{SSE(x_1,x_2,\cdots,x_p)-SSE(x_1,x_2,\cdots,x_q,x_{q+1},\cdots,x_p)}{\frac{SSE(x_1,x_2,\cdots,x_q,x_{q+1},\cdots,x_p)}{n-p-1}}$$$$ F\sim F(p-q,n-p-1) $$F越大，说明增加变量减少预测误差的效果越显著。变量选择过程 向前选择(Forward Selection) 从没有自变量的模型开始。 如果所有的F统计量的p-值大于预先设定的终止值，说明增加任一变量效果不显著，停止。 否则，加入具有最大F统计量值的变量。 重新回归， Go to Step 2。 后向消元(Backward Elimination) 从包含所有自变量的模型开始。 如果所有的F统计量的p-值小于预先设定的终止值，说明减少任一变量效果显著，停止。 否则，删除具有最小F统计量值的变量。 重新回归， Go to Step 2。 逐步回归(Stepwise regression procedure)向前选择和后向消元的结合。1.先检查是否有变量需从模型中删除。2.再检查增加一个变量是否能改善模型。3.重复以上过程。注意： α进≤α出，否则F进&lt;F&lt;F出，会导致无限循环。 最佳子集回归(Best-subset approach)对所有可能的自变量组合进行估计。找出具有最大的修正判定系数$adj.R^2$和最小的估计误差标准差$s_ε$。 10 回归中的常见错误（1）没有检验线性关系假设 画散点图。如果不是线性的，检验其它非线性。用线性关系描述非线性关系会引起误导。 （2）只看结果不看图表 要将画散点图作为回归分析的一部分。检验回归直线与实际观测值间的关系。对自动回归来说这一步更为重要。 （3）用回归系数判定变量的重要性 回归系数依赖于自变量的量纲，因此系数的大小与变量的重要性无关。例如，将秒变为微秒没有改变任何事实，但是变量的系数却有所改变。 （4）没有确定置信区间 观察值是随机样本，所以回归结果有一定随机性。不确定置信区间，不可能理解参数的真正含义。 （5）没有计算判定系数 没有$R^2$，很难确定多少变异是由回归解释的。即使$R^2$看起来很好，安全起见还应做F-test。 （6）错误解释相关系数 判定系数是$R^2$。相关系数是R。$R^2$给出变异由回归解释的百分比，不是R。如：R =0.5,$R^2$=0.25——回归解释了25%的变异，不是50%。 （7）使用强相关的自变量 模型同时包括两强相关的自变量会降低回归模型的显著性。要尽可能的了解自变量间的关系。 （8）用回归模型预测观测值范围之外的区域 回归是基于某一特定观测样本的。在样本观测值范围内能提供较为精确的估计。 （9）观测值取值范围太小 回归只有在观测值取值范围附近预测的结果比较好。如果不在常用的范围内取值，回归模型用处不大。 (10)包括太多的自变量 变量越多的模型不一定越好。有可能出现多重共线性。 （11）认为好的预测变量是好的控制变量相关关系不一定因果关系：A与B相关，并不意味着可以通过改变A来控制B。 （12）线性回归结果会给人以误导 为了提供一个简练的总结，回归过程中舍弃了一些信息。有时一些重要的特征也舍弃了——看图形表示可以告诉我们是否有问题。 11 Logistic 回归Logistic回归提出的目的是为了解决二值化数据的回归问题。那么为什么简单线性回归模型不适合二值化数据的回归呢？详细原因可见如下图。 二值化变量是“yes”或者”no”的数据。可以被编码为1和0，也就是说不会有其他的变异数值。所以对于这种情况模型的要求是：模型的边界为0和1，模型可以输出的是一个在这类或者另一类的概率。我们想要的是一个实际值落入这类或者另一类的概率大小。而理想的模型是很好的估计0和1，或者换句话说，结果是0或1。所以解决方案就是Logistic回归。 Logistic的基本形式为$$ \pi_i’=ln(\frac{\pi_i}{1-\pi_i})=\beta_0+\beta_1x_i $$通过观测值估计$n_i$的概率$p_i$，并且用$ln(\frac{p_i}{1-p_i})$估计。典型案例：城市增长问题，城市化预测模拟， 常见的问题 都有一个二值化（或分类）变量： 都涉及到预测的思想机会，概率，比例或百分比。 不像其他的预测情况，y值是有界的。 Logistic 回归与简单线性回归 logistic回归是一种统计技术，可以用二值化变量问题中。回归虽有相似之处，但它不同于普通最小二乘法。识别重要和相似之处是两种技术的区别。]]></content>
      <categories>
        <category>Statistics &amp; Model</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用统计学与R语言实现学习笔记（八）——方差分析]]></title>
    <url>%2F2017%2F06%2F11%2F%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8ER%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89%E2%80%94%E2%80%94%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Chapter 8 ANOVA本篇是第八章，内容是方差分析。前一段考试，汇报，作业。忙不过来，停更了一段时间，现在重新开始更这一部分内容。方差分析是很多实验的基础以及很重要的分析手段，这一章内容相比较而言比较多。 1.方差分析的引论方差分析其实对我们来说并不陌生，因为大学搞生态的那群同学，实验中无数次出现了单方差因素分析的方法。那么方差分析究竟是什么呢？从引论来说，我们举个跟地学领域相关的例子。不同地貌对土壤有机质是否有影响？简单地说方差分析实质适合分析的是一系列数值型数据存在某个属性（也可以是某些），然后这个属性可以按照一定的规则分成几个类别（或者叫水平），我们想了解的就是，不同类别或者不同水平的这个数值是否存在显著性差异。简单的理解，它是处理分类型数据的。这里需要跟上一章提到的拟合优度检验、后面讲到的回归分析做些区别，拟合优度检验通常是分析两个分类变量的关系，回归分析则分析的是一个数值型变量（或多个数值型变量）对一个数值型变量的影响（或者说二者的关系）。而方差分析则是分析一个分类变量（或多个分类变量）对于一个数值变量的影响（或者说二者的关系）。这里给出一些定义和术语（不喜好数学的同学可以跳过，但请记住我上面的内容）：方差分析(Analysis of Variance，ANOVA)研究分类型自变量对数值型因变量的影响 一个或多个分类型自变量 两个或多个 (k 个) 处理或分类 一个数值型因变量 通过检验多个总体均值是否相等来判断是否有显著影响 通过分析数据的误差判断各总体均值是否相等 有单因子方差分析和双因子方差分析 单因子方差分析：涉及一个分类型自变量 双因子方差分析：涉及两个分类型自变量 方差分析 vs 假设检验（1）假设检验：一次只能研究两个样本 需要比较的次数随因子的数量增多而增多； 第一类错误发生的可能性增大。 （2）方差分析：同时分析多个样本 提高检验效率； 将所有信息结合在一起， 增加了分析的可靠性。 1.1 方差分析的部分概念： 因子或因素 (factor)——所要检验的对象，要分析行业对投诉次数是否有影响， 行业是要检验的因子或因素。 水平或处理(treatment):因子的不同表现,零售业、 旅游业、 航空公司、 家电制造业就是因子的处理。 观察值：在每个因子处理下得到的样本数据，每个行业被投诉的次数就是观察值。 试验：涉及一个因子多水平， 可称为单因子多处理的试验。 总体：因子的每一个处理看作是一个总体。 样本数据：观察值可以看作是从着多个总体中抽取的样本数据 也就是说分类变量是因子或因素，而分的类别就可以称为水平或处理，观察值则是数值型变量。试验就是就是分类的过程，总体其实就是水平，样本数据就是观测值。接下来讲讲方差分析的基本思想和原理。 1.2 方差分析的基本思想和原理方差分析的基本思想和原理基于两类误差。也就是随机误差和系统误差。 随机误差——因子的同一处理(总体)下， 样本各观察值之间的差异，这种差异可以看成是随机因素的影响， 称为随机误差。 系统误差——因子的不同处理(不同总体)下， 各观察值之间的差异，这种差异可能是由于抽样的随机性所造成的， 也可能是由于行业本身所造成的， 后者所形成的误差是由系统性因素造成的， 称为系统误差。 所以方差分析的实质是——比较两类误差，以检验均值是否相等；比较的基础是方差比；如果系统（处理）误差明显地不同于随机误差，则均值就是不相等的；反之，均值就是相等的。这里数据的误差用平方和(sum of squares)表示。 组内平方和(within groups)——因子的同一处理(同一个总体)下样本数据的平方和。组内平方和只包含随机误差。 组间平方和(between groups)——因子的不同处理(不同总体)下各样本之间的平方和。组间平方和既包括随机误差， 也包括系统误差。 所以若原假设成立， 组间平方和与组内平方和经过平均后的数值就应该很接近， 它们的比值就会接近1。 若原假设不成立， 组间平方和平均后的数值就会大于组内平方和平均后的数值， 它们之间的比值就会大于1。 当这个比值大到某种程度时， 就可以说不同处理之间存在着显著差异， 也就是自变量对因变量有影响。 1.3 方差分析的基本假定（1）每个总体都应服从正态分布： 对于因子的每一个处理， 其观察值是来自服从正态分布总体的简单随机样本。 （2）各个总体的方差必须相同： 各组观察数据是从具有相同方差的总体中抽取的。 （3）观察值是独立的。（4）在上述假定条件下， 判断行业对投诉次数是否有显著影响， 实际上也就是检验具有同方差的四个正态总体的均值是否相等。（5）如果四个总体的均值相等， 可以期望四个样本的均值也会很接近： 四个样本的均值越接近， 推断四个总体均值相等的证据也就越充分； 样本均值越不同， 推断总体均值不同的证据就越充分。 这里要注意的是，往往很多人做统计的时候往往不考虑前提和假设，这是一个错误。经典统计学中很多模型都有严密的数学推导和前提假设，就笔者从事的地学领域里其实有很多现象不是太遵循经典统计学的前提，由此也衍生出了空间统计学理论，所以在做统计研究时需要考量自己数据的特征，了解统计学与模型的基本前提与假设。 原假设：$H_0：\mu_1=\mu_2=\mu_3=\cdots=\mu_n $ n个水平被投诉次数的均值都相等； 意味着每个样本都来自均值为$\mu$、方差为$\sigma^2$的同一正态总体。 若备择假设成立，即$H_1：\mu_i(i=1,2,3,\cdots,n)$不全相等 至少有一个总体的均值是不同的； 样本分别来自均值不同的多个个正态总体。 2.单因子方差分析（One-way ANOVA)从这章开始后面的部分基本是典型数据分析，故我会渗透更多的数据分析的一些经验和理念。在这里因为要正式进入方差分析的具体内容里，所以我想谈的一点是我曾经说过的一句话——编程先学数据结构。数据结构的重要性可以参加下面的知乎。 https://www.zhihu.com/question/29587605 当然对于R或是其他数据处理语言来说，我觉得最关键的是你在使用分析数据（调用各种包）时需要了解你所调用的包或者函数处理的是什么样的数据（你要把数据处理成你的函数可以读的形式）。当然这是题外话，还是回到标题的单因子方差分析。 如果一个试验中，只有一个因子在变，而其它因素保持不变，称此试验为单因子试验（只涉及一个分类型自变量）。那么它的数据结构如下所示： 当然事实上在分析的时候，个人觉得R和其他数据所能读取的数据结构或者说组织方式还是2列的变量（数值型变量与分类变量）。 分析步骤则是统计学的经典三部曲： 提出假设； 构造检验统计量； 统计决策。 假设的提法在前面已经提过了。$H_0：\mu_1=\mu_2=\mu_3=\cdots=\mu_n $（自变量对因变量没有显著影响）。$H_1：\mu_1,\mu_2,\mu_3,\cdots,\mu_n不全相等$（自变量对因变量有显著影响）。 构造统计量需要计算（1）处理的均值（2）全部观察值的总均值（3）平方和（4）均方(MS) （接下来是公式大全，公式恐惧症者请跳过）（1）处理的均值假定从第i个总体中抽取一个容量为$n_i$的简单随机样本， 第i个总体的样本均值为该样本的全部观察值总和除以观察值的个数。$$ \bar x_i=\frac{\sum_{j=1}^nx_{ij}}{n_i} (i=1,2,\cdots,k)$$式中： $n_i$为第 i 个总体的样本观察值个数，$x_{ij}$为第i个总体的第j个观察值。 （2）全部观察值的总均值全部观察值的总和除以观察值的总个数。$$ \bar x’=\frac{\sum_{i=1}^k\sum_{j=1}^nx_{ij}}{n}=\frac{\sum_{i=1}^kn_i\bar x_i}{n}$$ （3）平方和方差分析需要计算三个平方和。 总平方和 (Sum of Squares for Total), SST。全部观察值与总平均值的离差平方和，反映全部观察值的离散状况。$$ SST=\sum_{i=1}^k\sum_{j=1}^n(x_{ij}-\bar x’)^2 $$ 处理平方和 (Sum of Squares due to Treatment), SSTR，又叫组间平方和。各组平均值与总平均值的离差平方和，反映各总体的样本均值之间的差异程度， 又称处理平方和或组间平方和，该平方和既包括随机误差， 也包括系统误差。$$ SSTR= \sum_{i=1}^k\sum_{j=1}^n(\bar x_i-\bar x’)^2=\Sigma_{i=1}^kn_i(\bar x_i-\bar x’)^2 $$ 误差平方和 (Sum of Squares due to Error),SSE，又叫组内平方和。每个处理或组的各样本数据与其组平均值的离差平方和，反映每个样本各观察值的离散状况，又称组内平方和或残差平方和，该平方和反映的是随机误差的大小。$$ SSE=\sum_{i=1}^k\sum_{j=1}^n(x_ij-\bar x_i）^2 $$实际上，SST=SSTR+SSESST反映全部数据总的误差程度； SSE反映随机误差的大小； SSTR反映随机误差和系统误差的大小。如果原假设成立， 则表明没有系统误差， 处理平方和SSTR除以自由度后的均方与误差平方和SSE和除以自由度后的均方差异就不会太大；如果处理均方显著地大于误差均方， 说明各处理(总体)之间的差异不仅有随机误差， 还有系统误差。判断因子的处理是否对其观察值有影响， 实际上就是比较处理均方与误差均方之间差异的大小。 （4）均方——构建检验统计量各平方和的大小与观察值的多少有关， 为消除观察值多少对平方和大小的影响， 需要将其平均， 这就是均方， 也称为方差。计算方法是用平方和除以相应的自由度，三个平方和对应的自由度分别是： SST 的自由度为n-1， 其中n为全部观察值的个数，SSTR的自由度为k-1， 其中k为因子处理(总体)的个数，SSE 的自由度为n-k。处理均方：SSTR的均方， 记为MSTR， 计算公式为:$$ MSTR=\frac{SSTR}{k-1} $$误差均方：SSE的均方，记为MSE， 计算公式为:$$ MSE=\frac{SSE}{n-k} $$计算检验统计量F：将MSTR和MSE进行对比， 即得到所需要的检验统计量F，当$H_0$为真时， 二者的比值服从分子自由度为k-1、分母自由度为n-k的F分布， 即$$ F=\frac{MSTR}{MSE}\sim(k-1,n-k) $$。最后是统计决策将统计量的值F与给定的显著性水平$\alpha$的临界值$F_\alpha$进行比较，作出对原假设$H_0$的决策。 根据给定的显著性水平$\alpha$， 在F分布表中查找与第一自由度$df_1＝k-1$、 第二自由度$df_2=n-k$ 相应的临界值$F_\alpha$。 若F&gt;$F_\alpha$，则拒绝原假设$H_0$，表明均值之间的差异是显著的，所检验的因子对观察值有显著影响。 若F&lt;$F_\alpha$，则不能拒绝原假设$H_0$， 无证据支持表明所检验的因子对观察值有显著影响。 对前面的三部曲做一个进一步的总结： （1）提出假设；（2）构造检验统计量；均值：全部观察值的总均值、处理的均值。平方和：总平方和SST，处理平方和SSTR，误差平方和SSE。均方：处理均方MSTR，误差均方MSE。均方比：MSTR/MSE~F分布。（3） 统计决策。 在R语言中，方差分析函数较为简单，具体应用后面再说。value为观察值，factor为因素。 12a.aov&lt;-aov(value~factor,data=a)summary(a.aov) 误差来源（方差来源） 平方和(SS) 自由度(df) 均方(MS) F 组间（处理） SSTR k-1 MSTR=SSTR/(k-1) MSTR/MSE 组内（误差） SSE n-k MSE=SSE/(n-k) 总计（合计） SST n-1 当然仅仅证明有显著性差异，可能还不能满足我们的需求，所以需要测度方差分析的关系强度。关系强度的测量拒绝原假设表明因子(自变量)与观测值之间有关系，而处理平方和(SSTR)度量了自变量(行业)对因变量(投诉次数)的影响效应。 当处理平方和比误差平方和(SSE)大， 而且大到一定程度时， 就意味着两个变量之间的关系显著， 大得越多， 表明它们之间的关系就越强。 反之， 就意味着两个变量之间的关系不显著， 小得越多， 表明它们之间的关系就越弱。 变量间关系的强度用处理平方和(SSTR)及误差平方和(SSE)占总平方和(SST)的比例大小来反映。处理平方和占总平方和的比例记为$R^2$ ,即$$R^2=\frac{SSTR（处理平方和）}{SST（总平方和）}$$其平方根R就可以用来测量两个变量之间的关系强度。 3.方差分析中的多重比较多重比较（multiple comparison procedures）——通过对总体均值之间的配对比较来进一步检验到底哪些均值之间存在差异。 可采用Fisher提出的最小显著差异方法， 简写为LSD-least significant difference。LSD方法是对检验两个总体均值是否相等的t检验方法的总体方差估计加以修正（ 用MSE来代替） 而得到的。 方差分析中的多重比较分析步骤 （1）提出假设$H_0: \mu_i=\mu_j (第i个总体的均值等于第j个总体的均值)$$H_1: \mu_i\neq\mu_j (第i个总体的均值不等于第j个总体的均值)$（2）计算检验的统计量: $\bar x_i-\bar x_j$（3）计算LSD,t分布的自由度为n-k（MSE的自由度为n-k）。$$ LSD=t_{\alpha/2}\sqrt{MSE(\frac{1}{n_i}+\frac{1}{n_j})} $$（4）决策：若$\left|{\bar x_i-\bar x_j}\right|\gt LSD $，拒绝$H_0$；若$\left|{\bar x_i-\bar x_j}\right|\lt LSD $，不拒绝$H_0$。 4.双因子方差分析（Two-way ANOVA）前面介绍完了单因子方差分析，但是当我们的因子大于一个的时候，我们又该怎么分析呢？同样抛个样例问题出来。假设现在我们想了解北京城市人口空间分布是否受不同环路（一环、二环、三环乃至四、五、六环）或新老城区的显著影响。所以该问题是一个典型的双因子问题，可以拆分为如下的情况： 因子 新城区 老城区 一环 人口 人口 二环 人口 人口 三环 人口 人口 对于该问题我们可以考虑用单因子方差分析来解决——即通过考虑两个因子间所有的组合来分析是否有显著影响。（二环+新城区，二环+老城区，三环+新城区，……，六环+老城区）通过这样组合来得到最后的单因子水平。但是这样处理的问题是，我们无法了解到底是新老城区的因素影响了人口的空间分布，或者是不同的环路影响了人口的空间分布，亦或是二者共同影响。所以我们需要新的方法来分析。这就是题目所述的双因子方差分析。 4.1 双因子方差分析的基本假定 （1） 每个总体都服从正态分布（对于因素的每一个水平， 其观察值是来自正态分布总体的简单随机样本）。（2） 各个总体的方差必须相同（对于各组观察数据， 是从具有相同方差的总体中抽取的）。（3） 观察值是独立的。 双因子方差分析实质是分析两个因素(行因素Row和列因素Column)对试验结果的影响。如果两个因素对试验结果的影响是相互独立的， 分别判断行因素和列因素对试验数据的影响， 这时的双因素方差分析称为无交互作用的双因素方差分析或无重复双因素方差分析(Two-factor without replication)。如果除了行因素和列因素对试验数据的单独影响外，两个因素的搭配还会对结果产生一种新的影响， 这时的双因素方差分析称为有交互作用的双因素方差分析或可重复双因素方差分析 (Two-factor with replication )。 4.2 无交互作用双因子方差分析如果在一项试验中，有两个因子在变，而其余因子保持不变，则称之为双因子试验。 设因子A有a个水平$A_1，A_2，···，A_a$，因子Ｂ有b个水平$B_1，B_2，···，B_b$，每组因子组合进行1次试验，其结果为$x_{ij}$，$x_{ij}\sim N(\mu_{ij},\sigma^2)$,现在要研究它们对因变量X的影响。 （1）无交互作用双因子方差分析：模型$$ X_{ij}=\mu+\alpha_i+\beta_j+\varepsilon_{ij} $$这里$\varepsilon_{ij}\sim iid$ $N(0,\sigma^2) $（2）无交互作用双因子方差分析：假设因子A原假设：$H_0:α_1= α_2=… = α_a=0$备择假设：$H1:至少一个α_i不等于0$因子B原假设：$H_0: β_1 = β_2 = … = β_b=0$备择假设：$H1:至少一个β_i不等于0$（3）计算步骤（公式大全） 均方：$\bar x_{i.}$是A因素的第i个水平下各观察值的平均值$$ \bar x_{i.}=\frac{\sum_{j=1}^bx_{ij}}{b}（i=1,2,\cdots,a） $$$\bar x_{.j}$是B因素的第j个水平下各观察值的平均值$$ \bar x_{.j}=\frac{\sum_{i=1}^ax_{ij}}{a}（i=1,2,\cdots,b） $$$\bar x’$是B因素的第j个水平下各观察值的平均值$$ \bar x’=\frac{\sum_{i=1}^a\sum_{j=1}^bx_{ij}}{ab} $$ 平方和：$$ SST=\sum_{i=1}^a\sum_{j=1}^b(x_{ij}-\bar x’)^2 $$$$ SSA=b\sum_{i=1}^a(x_{i.}-\bar x’)^2 $$$$ SSB=a\sum_{j=1}^b(x_{.j}-\bar x’)^2 $$$$ SSE=\sum_{i=1}^a\sum_{j=1}^b(x_{ij}-x_{i.}-x_{.j}+\bar x’)^2 $$$$ SST=SSA+SSB+SSE $$ 计算均方（MS）构造检验统计量:误差平方和除以相应的自由度，四个平方和的自由度分别是：总离差平方和SST的自由度为 ab-1；A因素的离差平方和SSA的自由度为 a-1；B因素的离差平方和SSB的自由度为 b-1；随机误差平方和SSE的自由度为 (a-1)×(b-1)。A因素的均方，记为MSA，计算公式为：$$ MSA=\frac{SSA}{a-1} $$B因素的均方，记为MSB，计算公式为：$$ MSB=\frac{SSB}{b-1} $$随机误差项的均方，记为MSE，计算公式为：$$ MSE=\frac{SSE}{(a-1)(b-1)} $$ 计算检验统计量(F)检验行因素的统计量$$ F_A=\frac{MSA}{MSE}\sim F(a-1,(a-1)(b-1)) $$检验列因素的统计量$$ F_B=\frac{MSB}{MSE}\sim F(b-1,(a-1)(b-1)) $$ 统计决策将统计量的值F与给定的显著性水平$\alpha$的临界值$F_\alpha$进行比较， 作出对原假设$H_0$的决策：根据给定的显著性水平a在F分布表中查找相应的临界值$F_\alpha$；若$F_A&gt;F_\alpha$，则拒绝原假设，表明均值之间的差异是显著的， 即所检验的A因素对观察值有显著影响；若$F_A&gt;F_\alpha$，则拒绝原假设，表明均值之间有显著差异，即所检验的B因素对观察值有显著影响。 误差来源（方差来源） 平方和 自由度 均方 F 因子A SSA a-1 MSA=SSA/(a-1) MSA/MSE 因子B SSB b-1 MSB=SSB/(b-1) MSB/MSE 误差 SSE (a-1)(b-1) MSE=SSE/(a-1)(b-1)) 总计 SST ab-1 4.3 有交互作用双因子方差分析除了上面的无交互作用双因子方差分析之外，可能存在的一种情况就是二者同时作用，这就是有交互作用的双因子方差分析。即（$A_i,B_j$）下作了r个试验，所得结果记作$x_{ijk}$,$x_{ijk}$服从$N(\mu_{ij},\sigma^2)，i=1,\cdots,a,j=1,\cdots,b,k=1,\cdots,r$。且相互独立。（1）有交互作用双因子方差分析：模型$$ X_{ijk}=\mu+\alpha_i+\beta_j+(\alpha\beta)_{ij}+\varepsilon_{ijk} $$这里$\varepsilon_{ijk}\sim iid$ $N(0,\sigma^2) $（2）交互作用双因子方差分析：假设因子A原假设：$H_0:α_1= α_2=… = α_a=0$备择假设：$H_1:至少一个α_i不等于0$因子B原假设：$H_0: β_1 = β_2 = … = β_b=0$备择假设：$H_1:至少一个β_i不等于0$交互作用原假设：$H_0: αβ_{11} = αβ_{12} = … = αβ_{ab}=0$备择假设：$H1:至少一个αβ_{ij}不等于0$计算步骤（公式大全） 平方和$$ SST=\sum_{i=1}^a\sum_{j=1}^b\sum_{k=1}^r(x_{ij}-\bar x’)^2 $$$$ SSA=br\sum_{i=1}^a(x_{i.}-\bar x’)^2 $$$$ SSB=ar\sum_{j=1}^b(x_{.j}-\bar x’)^2 $$$$ SSAB=r\sum_{i=1}^a\sum_{j=1}^b(\bar x_{ij}-\bar x_{i.}-\bar x_{.j}+\bar x’)^2 $$$$ SSE=\sum_{i=1}^a\sum_{j=1}^b\sum_{k=1}^r(x_{ijk}-\bar x_{ij})^2 $$$$ SST=SSA+SSB+SSAB+SSE $$ 计算检验统计量(F)$$ F_A=\frac{MSA}{MSE}\sim F(a-1,ab(r-1)) $$$$ F_B=\frac{MSB}{MSE}\sim F(b-1,ab(r-1)) $$$$ F_{AB}=\frac{MSAB}{MSE}\sim F((a-1)(b-1),ab(r-1)) $$拒绝域$$ F_A\gt F_\alpha(a-1,ab(r-1)) $$$$ F_B\gt F_\alpha(b-1,ab(r-1)) $$$$ F_{AB}\gt F_\alpha((a-1)(b-1),ab(r-1)) $$ 误差来源（方差来源） 平方和 自由度 均方 F 因子A SSA a-1 MSA=SSA/(a-1) MSA/MSE 因子B SSB b-1 MSB=SSB/(b-1) MSB/MSE 交互作用 SSAB (a-1)(b-1) MSB=SSAB/(a-1)(b-1) MSAB/MSE 误差 SSE ab(r-1) MSE=SSE/ab(r-1)) 总计 SST abr-1 5.实验设计初步谈完了方差分析的各种理论，回顾开头我们提到的“搞实验的同学经常使用单因素方差分析”，所以在实验设计里，方差分析的应用是非常普遍的。所以这里也谈谈实验设计的一些内容（笔者非实验设计人员，所以仅谈谈一些理念）。一个实验必须施加一些处理，来观察这些处理会不会对实验结果或者测量值有影响。不同的处理是用来比较不同的总体。而好的实验，这些处理必须是随机的。所谓的随机就是指，每个样本有同等的机会（等概率事件）接收这些处理。所以对于这个随机化的比喻就是，你必须闭着眼睛选，才能保证你选的水平是随机的。实验相比于观察的优点也在于此，随机化使的两个比较总体尽可能相似，一切东西都是一样的除了选择处理的水平，如果实验结果存在差异的话，我们就能得出结论，这个处理是否会造成实验结果的不同。实验是我们设计的，可以控制实验的变量（很熟悉的控制变量法）——我们能保证我们比较的两个总体除了处理之外大致是一样的，而观察则无法保证我们所观察的两个总体仅仅存在某个处理上的差异，其他都是一致的。从这个角度来说，实验设计的注意要点如下： （1） 因子数量（单因子方差分析，双因子方差分析……）；（2） 因子处理的数量。（3） 实验设计类型 前两个点大家可能都很清楚了，主要谈谈第三个点。实验设计类型严格来说包括如下： （1）完全因素位级组合（Full factorial design） 完全随机化设计 随机化区组设计（2）部分因素位级组合（Fractional factorial design） （1）完全因素位级组合（Full factorial design）顾名思义，就是讲所有因子的所有组合考虑一遍，造成的问题就是——实验规模巨大。以下几个要点： 如果有k个因子，对于k个因子的第i个水平来说，会有$n_i$个水平的观测值：$$ n=\prod_{i=1}^k n_i $$ 必须实验每个可能的因子水平的组合。 必须捕获有关交互的全部信息。 大量的工作。 主要还包括两种类型。 完全随机化设计(completely randomized design)——“处理” 被随机地指派给试验单元的一种设计，“处理” 是指可控制的因子的各个水平 “试验单元(experiment unit)”是接受“处理”的对象或实体。 随机化区组设计(randomized block design)——先按一定规则将试验单元划分为若干同质组， 称为“ 区组(block)”，再将各种处理随机地指派给各个区组,分组后再将每个品种（ 处理） 随机地指派给每一个区组的设计就是随机化区组设计。如果可能， 我们应选择随机化区组设计。 （2）部分因素位级组合（Fractional factorial design） 仅测量部分因子水平的组合的结果。 必须认真设计来捕获所有可能的交互作用。 相比而言，工作量降低了，不确定性增大了。 在知道一些因子不存在交互作用的前提下特别有效。 典型的是正交试验设计——利用“正交表”进行科学地安排与分析多因子试验的方法。其主要优点是能在很多试验方案中挑选出代表性强的少数几个试验方案，并且通过这少数试验方案的试验结果的分析，推断出最优方案，同时还可以作进一步的分析，得到比试验结果本身给出的还要多的有关各因子的信息。 正交表的性质（正交性）每列中不同数字出现的次数是相等的。每个因子不同的水平出现的次数相同。表示：在试验安排中，所挑选出来的水平组合是均匀分布的（每个因子的各水平出现的次数相同）——整齐可比性。对于任意两列，将同一行的两个数字看成有序数对时，每种数对出现的次数是相等的。任意两个因子都全面试验。表示：任意两因子的各种水平的搭配在所选试验中出现的次数相等——均衡分散性。正交表的优点：各因子的各水平的搭配是均衡的。试验点均衡分散在全部试验条件之中，使得它的代表性很强，能够比较全面地反映、分析出全面试验的最优点来。 用正交表安排试验的步骤明确试验目的，确定试验指标。确定要考察的（主要）因子和水平——各水平次序最好随机排列（因为正交试验不是全面试验）。选用合适的正交表，安排试验计划：根据因子的水平，选择相应水平的正交表；再根据欲考察因子的个数选定正交表中因子的个数。根据计划进行试验，确定试验指标。对试验结果进行分析，得出合理的结论。 正交试验结果的分析方法直观分析法：简单、直观、容易操作，计算量少。方差分析：理论根据可靠，结果可信度高，计算量比较大。正交试验的直观分析法计算各因子各水平的综合平均值，选出各因子的最优水平。对给定因子的每个水平，其它因子对试验指标的影响是相同的，因此可用综合平均值来比较各指标对试验指标的影响（综合可比性）。计算个因子综合平均值的极差，分清因子的主次（在平均值中最大数与最小数之差，称为极差。极差的大小序列，表示因子的重要性大小）。选定最优组合——选定最优组合的原则：对于重要因子，一定要选最优水平，以期达到较好试验效果；对于不重要因子，由于它们的水平变动对试验结果影响不大，可根据节约、高效、简便易行等实际情况灵活选定其水平。正交试验的方差分析假定试验指标服从正态分布基本思想与双因子方差分析方法一致：将总的离差平方和分解成各因子及各交互作用的离差平方和，构造F统计量，对各因子是否对试验指标具有显著影响，作F检验。]]></content>
      <categories>
        <category>Statistics &amp; Model</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用统计学与R语言实现学习笔记（七）——拟合优度检验]]></title>
    <url>%2F2017%2F05%2F10%2F%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8ER%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89%E2%80%94%E2%80%94%E6%8B%9F%E5%90%88%E4%BC%98%E5%BA%A6%E6%A3%80%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[Chapter 7 Goodness of Fit本篇是第七章，内容是拟合优度检验。 1.多项分布拟合优度检验的第一个应用是关于多项总体。那么多项总体（或者多项分布）是什么呢？ 多项分布是二项分布的推广。 总体被分为几个互不相交的类别。 多项分布假设：每次试验有且仅有一个结果发生；每次试验独立；每次试验概率不变。 拟合优度检验-多项总体步骤 将所观测到的数据与理论上的期望值进行比较。 步骤：1.计算每一类实际观测到的频次$f_i$；2.计算每一类理论上的期望频次$e_i$；3.计算 Chi-square 统计量——$\chi^2=\sum(f_i-e_i)^2/e_i$。其中自由度 (df) = k-1， k 是多项总体的类别数。 拟合优度检验用于多项总体检验没有直接的函数，这里用R语言的自编函数实现，体会下具体的算法（当然感觉自己写的略复杂）。代码依旧是后面放出，函数具体使用说明也会附上。 2.独立性依旧是从问题出发——性别与购物频率是否有关系独立性检验——该统计方法常用于检验两个分类变量是否有关系。那么首先要提到两个概念——独立事件和非独立事件（independent and dependent events)。 独立事件——一个事物发生不会对其他事物发生概率造成影响。 非独立事件——一个事物发生会影响其他事物发生概率。 接着统计学构建出了一个表来进行独立性检验。这就是联立表（Contingency Tables)。 解决多总体比例问题。 之前通常用两个或两个以上特征来对样本观测值分类。 也被称为交叉表。 一般在R中，使用Table函数即可生成两个特征（分类变量）的联立表，xtabs则是根据公式创立联立表，prop.table则可以直接计算出比例。联立表如何做独立性检验呢？首先提出假设（这里不详述，相信大家应该懂怎么建立了），接着计算期望的联立表每个单元格的期望频次。$$ e_{ij}=\frac{(i^{th} Rowtotal)(j^{th} Columtotal)}{Total SampleSize}。 $$接着就可以对比实际频次和期望频次，然后我们用卡方（chi-square)统计量进行检验。$$ \chi^2=\sum_{i=1}^n\sum_{j=1}^m\frac{(f_{ij}-e_{ij})^2}{e_{ij}} with, df=(n-1)(m-1)。 $$n为行数，m为列数，$f_{ij}$,$e_{ij}$分别为第i行和第j列的$单元格_{ij}$实际频次和期望频次。当然这个方法也可以用来检验顺序变量和分类变量。方法类似，这里不赘述。 3.概率分布拟合优度检验的最重要的应用其实是探测一个数据具体的概率分布。当然探测数据分布的第一方式——是可见即可得的可视化。主要包括前面提到过的直方图和QQ图。QQ图——Quantile-Quantile Plots（分位数图）： 适用于小数据集。 猜测分布的基础方法。 用来绘制QQ图的数据必须落在该分布内。 如果散点图接近直线，说明数据分布接近正态分布。 这里给出绘制QQ图的原理： 对样本容量为N的样本数据按照升序排序。 计算从1到N排序的百分比。 从百分位数得分的关系找到中心分数。 找到对应于中心分数的z值（标准正态分布）。 绘制对应z值的观测点数据。 接着用R语言实现123456789#QQ plot#generation of random number that fall in normal distributiona&lt;-rnorm(200,0,1)#plotjpeg("plot1.jpg",width = 5000,height = 4000,units = "px",res = 1000)qqnorm(a)qqline(a,col="red")dev.off() 除了QQ图之外，另外一类方法就是通过统计方法——拟合优度检验来探测数据是否正态分布。以正态分布为例。过程： 获取样本数据。 将样本结果分组（单元格）。 比较实际与预期值。 统计量如下：$$ \chi^2=\sum_{i=1}^k\frac{(f_i-ei-)^2}{e_i}。 $$R语言中可以用chisp.test函数进行正态分布测验。 此外对于有某种特定分布的非正态数据可以通过数学变换转变为正态分布数据。常用的一般包括： 对数变换。 开方变换。 指数或平方变换。 这里的数学变换需要根据大家实际研究需求决定。]]></content>
      <categories>
        <category>Statistics &amp; Model</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用统计学与R语言实现学习笔记（六）——假设检验]]></title>
    <url>%2F2017%2F05%2F08%2F%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8ER%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[Chapter 6 Hypothesis Test本篇是第6章，内容是假设检验。 1.基本思想我们还是从问题开始讨论。这回提个接地气的问题——雄安新区批复前后对该地区房价是否有差异？嗯，假设检验其实就是为了解决这类问题。假设检验的基本思想——我们有样本，但是无法获得总体，需要对总体的分布形式或分布参数事先作出某种假设，然后根据样本观测值，运用统计分析的方法来检验这一假设是否正确。分解开来，假设检验=假设+检验（或者假设检验）。假设(hypothesis)——对总体的参数的具体数值（或分布形式）所作的陈述（总体参数包括总体均值、比例、 方差等，分析之前必需陈述）。假设检验(hypothesis test)—先对总体的参数（ 或分布形式） 提出某种假设，然后利用样本信息判断假设是否成立的过程（有参数检验和非参数检验；逻辑上运用反证法， 统计上依据小概率原理）。如图。 假设检验的思想还可以去搜索Fisher 显著性检验的思想(女士品茶试验)的故事深深体会，这里就不详述了。有兴趣的同学可以点击下文的科学网链接查看。 http://blog.sciencenet.cn/blog-624263-795715.html 2.原假设和备择假设从前面的介绍我们知道，假设检验的第一步是建立假设。那么假设分为两种（原假设和备择假设）。那么这二者具体又是什么呢？ 原假设(null hypothesis)——原假设又称“ 0假设”，总是有符号 =， ≥ 或≤，表示为 $H_0$。是研究者想收集证据予以反对的假设（生产实践中常对应正常情形，如均值与设计一致）；一般来说，原假设是一旦拒绝便要采取行动的假设。因此， 原假设总是“受到保护的假设” ，没有充分的证据是不能拒绝原假设的。例如，对一家信誉很好的工厂的产品进行检验，原假设一般是“ 产品合格”。 备择假设(alternative hypothesis)——研究者想收集证据予以支持的假设， 一旦发生就要采取行动， 是与原假设对立的假设，也称“研究假设”，总是有符号 ≠， &gt; 或 &lt;，表示为 $H_1$。 总结起来就是，原假设是统计学史上最悲催角色——它从一开始诞生，就是为了被科学家们发好人卡拒绝而存在的一个假设。备择假设才是科学家们追求的白富美。搞明白了这两个假设，下一步我们做假设检验的时候，就要先提出假设了，这里给了一些提出假设的要点： 原假设和备择假设是一个完备事件组， 而且相互对立（在一项假设检验中， 原假设和备择假设必有一个成立， 而且只有一个成立）。 先确定备择假设， 再确定原假设。 等号“ =” 总是放在原假设上。 因研究目的不同， 对同一问题可能提出不同的假设（ 也可能得出不同的结论）。 同时在实际应用中，我们有不同的需求，因此又有双侧检验和单侧检验的区分。 双侧检验——备择假设没有特定的方向性，并含有符号“=”的假设检验，称为双侧检验或双尾检验(two-tailed test) 单侧检验——备择假设具有特定的方向性，并含有符号“&gt;”或“&lt;”的假设检验，称为单侧检验或单尾检验(one-tailed test)。其中备择假设的方向为“&lt;”，称为左侧检验，备择假设的方向为“&gt;”，称为右侧检验。 原假设与备择假设形式： 双边检验：$ H_0: \mu=2，H_1: \mu\neq2 $。 单边检验：左侧检验——$ H_0: \mu\le2，H_1: \mu>2 $，右侧检验——$ H_0: \mu\ge2，H_1: \mu&lt;2 $。 所见即所得，用一张图来表示假设检验过程。 所以拒绝原假设的理由是假设检验中的小概率原理。那么什么是小概率？ 在一次试验中， 一个几乎不可能发生的事件发生的概率。 在一次试验中小概率事件一旦发生， 我们就有理由拒绝原假设。 小概率由研究者事先确定。 所以拒绝$H_0$的理由就是 3.第一类错误和第二类错误上文介绍了假设检验的过程，但是假设检验过程会不会出现错误呢？其实大家仔细分析拒绝原假设的理由就会发现问题了。通常情况下原假设是小概率事件，但是小概率事件≠0概率事件。小概率事件不是不发生，而是发生概率较小。就像天气预报说明天有99%的可能不下雨，结果1%的可能性成为了事实，明天下雨了。因此假设检验中会有两类错误（弃真错误和取伪错误）经常出现。（1）第一类错误(弃真错误)： 原假设为真时拒绝原假设。 第一类错误的概率为α（没错，就是它，我们的好朋友，小α。咳咳咳，就是显著性水平，一般由研究者事先指定，常用的值有0.01, 0.05, 0.10）。 （2）第二类错误（取伪错误）： 原假设为假时未拒绝原假设。 第二类错误的概率记为β。 α和β的关系——α和β的关系就像翘翘板， α小β就大，α大β就小。所以两类错误不可能同时发生（第一类只在$H_0$为真时发生，第而类只在$H_0$为假时发生）。影响β的因素： 总体参数的真值。 显著性水平α（当α减少时增大）。 总体标准差σ（当σ增大时增大）。 样本容量n（当n减少时增大）。 4.统计量与拒绝域讲了这么多，但是还没有介绍假设检验的计算过程。假设检验的过程依赖于两个重要数学概念（统计量与拒绝域，前面已经有稍微提到了）。这里再做具体介绍。检验统计量(test statistic)——根据样本观测结果计算得到的， 并据以对原假设和备择假设作出决策的某个样本统计量，是对样本估计量的标准化结果（原假设$H_0$为真，点估计量的抽样分布）。标准化的检验统计量公式为：$$ 标准化的检验统计量=\frac{点估计量-假设值}{点估计量的抽样标准差} $$显著性水平和拒绝域的三种情况：双侧检验： 左侧检验： 右侧检验： 统计量落在拒绝域时，我们就可以拒绝原假设。具体如下： 给定显著性水平α，查表得出相应的临界值$z_{\alpha},z_{\alpha/2},t_{\alpha},t_{\alpha/2},\cdots$。 将检验统计量的值与α水平的临界值进行比较。 作出决策：双侧检验——|统计量| &gt; 临界值，拒绝$H_0$；左侧检验——统计量 &lt; 临界值，拒绝$H_0$；右侧检验——统计量 &gt; 临界值，拒绝$H_0$。 5.利用p值进行决策如何利用假设检验解决实际问题？很重要的一个应用是在决策上。就如标题说的，利用p值进行决策。那么什么是p值?p值(p-value)：在一个假设检验问题中，拒绝原假设的最小显著性水平。 在原假设为真的条件下，检验统计量的观察值大于或等于其计算值的概率(双侧检验为分布中检验统计量两侧面积的总和;单侧检验为分布中检验统计量相应单侧面积）。 反映实际观测到的数据与原假设$H_0$之间的一致程度。 被称为观察到的（或实测的）显著性水平。 决策规则： 若p值&lt;α， 拒绝$H_0$。 p值法步骤（以大样本均值为例）将样本统计量转换成检验统计量z 计算p值： Z为标准正态分布随机变量（$p值=(\left|Z\right|\ge z)(双侧),p值=(Z\le z)(左侧),p值=(Z\ge z)(右侧)$） 比较p值和α：如果α≥p值，拒绝$H_0$;如果$α&lt;$p值，不能拒绝$H_0$。 假设检验结论的表述假设检验的目的就在于试图找到拒绝原假设的证据， 而不在于证明什么是正确的。 拒绝原假设时结论是清楚的。 当不拒绝原假设时——并未给出明确的结论，不能说原假设是正确的， 也不能说它不是正确的。但也未说它不是10。 我们只能说样本提供的证据还不足以推翻原假设。 假设检验步骤的总结 陈述原假设和备择假设。 从所研究的总体中抽出一个随机样本。 确定一个适当的检验统计量， 并利用样本数据算出其具体数值。 确定一个适当的显著性水平， 并计算出其临界值， 指定拒绝域。 将统计量的值与临界值进行比较， 作出决策——统计量的值落在拒绝域，拒绝$H_0$，否则不拒绝$H_0$，也可以直接利用p值作出决策。 6.一个总体参数的检验前面的理论讲的差不多了，又到了典型总体参数的检验内容的介绍了。依旧是先一个总体参数的检验（总体均值、总体比例、总体方差）。总体均值的检验(大样本： n≥30)使用z检验统计量：$\sigma^2已知$：$$ z=\frac{\bar x-\mu_0}{\sigma/\sqrt{n}}\sim N(0,1)。 $$$\sigma^2未知$：$$ z=\frac{\bar x-\mu_0}{s/\sqrt{n}}\sim N(0,1)。 $$ 总体均值的检验(正态总体小样本)检验统计量：$\sigma^2已知$：$$ z=\frac{\bar x-\mu_0}{\sigma/\sqrt{n}}\sim N(0,1)。 $$$\sigma^2未知$：$$ t=\frac{\bar x-\mu_0}{s/\sqrt{n}}\sim t(n-1)。$$总体比例的检验假定条件： 总体服从二项分布； 可用正态分布来近似(大样本)。 检验的Z统计量：$$ z=\frac{\bar q-p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}\sim N(0,1)，p_0为假设的总体比例。 $$总体方差的检验检验一个总体的方差或标准差，假设总体近似服从正态分布，使用$\chi^2$分布。检验统计量：$$ \chi^2=\frac{(n-1)s^2}{\sigma_0^2}\sim \chi^2(n-1)。 $$ 这里顺带提下作为统计推断的两大分支的区间估计和假设检验的关系。 过程相似：如果假设均值在95%的置信区间之外，双边检验将拒绝原假设（显著性水平为5%）。 逻辑不同：置信区间——不知道均值多少而要估计它；假设检验: 假定一个均值要看数据是否支持这个假设。 另外还是要谈一谈统计学与实际问题——这里谈的是统计显著性和实际显著性。 一个被拒绝的原假设意味着有统计显著性，但未必有实际显著性。这种情况常发生在大样本或精确测量场合，如Kepler的行星运行第一定律：行星轨道是椭圆的，当时吻合程度很好，100年后，仪器更高级、测量更精确，该假设被拒绝，因为行星间交互作用导致摄动。因此不要盲目使用统计显著性。此外，显著性水平α的选择也是个很关键的问题。一般来说： α不宜过小，否则第二类错误概率会较大。 α的选择与判断发生错误时要付出的代价大小有关。 α的选择是决策问题。 7.两个总体参数的检验讲完了一个总体参数，照例来讲就两个总体参数（两个总体均值之差，两个总体比例之差，两个总体方差比）。独立大样本两总体均值之差检验假定条件： 两个样本是独立的随机样本。 大样本($n_1\ge30和n_2\ge30$)。 检验统计量：$$ \sigma_1^2，\sigma_2^2已知：z=\frac{(\bar x_1-\bar x_2)-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\sim N(0,1)。 $$$$ \sigma_1^2，\sigma_2^2未知：z=\frac{(\bar x_1-\bar x_2)-(\mu_1-\mu_2)}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}\sim N(0,1)。 $$正态总体独立小样本均值之差检验（$\sigma_1^2，\sigma_2^2已知$）假定条件： 两个独立的小样本。 两个总体都是正态分布。 $\sigma_1^2，\sigma_2^2已知。$ 检验统计量:$$z=\frac{(\bar x_1-\bar x_2)-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\sim N(0,1)。 $$正态总体独立小样本均值之差检验($\sigma_1^2，\sigma_2^2$未知但$\sigma_1^2=\sigma_2^2$)假定条件： 两个独立的小样本。 两个总体都是正态分布。 $\sigma_1^2，\sigma_2^2未知但相等，即\sigma_1^2=\sigma_2^2。$ 检验统计量：$$t=\frac{(\bar x_1-\bar x_2)-(\mu_1-\mu_2)}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}，其中s_p=\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}，自由度：n_1+n_2-2。 $$ 两个总体均值之差的检验($\sigma_1^2，\sigma_2^2$未知且不相等$\sigma_1^2\ne\sigma_2^2$)假定条件： 两个总体都是正态分布。 $\sigma_1^2，\sigma_2^2未知且不相等，即\sigma_1^2\ne\sigma_2^2。$ 样本容量相等，$n_1=n_2=n$。 检验统计量：$$t=\frac{(\bar x_1-\bar x_2)-(\mu_1-\mu_2)}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}=\frac{(\bar x_1-\bar x_2)-(\mu_1-\mu_2)}{\sqrt{\frac{s_1^2+s_2^2}{n}}}，自由度：n_1+n_2-2=2(n-1)。 $$ 两个总体均值之差的检验($\sigma_1^2，\sigma_2^2$未知且不相等$\sigma_1^2\ne\sigma_2^2$)假定条件： 两个总体都是正态分布。 $\sigma_1^2，\sigma_2^2未知且不相等，即\sigma_1^2\ne\sigma_2^2。$ 样本容量不相等，$n_1\ne n_2$。 检验统计量：$$t=\frac{(\bar x_1-\bar x_2)-(\mu_1-\mu_2)}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}，自由度：最接近v的整数——v=\frac{(\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2})^2}{\frac{(s_1^2/n_1)^2}{n_1-1}+\frac{(s_2^2/n_2)^2}{n_2-1}}。 $$ 两个总体均值之差的检验(匹配样本)假定条件: 两个总体配对差值构成的总体服从正态分布。 配对差是由差值总体中随机抽取的。 数据配对或匹配(重复测量 (前/后))。 $$ 样本差值均值\bar d=\frac{\sum_{i=1}^n d_i}{n_d}, 样本差值标准差值 s_d=\sqrt{\frac{\sum_{i=1}^n(d_i-\bar d)^2}{n_d-1}}。 $$大样本检验统计量：$$ z=\frac{\bar d-d_0}{s_d/\sqrt{n_d}}\sim N(0,1)。 $$小样本检验统计量：$$ t=\frac{\bar d-d_0}{s_d/\sqrt{n_d}}\sim t(n-1)。 $$ 两个总体比例之差的检验假定条件： 两个总体都服从二项分布。 可以用正态分布来近似。 检验统计量：检验$H_0$： $p_1-p_2=0$$$ z=\frac{\bar p_1-\bar p_2}{\sqrt{\bar p(1-\bar p)(\frac{1}{n_1}+\frac{1}{n_2})}}，其中\bar p=\frac{x_1+x_2}{n_1+n_2}=\frac{\bar p_1n_1+\bar p_2n_2}{n_1+n_2}。 $$检验$H_0$： $p_1-p_2=d_0$$$ z=\frac{(\bar p_1-\bar p_2)-d_0}{\sqrt{\frac{\bar p_1(1-\bar p_1)}{n_1}+\frac{\bar p_2(1-\bar p_2)}{n_2}}}。 $$ 两个总体方差比的检验(F检验)假定条件： 两个总体都服从正态分布。 两个独立的随机样本。 检验统计量：$$ F=\frac{s_1^2}{s_2^2}\sim F(n_1-1,n_2-1)或 F=\frac{s_2^2}{s_1^2}\sim F(n_2-1,n_1-1)。 $$ 最后的总结就是如下图。 最后的最后，回到开头提的问题——雄安新区。该问题其实是两个总体参数的检验问题——两个总体均值之差的问题（两个总体分别是批复前的房价和批复后的房价）。所以如果要讨论该问题，可以考虑从批复前后的房价，抽取配对大样本或小样本(楼盘房价）进行假设检验，这样我们就能在统计学上证明这件事对雄安房价的显著影响啦。本篇涉及的R语言内容较少，还是老规矩，放到后面的第14章去讨论。]]></content>
      <categories>
        <category>Statistics &amp; Model</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用统计学与R语言实现学习笔记（五）——参数估计]]></title>
    <url>%2F2017%2F05%2F07%2F%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8ER%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[Chapter 5 Estimation本篇是第五章，内容是参数估计。 1.参数估计的一般问题正如前面介绍的，统计学的两大分支，分别是描述统计和推断统计。所以今天来谈谈推断统计的第一大问题——参数估计。当然一般叫统计推断的会更多些，二者是一样的。统计推断(Statistical Inference)——主要包括参数估计和假设检验，实质就是通过样本的均值、标准差、方差等去估计总体的均值、标准差、方差或者判断总体的分布形式和分布参数。 参数估计：根据从总体中抽得的样本所提供的信息，对总体分布中包含的未知参数作出数值上的估计。点估计：用样本的某一函数值来估计总体分布中的未知参数;区间估计：按照一定的可靠度估计出参数的一个范围，即确定一个区间，使这一个区间内包含参数真值的概率达到预先所要求的程度。 假设检验：需要对总体的分布形式或分布参数事先作出某种假设，然后根据样本观测值，运用统计分析的方法来检验这一假设是否正确。 上一篇提到的，获取样本之后，我们需要去猜总体，参数估计就是猜总体的参数（分布中所含的未知参数；分布特征：均值、方差等；事件的概率等）或者参数空间(参数的可能取值范围)。假设检验是下一章内容，这里就不细述了。首先明确两个概念：估计量（estimator）与估计值(estimated value)。 估计量： 用于估计总体参数的随机变量，一般为样本统计量（如样本均值、 样本比例、 样本方差等； 例如：样本均值就是总体均值$ \mu $的一个估计量）。 估计值： 估计参数时计算出来的统计量的具体值,如果样本均值=80， 则80就是总体均值的估计值。 既然是估计量，就必须有评价估计量的标准。一般包括以下几点： 无偏性：估计量的数学期望等于被估计的总体参数，样本的随机性导致估计偏差， 偏差平均值为0， 无系统误差（所以在这里又提出了渐进无偏估计：估计随着样本量的增加而逐渐趋近于真值。渐进无偏估计指系统偏差会随着样本量的增加而逐渐减小，趋于0，在大样本时可近似当无偏估计使用）。 有效性： 对同一总体参数的两个无偏点估计量， 有更小标准差的估计量更有效。 一致性： 随着样本容量的增大， 估计量的值越来越接近被估计的总体参数。 由于无偏性是最普遍的标准。这里再介绍部分无偏性的几个要点： 样本均值是总体期望的无偏估计。 诸观测值对样本均值的偏差可正可负，其和恒为0（n个偏差中只有n-1个是独立的）。 自由度：独立偏差个数。 偏差平方和（样本量相等情况下，偏差平方和的大小反映样本散布的大小， 样本量大，偏差平方和大趋近于平均偏差平方和，偏差平方和的期望小于方差，有偏估计，渐进无偏估计。 点估计（point estimate） 用样本估计量的某个取值直接作为总体参数的估计值（例如：用样本均值直接作为总体均值的估计；用两个样本均值之差直接作为总体均值之差的估计）。 无法给出估计值接近总体参数程度的信息（虽然在重复抽样条件下，点估计的均值可望接近总体真值，但由于样本是随机的，抽出一个具体的样本得到的估计值等同于总体真值的可能性很小，特别是在连续分布时，该概率几乎为0，一个点估计量的可靠性是由它的抽样标准误差来衡量的，这表明一个具体的点估计值无法给出估计的可靠性的度量）。 2.区间估计 Confidence Intervals正如前面提到的点估计可靠性较低，因此在点估计的基础上又提出了区间估计(interval estimate)，它能解决的问题包括： 为解决参数估计的精确度和可靠性问题， 在点估计的基础上给出总体参数估计的一个区间范围（该区间一般由样本统计量加减抽样误差而得到），使这一个区间内包含参数真值的概率大到预先所要求的程度。 它不具体指出总体参数等于什么，但能指出总体的未知参数落入某一区间的概率有多大。 二者的区别在于：点估计是一个数，区间估计给出一个区间，提供更多关于变异性的信息。通俗的解释，你女朋友买了件衣服，让你猜价格，你猜中准确价格很难，但是你猜一个范围还是准确度比较高的。 所以区间估计(interval estimate)的概念是——根据样本统计量的抽样分布能够对样本统计量与总体参数的接近程度给出一个概率度量。由概率度量则引出了置信区间（Confidence Intervals）的概念。 $$ 设x_1,x_2,\cdots,x_n是来自f(x,\theta)的样本，对于给定的\alpha，0&lt;\alpha&lt;1, $$ $$ 如能找到两个统计量\theta_1(x_1,x_2,\cdots,x_n)和\theta_2(x_1,x_2,\cdots,x_n) $$ $$ 使得P[(\theta_1(x_1,x_2,\cdots,x_n)&lt;\theta&lt;\theta_2(x_1,x_2,\cdots,x_n)]\ge1-\alpha, $$ $$ 称(\theta_1(x_1,x_2,\cdots,x_n),\theta_2(x_1,x_2,\cdots,x_n))是\theta的置信度为1-\alpha的置信区间(Confidence interval);$$ $$ \theta_1,\theta_2为置信上限与置信下限,1-\alpha为置信度,\alpha为显著性水平(Significance level)。 $$ 置信区间实质上是由样本统计量所构造的总体参数的估计区间。在某种程度上确信这个区间包含真正的总体参数（用一个具体的样本所构造的区间是一个特定的区间，我们无法知道这个样本所产生的区间是否包含总体参数的真值，我们只能是希望这个区间是大量包含总体参数真值的区间中的一个，但它也可能是少数几个不包含参数真值的区间中的一个）。置信区间表明了区间估计的精确性， 区间越小越精确，区间越大越不精确。置信水平——将构造置信区间的步骤重复很多次，置信区间包含总体参数真值的次数所占的比例称为置信水平（置信度）。置信水平表明了区间估计的可靠性， 表示为 $ (1 - \alpha) $($\alpha$是总体参数未在区间内的比例， 区间估计不可靠的概率为$\alpha$， 如$\alpha$=0.05， 表明结论犯错误的概率为0.05),常用的置信水平值有99%, 95%, 90%。那么什么样的置信区间是好的置信区间呢？也就是区间估计的评价标准是什么呢？一般包括如下两点： 置信度（置信系数）越大越好——概率越大越放心，但不能一味求大。 随机区间平均长度越短越好——估计精度越高。 但是在某些实际问题中，我们可能更关心置信上限或置信下限(合金钢强度，越大越好（望大特性），平均强度下限是个重要指标,药物毒性，越小越好（望小特性），平均毒性上限是个重要指标)。这就是单侧置信限问题。谈完了这么多理论，接下来进入实践，如何做一个总体参数的区间估计？按照前一章，我们还是讨论三个重要的总体参数：均值、比例、方差。也是先谈一个总体参数的区间估计。首先规定好符号对应统计量和参数。总体均值——$\mu$，总体比例——p，总体方差——$\sigma^2$;样本均值——$\bar x$，样本比例——$\bar p$，样本方差——$s^2$。一个总体均值的置信区间估计方法总结起来就是： 正态分布，且总体方差$\sigma$已知，用Z值； 正态分布，且总体方差$\sigma$未知，用t值； 非正态分布但是大样本，无论总体方差$\sigma$是否已知，用Z值。 第一种情况：正态分布统计量z——$z=\frac{\bar x-\mu}{\sigma/\sqrt{n}}\sim N(0,1)$，总体均值$\mu$在$1-\alpha$置信水平下的置信区间为$\bar x\pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$，置信下限为$\bar x- z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$，置信上限为$\bar x+ z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$。第二种情况：t分布统计量——$t=\frac{\bar x-\mu}{s/\sqrt{n}}\sim t(n-1)$，总体均值$\mu$在$1-\alpha$置信水平下的置信区间为$\bar x\pm t_{\alpha/2}\frac{s}{\sqrt{n}}$，置信下限为$\bar x- t_{\alpha/2}\frac{s}{\sqrt{n}}$，置信上限为$\bar x+ t_{\alpha/2}\frac{s}{\sqrt{n}}$。第三种情况：正态分布统计量z——$z=\frac{\bar x-\mu}{\sigma/\sqrt{n}}\sim N(0,1)$，总体均值$\mu$在$1-\alpha$置信水平下的置信区间为$\bar x\pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$（$\sigma$未知的话，把$\sigma$换成s即可）。 一个总体比例的置信区间估计方法如下：假定条件np≥5, n(1-p)≥5, n≥30。正态分布统计量z——$z=\frac{\bar p-p}{\sqrt{\frac{p(1-p)}{n}}}\sim N(0,1)$，总体比例的置信区间为$\bar p\pm z_{\alpha/2}\sqrt{\frac{p(1-p)}{n}}$或$\bar p\pm z_{\alpha/2}\sqrt{\frac{\bar p(1-\bar p)}{n}}$。 一个正态总体方差的置信区间估计方法如下：总体方差$\sigma^2$的点估计量为$s^2$，则$\frac{(n-1)s^2}{\sigma^2}\sim \chi^2(n-1) $，总体方差在$1-\alpha$置信水平下的置信区间为：$ \frac{(n-1)s^2}{\chi^2_{\alpha/2}(n-1)}\le \sigma^2 \le \frac{(n-1)s^2}{\chi^2_{1-\alpha/2}(n-1)}$ 接下来谈谈两个总体参数的置信区间的估计方法。估计的一般包括均值差、比例差、方差比，主要包括两种抽样方法——独立样本和配对样本。两个正态总体均值之差的置信区间（独立样本）： $\sigma_1^2$，$\sigma_2^2$已知，使用正态分布统计量z：$z=\frac{(\bar x_1-\bar x_2)-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\sim N(0,1)$，两个总体均值之差$\mu_1-\mu_2$在$1-\alpha$置信水平下的置信区间为：$ (\bar x_1-\bar x_2)\pm z_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}} $。 $\sigma_1^2$=$\sigma_2^2$未知，总体方差的合并估计量：$s_p^2=\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}$，估计量$ \bar x_1-\bar x_2 $的抽样标准差：$\sqrt{\frac{sp_1^2}{n_1}+\frac{sp_2^2}{n_2}}$，两个样本均值之差的标准化：$ t=\frac{(\bar x_1-\bar x_2)-(\mu_1-\mu_2)}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\sim t(n_1+n_2-2) $，两个总体均值之差$\mu_1-\mu_2$在$1-\alpha$置信水平下的置信区间为：$ (\bar x_1-\bar x_2)\pm t_{\alpha/2}(n_1+n_2-2)\sqrt{s_p^2(\frac{1}{n_1}+\frac{1}{n_2})} $。$\sigma_1^2\neq\sigma_2^2$未知，$n_1=n_2$：$ (\bar x_1-\bar x_2)\pm t_{\alpha/2}(n_1+n_2-2)\sqrt{(\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2})} $。$\sigma_1^2\neq\sigma_2^2$未知，$n_1\neq n_2$：$ (\bar x_1-\bar x_2)\pm t_{\alpha/2}(v)\sqrt{(\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2})} $，$ v为自由度，v=\frac{(\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2})^2}{\frac{(s_1^2/n_1)^2}{n_1-1}+\frac{(s_2^2/n_2)^2}{n_2-1}} $。 两个总体均值之差的区间估计(独立大样本)两个总体均值之差的估计：$\sigma_1^2$，$\sigma_2^2$已知时，两个总体均值之差$\mu_1-\mu_2$在$1-\alpha$置信水平下的置信区间为：$ (\bar x_1-\bar x_2)\pm z_{\alpha/2}\sqrt{(\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2})} $。$\sigma_1^2$，$\sigma_2^2$未知时，两个总体均值之差$\mu_1-\mu_2$在$1-\alpha$置信水平下的置信区间为：$ (\bar x_1-\bar x_2)\pm z_{\alpha/2}\sqrt{(\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2})} $。 两个总体均值之差的区间估计(匹配样本)匹配大样本的假定条件——两个匹配的大样本($n_1\ge30和n_2\ge30$)；两个总体均值之差$ \mu_d=\mu_1-\mu_2 $在$1-\alpha$置信水平下的置信区间为：$ \bar d\pm z_{\alpha/2}\frac{\sigma_d}{\sqrt{n}}或\bar d\pm z_{\alpha/2}\frac{s_d}{\sqrt{n}}$，$\bar d$为对应差值的均值，$\sigma_d$为对应差值的标准差。 匹配小样本的假定条件——两个匹配的小样本($n_1&lt;30和n_2&lt;30$)，两个总体各观察值的配对差服从正态分布。两个总体均值之差$ \mu_d=\mu_1-\mu_2 $在$1-\alpha$置信水平下的置信区间为：$ \bar d\pm t_{\alpha/2}(n-1)\frac{s_d}{n} $ 两个总体比例之差区间的估计假定条件——两个总体服从二项分布，可以用正态分布来近似，两个样本是独立的。两个总体比例之差$p_1-p_2$在$1-\alpha$置信水平下的置信区间为：$\bar p_1-\bar p_2\pm z_{\alpha/2}\sqrt{\frac{\bar q_1(1-\bar q_1)}{n_1}+\frac{\bar q_2(1-\bar q_2)}{n_2}}$。 两个正态总体方差比的置信区间实际应用如两种不同方法生产的产品性能的稳定性或两种不同测量工具的精度，需要我们去比较两个总体方差。 两个正态总体方差比的估计比较两个总体的方差比，用两个样本的方差比来判断（如果$s_1^2/s_2^2$接近于1，说明两个总体方差很接近；如果$s_1^2/s_2^2$远离1，说明两个总体方差存在差异）。总体方差比在$1-\alpha$置信水平下的置信区间为：$\frac{s_1^2/s_2^2}{F_{\alpha/2}}&lt;\frac{\sigma_1^2}{\sigma_2^2}&lt;\frac{s_1^2/s_2^2}{F_{1-\alpha/2}},F\sim F(n_1-1,n_2-1)$(F分布性质：$F_{1-\alpha/2}(n_1,n_2)=\frac{1}{F_{\alpha/2}(n_1,n_2)}$)。 总的来说，参数估计的东西很多，根据具体研究情况，我们可以根据自己需求选择不同的参数估计。当然据笔者所知，R语言在参数估计上，现成函数（指默认的基础包）比较少，一般需要自编函数或者有额外的包。这里先给出一个样例函数（14章中会涉及到一部分，这里不详述）。 conf.int=function(x,sigma,alpha) { mean=mean(x) n=length(x) z=qnorm(1-alpha/2,mean=0,sd=1,lower.tail = T) c(mean-sigma*z/sqrt(n),mean+sigma*z/sqrt(n)) } 3.样本容量的确定前一章我们提到统计学闻名于世的规定，样本容量一般必须＞30。但是这种规定，并不是万能的。所以样本容量的确定就成了一个问题。n过大费用高、时间长、人力多；n过小误差增大。事实上n的确定依赖于多大置信度（可靠性），什么样的精度（多宽的区间）。所以样本容量的确定需要根据置信区间的性质来决定。置信区间的性质——以正态总体小样本容量为例。首先置信区间的宽度:$ w=2z\frac{\sigma}{\sqrt{n}} $，因此很容易发现影响区间宽度的因素包括了： 样本容量：大样本容量——小区间。 总体数据的离散程度：小方差——小区间。 置信水平：高置信度——大t值——大区间。 边际误差（margin error)——置信区间上下限与点估计之间的距离。$$ E=z\frac{\sigma}{\sqrt{n}} $$给定边际误差E和置信水平$1-\alpha$，可以找到所需要的样本容量。 估计总体均值时样本容量的确定($\sigma^2$已知)：$n=\frac{(z_{\alpha/2})^2\sigma^2}{E^2}，其中E=z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$样本容量n与总体方差$\sigma^2$、边际误差E、置信水平$1-\alpha$之间的关系为： 随总体方差增大而增大。 随边际误差减小而增大。 随$1-\alpha$增大而增大，随$\alpha$减小而增大。 $\sigma$未知，如有近期样本可用，用其样本标准差代替$\sigma$，用t分布分位数代替标准正态分布分位数，自由度为近期样本容量-1。否则，可以用一个至少比$\sigma$大的数来替代$\sigma$，抽一个样本，用s代替$\sigma$——Stein 两步法。 估计总体比例时样本容量的确定：根据比例区间估计公式可得样本容量n为$$ n=\frac{(z_{\alpha/2})^2\cdot p(1-p)}{E^2}，其中：E=z_{\alpha/2}\sqrt{\frac{p(1-p)}{n}} $$E的取值一般小于0.1，p 未知时， 可用之前样本比率估计，或保守的取最大值0.5。 估计两个总体均值之差时样本容量的确定：设$n_1$和$n_2$为来自两个总体的样本，并假定$n_1=n_2$。根据均值之差的区间估计公式可得两个样本的容量n为：$$ n_1=n_2=n=\frac{(z_{\alpha/2})^2\cdot (\sigma_1^2+\sigma_2^2)}{E^2}，其中E=z_{\alpha/2}\sqrt{\frac{(\sigma_1^2+\sigma_2^2)}{n}} $$。估计两个总体比例之差时样本容量的确定：设$n_1$和$n_2$为来自两个总体的样本，并假定$n_1=n_2$。根据比例之差的区间估计公式可得两个样本的容量n为：$$ n_1=n_2=n=\frac{(z_{\alpha/2})^2\cdot [p_1(1-p_1)+p_2(1-p_2)]}{E^2}，其中E=z_{\alpha/2}\sqrt{\frac{(p_1(1-p_1)+p_2(1-p_2))}{n}} $$。总的来说，样本容量的确定也是根据具体需要以及显著性水平计算得到的。]]></content>
      <categories>
        <category>Statistics &amp; Model</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用统计学与R语言实现学习笔记（四）——抽样方法与抽样分布]]></title>
    <url>%2F2017%2F05%2F06%2F%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8ER%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%E6%8A%BD%E6%A0%B7%E6%96%B9%E6%B3%95%E4%B8%8E%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[Chapter 4 Sampling And Sample Distribution本篇是第四章，内容主要是抽样方法与抽样分布。这一章内容比较多（从抽样方法一直到许多分布函数，尤其是介绍了四个重要分布——正态分布、卡方分布、t分布、F分布，以及部分统计推断的内容）。 1.抽样方法抽样调查的概念前面已经有所涉及到，这里就不详述了。大部分情况下，普查是不太可能的，所以抽样调查是科学研究中应用最为广泛的收集数据的方法。但是正如前面在谈论precision和accuracy问题的时候说的，我们希望数据的质量是Low Bias and Low Variance，抽样调查的样本既能很好地代表总体（非抽样误差小），同时多次抽样的话，也希望抽样的样本大致都接近，降低抽样误差。所以从统计学诞生至今，已经提出了很多的抽样方法。可以说并没有任何一种方法能完全避免这些误差，这些方法需要根据具体情境具体使用。总的来说，抽样方法可以分为两大类：概率抽样与非概率抽样。概率抽样包括了： 简单随机抽样 系统抽样 分层抽样 整群抽样 多阶段抽样 概率抽样是根据一个已知的概率来抽取样本单位（也称为随机抽样），概率抽样要求按照一定的概率随机抽取样本，也就是说每个样本都有一定的机会被抽中，同时每个样本被抽中的概率是可以已知或计算出来的，而当运用概率抽样的样本进行参数估计的时候必须考虑样本被抽中的概率（某种程度来说感觉类似贝叶斯，先验概率和后验概率的问题）。简单随机抽样——从总体N个单位里抽出n个单位作为样本（可以重复抽样，也可以不重复抽样），最常用的抽样方式，参数估计和假设检验主要依据的就是简单随机样本。系统抽样——将总体中的所有单位(抽样单位)按一定顺序排列， 在规定的范围内随机地抽取一个单位作为初始单位， 然后按事先规定好的规则确定其他样本单位（先从数字1到k之间随机抽取一个数字r作为初始单位，以后依次取r+k， r+2k…等单位）。分层抽样——将总体单位按某种特征或某种规则划分为不同的层(Strata)， 然后从不同的层中独立、 随机地抽取样本。整群抽样——将总体中若干个单位合并为组(群)， 抽样时直接抽取群， 然后对中选群中的所有单位全部实施调查。多阶段抽样——先抽取群， 但并不是调查群内的所有单位， 而是再进行一步抽样，从选中的群中抽取出若干个单位进行调查（群是初级抽样单位，第二阶段抽取的是最终抽样单位。将该方法推广， 使抽样的段数增多， 就称为多阶段抽样）非概率抽样包括了： 方便抽样 判断抽样 自愿样本 滚雪球抽样 配额抽样 非概率抽样则不是按照随机的原则选取样本，而是根据研究的具体需求选取调查样本。方便抽样——研究员依据方便的原则选取对应的样本。判断抽样——研究员根据自己的判断选择样本。自愿样本——被调查者自愿参加调查提供信息。举个跟地学相关的例子——志愿地理信息（Volunteer Geographcial Information,VGI），是指利用工具创建、组装和传播个人资源提供的地理数据，像社交媒体中的签到。滚雪球抽样——首先选择一组进行调查，让调查者提供另外一些属于调查总体的调查对象，然后持续下去。配额抽样——先将体中的所有单位按一定的标志（变量） 分为若干类， 然后在每个类中采用方便抽样或判断抽样的方式选取样本单位。总的来说，各种抽样方式各有各有的优缺点，根据研究具体情况进行选择。而实际研究中简单随机抽样的应用更多些，这边提供R语言中做简单随机抽样的代码示例。 #N表示总体的数据，n为抽样单位，replace=FALSE代表不重复抽样，replace=TRUE代表重复抽样。 n&lt;-sample(N,n,replace = FALSE) 2.正态分布正态分布由高斯作为描述误差相对频数分布的模型而提出的： 描述连续型随机变量的最重要的分布 许多现象都可以由正态分布来描述 可用于近似离散型随机变量的分布 经典统计推断的基础 正态分布的意义，多多少少大家都有了解，这里就不再详述了。随机变量服从$$ X\sim N (\mu,\sigma^2) $$则X的概率密度函数为$$ f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2} -\infty&lt;x&lt;\infty $$这就是正态分布的概率密度函数。正态分布具有如下性质： 关于x=μ的钟形对称性质，峰值在x=μ处。 均值和标准差一旦决定，该分布形式也就决定了。 均值决定分布函数位置，标准差决定函数的扁平程度。 X轴两侧无限延伸，f(x)无限逼近x轴,但理论上不可能相交。 正态随机变量在特定区间上的取值概率由正态曲线下的面积给出，而且其曲线下的总面积等于1 下图给出了两个图（一个是用核密度生成的曲线，一个是正态分布概率密度函数）来说明以上的部分性质（具体实现的R语言代码会在笔记写完后给出）。 标准正态分布就是指均值为0，标准差为1的正态分布。通过标准正态分布可以很方便地求算各种概率，所以实际应用中，往往将正态分布数据通过标准化的方式转化为标准正态分布求解具体概率。即令$$ Z=\frac{x-\mu}{\sigma} $$则Z服从标准正态分布。那么如何检验数据的正态性呢？一般有以下几种方法： 对数据画出频数分布的直方图或茎叶图（若数据近似服从正态分布， 则图形的形状与上面给出的正态曲线应该相似）。 求出样本数据的四分位差和标准差， 然后二者计算比值。 若数据近似服从正态分布，则有$$ Q_d/s\approx1/3 $$ 拟合优度检验 一般可以通过画这个图来进行检验（代码同在笔记写完后给出）。 或者计算四分位差和标准差比值。这里给出这个方法的R语言实现(用户自编函数）。 Normaltestindex&lt;-function(x) { q=fivenum(x) Qd=q[4]-q[2] s=sd(x) Normaltestindex=Qd/s cat(&quot;The Qd/s&quot;, Normaltestindex) } 拟合优度检验是后面章节内容，这里不详述。正态分布在各样本相互前提下存在线性可加性。$$ x_i\sim N(\mu_i,\sigma_i)，且x_i相互独立，则\Sigma a_ix_i\sim N(\Sigma a_i\mu_i,\Sigma a_i^2\sigma_i^2)$$同时样本量够大情况下，n个独立随机变量之和服从正态分布。 3.三种不同性质的分布统计量(statistic)——样本来自总体，必然携带有反映总体性质的各种信息。统计的基本任务就是通过对样本的研究来对总体的未知参数或分布类型作出估计，对有关总体的假设作出推断。样本是进行统计推断的依据。但在实际应用时，一般不是直接使用样本本身，而是对样本进行整理和加工, 即针对具体问题构造适当的函数—统计量， 利用这些函数来进行统计推断，揭示总体的统计特性。事实上统计量把分散在样本中的总体信息按需要集中在一个函数上，使该函数能反映总体方面的信息。概念很拗口，总结起来就是，我懒得分析（也没法分析，因为有些总体无法穷尽）总体的分布，我就偷懒地先抽样，并且认为样本能够代表总体特征，再偷懒地计算某些指标，这些指标可以反映样本数据分布特征，这些指标就叫统计量，然后再用统计量去推出（猜）总体的分布特征（第一章提到了，应该叫参数）——果然“懒”才是人类进步的动力。当然这里要区分两个概念——统计量与观察值。$$ 假设X_1,X_2, \cdots ,X_n是来自总体X的样本, x_1,x_2,\cdots ,x_n为其样本值, $$$$则称不含任何总体分布中未知参数的函数g(X_1,X_2,\cdots,X_n)为统计量，相应实数g(x_1,x_2,\cdots,x_n)为观察值 $$如何理解这二者区别呢？其实这里把样本看成了一组随机变量，因为在未抽样前，样本观察值未知，样本就是个随机变量（所以一般来说统计推断的基础是简单随机抽样），但是抽样之后，样本就是一组确定的观察值，这也可以说是样本的二重性。常用的统计量包括了样本均值、样本方差、样本标准差、样本k阶原点矩、样本k阶中心距（具体公式的话，文末附录给出）。从前面提到的统计推断基础是简单随机抽样，也就是要求样本是简单随机样本，那么简单随机样本又是什么呢？首先随机样本的概念：$$ 随机抽取的n个个体的集合(X_1,X_2, \cdots ,X_n),n为样本容量。 $$而简单随机样本则需要在随机样本的前提上满足以下两个条件： 随机性：总体中每个个体都有同等机会被选到样本中,即$$ X_i 与X同分布$$ 独立性：样本中每个个体的选取不影响其他个体的选取，即$$ X_1,X_2, \cdots ,X_n是相互独立的随机变量 $$ 接下来是标题提到的三种不同性质的分布：总体分布、样本分布、抽样分布。 总体分布——总体中各元素的观察值所形成的分布，分布通常是未知的，可以假定它服从某种分布。 样本分布——一个样本中各观察值的分布，也称经验分布，当样本容量 n 逐渐增大时，样本分布逐渐接近总体的分布。 抽样分布——样本统计量的概率分布， 是一种理论分布，又称为诱导分布，在重复选取容量为n 的样本时，由该统计量的所有可能取值形成的相对频数分布，随机变量是样本统计量（样本函数，如样本均值，样本比例，样本方差等），结果来自容量相同的所有可能样本，提供了样本统计量长远而稳定的信息，是进行推断的理论基础，也是抽样推断科学性的重要依据（ 点估计、 置信区间、假设推断等）。 用一个简单的例子来说明三者的区别。假设总体N=4，随机变量X=年龄。总体分布如下，均值为21，方差为2.236。这里提醒用R语言做统计的同学，R语言默认的var和sd都是求样本的标准差（分母是n-1和n的差别），当你的数据是总体时，建议另外计算，或者可以使用我下面的自编函数（给了个标准差的样例，方差的会在笔记写完后给出）。 Populationsd&lt;-function(x){ n=length(x) m=mean(x) Psd=sqrt(sum((x-m)^2)/n) cat(&quot;The Standard deviation of Population : &quot;,Psd) } 建立n=2的抽样分布，样本均值分布如下，均值为21，方差为1.58 可以发现总体分布是均匀分布，而样本均值的抽样分布却呈现了近似正态分布，均值是相同的，但是方差却有差异。根据总体分布以及样本容量可以将抽样分布分为以下三类： 精确抽样分布：当总体分布已知时，如果对任一自然数都能导出统计量分布的显示表达式，这样的抽样分布称为精确抽样分布（对小样本的统计推断特别有用，大多数是在正态总体下得到的， t分布、 F分布等）。 渐近抽样分布：样本量无限大时统计量的极限分布（大样本问题）。 近似抽样分布：注意获得近似分布的条件（用统计量的前二阶矩当作正态分布的前二阶矩获得正态近似，随机模拟法获得统计量的近似分布）。 4.一个总体样本统计量的抽样分布样本均值的抽样分布——在重复选取容量为 n 的样本时，由样本均值的所有可能取值形成的相对频数分布（一种理论概率分布，推断总体均值的理论基础）。 正态总体均值抽样分布——精确分布（均值无偏）。 样本均值的中心极限定理——渐进分布。中心极限定理：$$设从均值为\mu， 方差为\sigma^2的一个任意总体中抽取容量为n的样本，$$ $$ 当n充分大时，样本均值的抽样分布近似服从均值为\mu、 方差为\sigma^2/n的正态分布 $$ 用一张图来说明这个定理（摘自参考书目1：贾俊平，《统计学》（第五版），中国人民大学出版社，2012.）。 当然也在这里诞生了一个统计学闻名于世的规定，样本容量n一般至少要求&gt;30。因此样本均值的抽样分布中，样本均值的数学期望（也就是均值）和方差就有对应的公式了。样本均值的数学期望和方差：数学期望：$$ E(\bar x) =\mu $$方差： $$ \sigma_{\bar x}^2=\frac{\sigma^2}{n} （重复抽样）,$$ $$ \sigma_{\bar x}^2=\frac{\sigma^2}{n}\frac{N-n}{N-1} （样本总体有限，且n\ge 5\%N不重复抽样）$$ 总结来说，总体分布为正态分布的话，抽样分布也是正态分布，总体分布为非正态分布的话，大样本情况下也是近似正态分布，小样本则为非正态分布。 除了均值之外，实际生活中比例也是一个很重要的参数。比例——总体（或样本）中具有某种属性的单位与全部单位总数之比（如不同性别的人与全部人数之比，合格品(或不合格品) 与全部产品总数之比）总体比例可表示为$$ p=\frac{N_0}{N},1-p=\frac{N_1}{N} $$样本比例可表示为$$ p=\frac{n_0}{n},1-p=\frac{n_1}{n} $$样本比例的抽样分布——在重复选取容量为n的样本时，由样本比例的所有可能取值形成的相对频数分布。 一种理论概率分布。 当样本容量很大时（满足np≥5, n(1-p)≥5)，样本比例的抽样分布可用正态分布近似。 推断总体比例p的理论基础。 类似于均值的抽样分布我们可以得到样本比例的数学期望和方差：数学期望：$$ E(\bar p) =p $$方差： $$ \sigma_{\bar x}^2=\frac{p(1-p)}{n} （重复抽样）,$$ $$ \sigma_{\bar x}^2=\frac{p(1-p)}{n}\frac{N-n}{N-1} （不重复抽样）$$ 接下来介绍一个重要的分布——卡方分布。$$ 若随机变量\xi_1,\xi_2,\cdots,\xi_n是n个相互独立的标准正态变量（独立同分布于标准正态分布），$$ $$ 则这n个随机变量的平方和Y=\Sigma\xi_i^2构成的随机变量的分布称为自由度为n的\chi^2分布（chi-square distribution），$$ $$ 记为\chi^2(n)分布 $$卡方分布的性质和特点如下： 分布的变量值始终为正 分布的形状取决于其自由度n的大小， 通常为不对称的单峰右偏（ 正偏） 分布， 但随着自由度的增大逐渐趋于对称， 当n&gt;30时， 接近正态分布 期望为：$$ E(\chi^2)=n，$$方差为：$$ D(\chi^2)=2n (n为自由度) $$ 可加性：$$ 若U和V为两个独立的\chi^2分布随机变量，U\sim \chi^2(n_1)， V\sim \chi^2(n_2), $$ $$则U+V这一随机变量服从自由度为n_1+n_2的\chi^2分布$$ 卡方分布的性质可以根据这张图来看。 卡方分布一般用于样本方差的分布的计算。样本方差的分布——在重复选取容量为n的样本时， 由样本方差的所有可能取值形成的相对频数分布。对于来自正态总体的简单随机样本，则比值$$ \frac{(n-1)s^2}{\sigma^2} $$该比值的抽样分布服从$$ 自由度为(n-1)的\chi^2分布，即 \frac{(n-1)s^2}{\sigma^2}\sim \chi^2(n-1) $$ 接着再介绍一个耳熟能详的t分布。t 分布是类似正态分布的一种对称分布， 它通常要比正态分布平坦和分散。t分布的性质和特点如下： 自由度为1的t 分布为柯西分布，期望值不存在。 n&gt;1时，期望值为0。 n&gt;2时，方差存在，为n/(n-2)。 随着自由度的增大，分布也逐渐趋于标准正态分布。（t 分布的极限为标准正态分布，当n&gt;30时， t 分布可用标准正态分布近似） t分布的性质可以根据这张图来看。 t分布的应用是在求样本均值与样本标准差之比上样本均值与样本标准差之比的分布为：$$ t=\frac{\bar x-\mu}{s/\sqrt{n}}\sim t(n-1) $$自由度为(n-1)的t 分布 5.两个总体样本统计量的抽样分布其实从前面第4点内容可以看出，其实实际应用中，均值、比例、方差的估计是比较多的，因此这三个总体样本统计量的抽样分布特别提出来了。而第4点讨论的是一个总体的，两个总体的也可以类比，道理是一样的。两个样本均值之差的抽样分布： 两个总体均为正态分布，即$$ X_1\sim N(\mu_1,\sigma_1^2) ,X_2\sim N(\mu_2,\sigma_2^2)$$ 两个样本均值之差的抽样分布服从正态分布，其分布的数学期望为两个总体均值之差：$$ E(\bar x_1-\bar x_2)=\mu_1-\mu_2 $$ 方差为各自的方差之和：$$ \sigma_{\bar x_1-\bar x_2}^2=\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2} $$ 两个样本比例之差的抽样分布： 两个总体都服从二项分布。 分别从两个总体中抽取容量为$n_1$和$n_2$的独立样本，当两个样本都为大样本时，两个样本比例之差的抽样分布可用正态分布来近似。 分布的数学期望为：$$ E(\bar p_1-\bar p_2)=p_1-p_2 $$ 方差为各自方差之和：$$ \sigma_{\bar p_1-\bar p_2}^2=\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2} $$ 最后的最后，我们来介绍本片的最后一个重要的分布——F分布。F分布：$$ 设若U为服从自由度为n_1的\chi^2分布，即U\sim \chi^2(n_1), $$ $$ V为服从自由度为n_2的\chi^2分布，即V\sim \chi^2(n_2),且U与V相互独立, $$ $$ F=\frac{U/n_1}{V/n_2},F服从自由度n_1和n_2的F分布, $$ $$记为 F\sim F(n_1,n_2) $$不同自由度下的F分布 两个样本方差比的抽样分布： 两个总体都为正态分布，即$$ X_1\sim N(\mu_1,\sigma_1^2), X_2\sim N (\mu_2,\sigma_2^2） $$ 从两个总体中分别抽取容量为$n_1$和$n_2$的独立样本。 两个样本方差比的抽样分布， 服从分子自由度$(n_1-1)$， 分母自由度为$(n_2-1)$的F分布， 即$$ \frac{s_1^2/\sigma_1^2}{s_2^2/\sigma_2^2}\sim F(n_1-1,n_2-1) $$ 附录常用统计量公式样本均值：$$ \bar X=\frac{1}{n} \sum_{i=1}^n{X_i} $$ 样本方差：$$ S^2=\frac{1}{n-1} \sum_{i=1}^n(X_i-\bar X)^2 $$ 样本标准差：$$ S=\sqrt{\frac{1}{n-1} \sum_{i=1}^n(X_i-\bar X)^2} $$ 样本k阶原点矩： $$ A_k=\frac{1}{n} \sum_{i=1}^nX_i^k (k=1,2,\cdots) $$ 样本k阶中心矩： $$ B_k=\frac{1}{n} \sum_{i=1}^n(X_i-\bar X)^k (k=1,2,\cdots) $$ 各统计量的观察值： $$ \bar x=\frac{1}{n} \sum_{i=1}^n{x_i} $$ $$ s^2=\frac{1}{n-1} \sum_{i=1}^n(x_i-\bar x)^2 $$ $$ a_k=\frac{1}{n}\sum_{i=1}^nx_i^k (k=1,2,\cdots) $$ $$ b_k=\frac{1}{n} \sum_{i=1}^n(x_i-\bar x)^k (k=1,2,\cdots) $$]]></content>
      <categories>
        <category>Statistics &amp; Model</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用统计学与R语言实现学习笔记（三）——描述性统计]]></title>
    <url>%2F2017%2F05%2F05%2F%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8ER%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%BB%9F%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[Chapter 3 Descriptive Statistics本篇是第三章，内容是描述性统计。同时在这一章会开始渗透R语言的相关内容。但整体还是以理论为主。 1.数据的预处理本章正式进入统计学的一大分支——描述统计。很多人会疑惑做一个Project或者写一篇Paper，最难的是什么？我曾经不止一次说过，最难的是数据。数据收集完成，项目完成了50%。而数据收集完成之后，很多人就会马上开始进行数据处理和分析，事实上这是不对的。因为你不清楚你的数据是否有问题（什么问题都有可能，会导致你的分析出现各种问题）。所以你拿到数据后的第一步，应该是对数据做预处理，或者用大数据时代的话——叫数据清洗或者ETL（Extract-Transform-Load），我想预处理还会占掉Project花费时间的20%吧。那么接下来先介绍下预处理的内容。数据预处理： 数据审核 数据筛选 数据排序 数据透视 数据审核，包括直接数据的完整性审核以及准确性审核（是否客观），间接数据的适用性审核以及时效性审核；数据筛选，就是对于数据里面的异常值（存在错误，不符合调查要求等），在现在来说就是dirty data（脏数据），将这些数据剔除；数据排序，事实上数据排序更多的目的还是为了更方便地发现异常值，是做数据清洗的手段；数据透视，借鉴于Excel里的数据透视表，事实上就是数据的重铸，融合和汇总，从而得到我们需要的数据。总的来说，前期预处理需要对数据进行排序、汇总和观察发现相关的数据异常值等。在这个阶段，不喜编程的同学推荐用Excel来做数据预处理（通过数据透视图、替换数据、排序、Countif等工具和Excel函数高效完成预处理），更高级的一般可以考虑用R、Python等编程语言进行清洗预处理，或者像在数据库里用SQL语句也是可以的。响应一下本部分的标题，R语言实现，交代几个简单的语句进行数据清洗。 12345678#x为数据框、数组或矩阵，通过summary可以获取平均值、中位数、四分位数等，如果有缺失数据，则会显示NAN等。summary(x)#表示y是按照x的第一行先升序排列，然后再按x的第二列降序排列得到的数据，-表示降序。y&lt;-x[order(x[1],-x[2)]#去除NA所在行和列y&lt;-na.omit(x) 2.数据的整理与展示这部分的数据整理是在预处理完毕后，根据我们需要对数据进行整理和简单可视化（多画图，多可视化，你能发现很多事情）。那么第一步就是先把我们的数据类型搞清楚。因为不同类型数据，整理方式不同。对于分类数据和顺序数据主要是分类整理。对于数值数据主要是做分组整理。 分类数据的整理核心就是计算频数、比例、百分比、比率，一般可视化用条形图（柱状图）。此外还可以考虑使用帕累托图。帕累托图（Pareto chart）是以意大利经济学家V.Pareto的名字而命名的。这是一个双坐标轴图，一侧纵坐标是频率，另一侧纵坐标是累计频率。是在条形图基础上加上一条折线图（累计频率曲线）。通常用帕累托图来表示，就是研究事物特征是否存在二八定律（20/80规律，典型案例：20%的人拥有80%的财富）。除此之外，分类型数据还可以用饼图来进行可视化。 顺序数据则一般选用累计频率曲线和环状图进行可视化。 数值型数据的可视化方式是最多的。主要包括了直方图、折线图（频数多边形图）、打点图、茎叶图、箱线图、线图（时间序列数据）、双变量问题（二维散点图与散点图矩阵）、三变量问题（三维散点图或气泡图）、多变量问题（雷达图）。 其中这里面有一个直方图分组使用的经验公式。 $$ K=1+\frac{\lg {n}}{\lg {2}} $$ K为组数，n为样本数。确定组数，通过极差和组数求组距即可分组。这部分有很多可视化内容，暂时就不在这部分讲述了（第14章会重点讲解几个典型的可视化方式的R语言绘制)。最后小结下数据可视化的内容。 品质数据——先制作汇总表，然后可以采用条形图、饼图、环状图可视化； 数值数据中的原始数据——茎叶图、箱线图可视化； 数值数据中的分组数据——直方图、折线图； 数值数据中的时间序列数据——线图； 数值数据中的多元数据——散点图、气泡图、雷达图。 此外对于图表可视化来说，好的图表可视化应当具有如下特征： 显示数据； 让读者把注意力集中在图表的内容上，而不是制作图表的程序上； 强调数据之间的比较； 服务于一个明确的目的； 有对图表的统计描述和文字说明。 鉴别图表优劣的准则： 精心设计、 有助于洞察问题的实质； 使复杂的观点得到简明、 确切、 高效的阐述； 能在最短的时间内以最少的笔墨给读者提供最大量的信息； 表述数据的真实情况， 避免歪曲。 当然图表可视化不仅仅只有R，Excel、SPSS、Tableau都可以使用。 3.数据的概括性度量当你面对一堆数据时，你还是不知道从何下手，因为我们不可能强行记住每个数据，然后在脑海里对各个数据的分布进行比较，所以科学家们在处理数据的时候，都希望用数据规模尽可能小的一个指标去描述数据尽可能多的信息。那么从数据的角度出发，针对数据分布的不同方面，科学家们也都找出了不相同的指标来进行描述。简单来说，数据分布包括了集中趋势、离散程度、分布形状三个方面的内容。 集中趋势：众数、中位数、平均数； 离散程度：异众比率、四分位差、极差、方差或标准差、离散系数； 分布形状：偏态系数、峰态系数。 集中趋势的几个指标想必大家较为清楚，就不展开详述了。而离散程度中极差、方差和标准差也是如此，同上，不过单独解释下自由度的概念（一组数据中可以自由取值的数据的个数，与附加给独立观测值的约束或限制的个数有关，比如三个数据的均值已经知道，知道其中两个数据，第三个数据是固定的，也就是说在添加了均值这个约束之后，观测数据自由取值的个数是n-1=2个）。这里重点解释异众比率，四分位差、离散系数、偏态系数和峰态系数。异众比率——从字面理解即可，非众数的比率。也就是——不是众数的组的频数占总频数的比率。四分位差——上四分位数减去下四分位数。离散系数——也就是标准差系数，即用标准差除以平均值。偏态系数——用来描述数据分布特征（分布偏斜程度）的系数，该系数&gt;0为右偏分布，0为尖峰分布，&lt;0为扁平分布，=0为扁平峰度适中。最后单列出以上部分指标的公式（有数学恐惧症的同学请跳过）： 中位数： $$ x_{((n+1)/2)} （n为奇数）$$ 或 $$ (x_ {(n/2)}+x_{(n/2)+1})/2 （n为偶数）$$ 四分位数：$$ Q_ {L位置}=\frac{n}{4},Q_{U位置}=\frac{3n}{4} $$ $$ Q_ {L位置}=\frac{n+1}{4},Q_{U位置}=\frac{3（n+1）}{4} $$ $$ Q_ {位置}=\frac{[\frac{n+1}{2}]+1}{2} $$ $$ Q_ {L位置}=\frac{n+3}{4},Q_{U位置}=\frac{3n+1}{4} $$ 平均数： $$ \bar{x}=\frac{\sum_ {i=1}^n x_{i}}{n}(简单平均数), \bar{x}=\frac{\sum_{i=1}^k M_{i}f_{i}}{n}(加权平均数)$$ $$ G_{m}=\sqrt[n] {\prod_{i=1}^n {(1+x_{i})}}-1(几何平均数)$$ 异众比率：$$ v_r=1-\frac{f_m}{\sum f_i} $$ 极差： $$ R=max(x_i)-min(x_i) $$ 四分位差： $$ Q_d=Q_U-Q_L $$ 平均差：$$ M_d=\frac{\sum_{i=1}^n \left|{x_i-\bar {x}}\right|}{n} $$ 或 $$ M_d=\frac{\sum_{i=1}^k \left|{M_i-\bar {x}}\right|f_i}{n} $$ 总体方差： $$ \sigma^2=\frac{\sum_{i=1}^N (x_i-\mu)^2}{n} $$ 或 $$ \sigma^2=\frac{\sum_{i=1}^k (M_i-\mu)^2f_i}{n} $$ 总体标准差： $$ \sigma=\sqrt {\frac{\sum_{i=1}^N (x_i-\mu)^2}{n}} $$ 或 $$ \sigma=\sqrt{\frac{\sum_{i=1}^k (M_i-\mu)^2f_i}{n}} $$ 样本方差： $$ s^2=\frac{\sum_{i=1}^N (x_i-\mu)^2}{n-1} $$ 或 $$ s^2=\frac{\sum_{i=1}^k (M_i-\mu)^2f_i}{n-1} $$ 样本标准差：$$ s=\sqrt {\frac{\sum_{i=1}^N (x_i-\mu)^2}{n-1}} $$ 或 $$ s=\sqrt{\frac{\sum_{i=1}^k (M_i-\mu)^2f_i}{n-1}} $$ 标准分数： $$ z_i=\frac{x_i-\bar{x}}{s} $$ 标准差系数： $$ v_s=\frac{s}{\bar{x}} $$ 偏态系数： $$ SK=\frac{n\sum(x_i-\bar{x})^3}{(n-1)(n-2)s^3} $$ 或 $$ SK=\frac{\sum_{i=1}^k(M_i-\bar{x})^3f_i}{ns^3} $$ 峰态系数： $$ K=\frac{n(n+1)\sum{(x_i-\bar{x})^4}-3[\sum{(x_i-\bar{x})^2}]^2(n-1)}{(n-1)(n-2)(n-3)s^4} $$ 或 $$ K=\frac{\sum_{i=1}^k{(M_i-\bar{x})^4f_i}}{ns^4}-3 $$]]></content>
      <categories>
        <category>Statistics &amp; Model</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用统计学与R语言实现学习笔记（二）——数据收集]]></title>
    <url>%2F2017%2F05%2F04%2F%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8ER%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%2F</url>
    <content type="text"><![CDATA[Chapter 2 Data Collection本篇是第二章，内容是数据收集。 1.数据来源做科学研究离不开数据，而数据的来源有哪些呢？这里比较简单地将数据来源分为两类：直接（一手）数据和间接（二手）数据。直接数据的数据获取来源包括：观测、调查、实验。间接数据的数据获取来源包括：出版物、互联网等。接下来分别谈谈这几个来源。观测——自然科学里有观测，如气象气候、植物生长期等，社会科学同样有观测，譬如像街区人的观测等。观测的数据可以说是纯粹第一手数据，在研究中是很宝贵的数据，但是很容易受到观测记录员主观因素的影响。调查——自然科学里的调查（室外样品采集，环境状况调查）一般是跟室内实验相结合，而社会科学的调查会更丰富，如典型的问卷调查、访谈、座谈会等。实验——实验是自然科学的核心，这里就不详述了（比如：土壤理化性质分析、植物生态生理特性分析）。不过近年来随着学科交叉增多，社会科学也开始更多地引入实验的方法（以笔者另一门公选课《初级社会网络》为例，耶鲁大学的社会心理学家米尔格兰姆(Stanley Milgram)就设计了一个连锁信件实验，这就是著名的六度分割理论的由来）。当然除了以上三种，我认为在现在的大数据时代，还存在一些新的直接数据来源。 物联网（Interest of Thing,IOT),以各类传感器（RFID、红外感应系统、GPS、通量塔等）为代表，代表数据就是如今火热的大数据——如RFID记录数据、浮动车与出租车GPS轨迹数据、通量塔测量的NEE等。 遥感（Remote Sensing，RS），某种程度上，遥感也是靠传感器接收数据，但是它与物联网还是有所差别，故单列出来。作为地学和生态学背景（尤其是GIS和RS相关方向的）的学生，对遥感会非常熟悉。遥感的特征就是，可以大范围快速获取地表信息数据（譬如地形、地表温度、气溶胶、albedo等，当然这些都需要进行反演等）。 总的来说，观测在自然科学和社会科学中都有渗透较多，但是观测往往受到记录人员主观因素影响导致误差。而且观测的数据结构一般来说呈现非结构化的特征。调查在社会科学中有较多应用，自然科学中较少，而实验则是在自然科学中应用广泛，社会科学则应用较少。这两类的实质是类似的，需要提前设计好调查的大纲或者实验方案，然后按照设计好的大纲和方案进行调查和实验。也因此这两类数据结构化特征比较明显。所谓的间接数据就是指已经经过他人整理的相关数据。这边列出来的主要包括：出版物：统计年鉴、书籍、论文等。统计年鉴是大部分社会科学相关研究的重要数据来源，这边就不详述了。书籍对于很多如社会研究的文本分析是重要的数据来源。论文作为数据，是近年来兴起的文献计量学的典型数据。此外对Meta分析，论文里的数据则是重要来源。互联网：百度指数、阿里指数、大众点评等数据。互联网数据可以利用网络爬虫获取。总的来说，间接数据易于获取，作用广泛，但使用的时候需要控制数据质量以及引用。 2.调查设计这边主要介绍的是数据的调查方式、调查方案的结构和设计以及调查问卷设计。（1）数据的调查方式数据的调查方式一般而言是遵循统计学规律的（我们称之为统计调查方式），这里列举了我国统计调查的常用方式：普查（人口普查、农业普查、甚至到最近刚刚发布成果的全国第一次地理国情普查）、抽样调查（概率抽样、非概率抽样，具体后面第三章会详述）、统计报表（统计公报）。而除了以上之外，当我们需要自己收集直接数据的时候又可以分为以下几种：询问调查类： 访问调查 邮寄调查 电话调查 电脑辅助 座谈会 个别深访 观察实验 观察 实验 （2）调查方案的结构和设计如何做调查？是很多人在科学研究中的第一道难关。这里给出一个关于做调查的普遍步骤流程图： 那么调查方案又是什么呢？我认为调查方案就是调查的策划书。明确你调查的一些目的、对象、项目以及调查方法等。一般结构如下： 调查目的 调查对象调查单位 调查项目 其他 （3）调查问卷设计最后这部分是谈谈调查问卷设计的一些内容（包括笔者自己的一些经验）。问卷结构 开头部分（问候语、填写说明、问卷编号 ） 甄别部分 主体部分 背景部分 其他部分就不详述了，甄别部分一般是针对过滤的问题，就是不符合条件的即可跳过部分调查题目。接下来主要针对主体部分简单介绍。主体部分其实就是问卷主要调查的部分。一般来说要注意一下几点。 提问内容尽可能简短 用词准确通俗（可按6W原则推敲：Who,Where,When,Why,What,How） 一项提问只包括一项内容 避免诱导性提问、否定形式提问、敏感性问题 而问题则又可以分为两大类：开放性问题（自由回答型）和封闭性问题（选择回答型）。封闭性问题包括了二项选择、多项选择（单项、多项、限制选择）、顺序选择法、评定尺度法、双向列联表法。 开放性问题——一般就是可以随便答，这类数据一般是问卷者的主观感受，不会受客观影响。但是最大的问题在于数据收集呈现非结构化特征，多以文本形式存在。研究时必须通过重编码、文本分析等方法。 封闭性问题——相当于是选择题或者填空题。二项选择就是，只有两个选项（A或B）；多项选择则是有多个选项，可以选至少一个（一个为单项、一个以上且不限制选择的数量为多项、一个以上且限制选择的数量为限制）；顺序选择法，就是给出多个选项，让你按照自己的认识对选项进行排序；评定尺度法，给出多个选项且是有等级划分的（如很差，差，一般，好，很好）进行选择；双向列联表法，将两类不同问题综合到一起，用表格形式，横向为一类问题，纵向为一类问题。 从笔者的经验来说，在设置问卷的时候，必须要先从自己想研究的问题出发，思索如何用数据分析证明自己的结论，然后大致思索需要用来分析的统计方法与统计指标，然后对应选择问题的形式，因为不同的问题形式对应的数据结构大不相同，而且统计方法也不尽相同。最后的最后安利大家一个软件：Survey123 for ArcGIS这是由esri北京研发中心开发的一款外业数据收集软件——获得“问卷好帮手”称号的application。 http://www.esri.com/products/survey123 主要包括了桌面端Survey123 connect和移动端Survey123 app两大软件。可以简便地建立问卷、分享问卷、搜集数据、分析数据，同时采集时受访者的GPS位置也将被记录。具体教程参照如下网址。 http://doc.arcgis.com/zh-cn/survey123/ 3.数据质量采集数据的时候必须考虑的就是数据的质量，即降低采集数据时产生误差。科学研究中的数据误差无可避免，而误差的来源主要包括：抽样误差、非抽样误差。抽样误差，在抽样方式确定时就无法避免，具体的方法可能还是统计学万能解药———增加样本量。非抽样误差则包括了如下的内容： 抽样框误差 回答误差 无回答误差 调查员误差 抽样框误差——其实就是抽取的样本无法代表总体；回答误差和无回答误差都是由于受访者导致的错误，而调查员误差则无须再介绍，即采集者自身的误差。那么控制误差的方法无非就在于样本大小以及合适的数据框（针对非抽样误差和抽样框误差），靠重访来进行修正（回答误差和无回答误差），调查员误差则需要对调查员进行培训。当然这里还得普及一个概念，在统计学里面，precision（精度）和accuracy（准确性）是不相同的。中文里面往往因为两个单词都翻译成精度，事实上这两个词指的是不一样的内容。二者的区别可以看下面的图。 这里做个简单的解释，事实上就是我们研究事物是个无法穷尽的总体，因此我们只能进行抽样调查，那么多次抽样调查研究之后，我们可以得到每次抽样调查的均值（也可以是其他统计量），在图中就是蓝色的点，那么在靶中心的绿色部分，可以认为是总体的真正均值。那么也就是说高精度一般指的是，我们的样本数据自身的变异性很小，也就是说，我们做了N次抽样调查，而每次抽样调查的样本均值基本是稳定的。我们抽的N次都是相近的数据，也就是说我们的抽样误差尽可能小了（因为抽了N次数据变化不会太大）。而高准确性一般指的是，我们N次抽样的样本数据的平均值与总体数据差异很小。也就是说我们的N次样本的均值与总体均值很接近，也就是说我们的非抽样误差尽可能小了（因为N次数据平均值与总体均值差异较小，说明我们抽的样本能够反映总体均值的特征）。最后，总结下数据质量的控制要求： 精度(precision)： 最低的抽样误差或随机误差 准确性(accuracy)： 最小的非抽样误差或偏差 关联性： 满足用户决策、 管理和研究的需要 及时性： 在最短的时间里取得并公布数据 一致性： 保持时间序列的可比性 最低成本： 以最经济的方式取得数据st=>start: 确定调查目的（Define the issue） op1=>operation: 确定感兴趣的总体和抽样单元（Define the population interest and sampling unit） op2=>operation: 规范调查问题（Formulate survey questions) op3=>operation: 构建抽样框（Construct sampling frame) op4=>operation: 选择样本（Select sample) op5=>operation: 收集数据（Collect data) e=>end: 分析数据（Analyze data) st->op1->op2->op3->op4->op5->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);]]></content>
      <categories>
        <category>Statistics &amp; Model</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用统计学与R语言实现学习笔记（一）——简介]]></title>
    <url>%2F2017%2F05%2F02%2F%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%8ER%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Chapter 1 Introduction本部分内容是我这学期公选课《应用统计学》的学习笔记，主要参考书目为如下两本：贾俊平，《统计学》（第五版），中国人民大学出版社，2012.何晓群，《多元统计分析》（第三版），中国人民大学出版社，2012.本篇为第一章节，也就是Introduction（简介）部分。 1.从问题说起常常听到的一句话，好的科学论文解决一个科学问题，科学的诞生本身就和问题离不开。老生常谈的就是像牛顿被苹果砸了之后，就想到一个问题，为啥苹果不飞上天和太阳肩并肩呢？我答：因为会被烤焦。。。。嗯，幽默一下。总结下来说，科研中有很多问题跟统计学相关（笔者是地学和生态学背景，就提点接地气的问题）。譬如：（1)人口研究当中，我们希望了解65岁以上老年人所占的比例，以便于我们更好地研究老龄化的问题。（2)从几个监测站点的汽车尾气监测推断今天北京市的汽车尾气排放是否达到大气污染物排放标准。（3）影响植物光合作用的因素是什么，各个因素的影响有多大？以及等等等。总结来说，可以分为以下的几类：（1）统计量问题；（2）参数（推断统计）问题；（3）归因问题；（4）预测问题。 2.统计学及其研究过程那么统计学又是什么呢？ statistics: the science of collecting,analyzing, presenting, and interpreting data.Copyright 1994-2000 Encyclopaedia Britannica, In 翻译过来就是 统计学是收集、分析、表述和解释数据的科学（ 不列颠百科全书） 所以统计学包括了： 数据收集：取得数据 数据处理：整理与图表展示 数据分析：利用统计分析方法分析数据 数据解释：结果的说明 得到结论：从数据分析中得出客观结论。 同时跟统计学密切相关的就是概率论。这二者都是研究随机现象数量规律的学科。而二者的区别可以用一张图来形象体现： 也就是说，概率论是——我知道箱子里面是什么样的，我想知道我拿在手里的球是什么样的可能性分别有多大。统计学则是——我不知道箱子里面是什么样的，但是我已经知道我拿在手里的球是什么样的，我想靠我手里的球的样子去推断箱子是什么样的。有兴趣的也可以查看知乎上的回答。 https://www.zhihu.com/question/20269390 总结起来，统计学的研究过程就像下面的流程图。 当然这里面很容易出问题的是解释数据——数学上有意义，并不代表现实中有意义，非常容易出现很多的悖论。比如太阳升起的时间与每个人起床时间相关性很高，但是我不能说因为每个人都起床了，所以太阳升起了。 3.统计方法及其应用领域从前面提到的我们知道，统计方法是通过已知的观测数据去分析随机现象的数量规律。因此统计方法就包括了两大部分：描述统计与推断统计。其实核心就在于我们所观测的样本是否等于总体。样本=总体，那么使用描述统计就能够用来描述我们所研究的现象。样本≠总体，那么使用推断统计才能较为准确地描述我们所研究的现象。事实上，近年来火热的大数据就是因为技术（传感器等）发展，我们足够获取可以近似等于全样本甚至全样本的数据而不是以往的样本数据所引起的一场变革，也就是说是由数据驱动的变革。 统计学应用领域十分广泛，这里就不细谈了。 4.统计数据类型由于应用广泛，所以统计数据类型也是多样化的。不同的划分标准类型也不相同：（1）按照计量层次划分 分类数据 顺序数据 数值数据 （2）按收集方法划分 调查观察数据 试验数据 （3）按时间状况划分 截面数据 时序数据 5.统计学中的几个基本概念统计学中的基本概念分别是： 总体（population） 样本（sample) 参数（parameter) 统计量（statistic) 变量（variable) 总体——研究对象的全体样本——研究对象的部分个体，观测数据参数——用来描述总体的数学度量统计量——用来描述样本的数学度量变量——描述现象的某种特征 st=>start: 实际问题 op1=>operation: 收集数据（取得数据） op2=>operation: 整理数据（处理数据） op3=>operation: 分析数据（研究数据） op4=>operation: 解释数据（结果说明） st->op1->op2->op3->op4{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);st=>start: 统计方法 e=>end: 结论 sub=>subroutine: 描述统计 cond1=>condition: 样本=总体？ cond2=>condition: 分布已知？ op1=>operation: 推断统计 op2=>operation: 参数估计 op3=>operation: 假设检验 st->cond1(no)->op1->cond2(no)->op2->e cond1(yes)->sub->e cond2(yes)->op3->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-1-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-1-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-1", options);st=>start: 总体（参数） op=>operation: 样本（统计量） st(right)->op{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-2-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-2-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-2", options);]]></content>
      <categories>
        <category>Statistics &amp; Model</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Git使用的一些心得]]></title>
    <url>%2F2017%2F05%2F01%2F%E5%85%B3%E4%BA%8EGit%E4%BD%BF%E7%94%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[本篇稍微记录下Git使用的一些心得。对Git的使用，应该是从搭建自己的博客开始的。当时看到开源中国推荐的一篇基于码云+hexo搭建自己博客的文章。所以就花了一天时间鼓捣了下博客。顺带整理下目前能看到我写的博客文章的几个地址：自己搭建的博客（Hexo）： https://giserdaishaoqing.github.io/ CSDN博客： http://blog.csdn.net/esa_dsq 简书（相比而言，简书少了一篇关于桌面GIS连接Postgresql的文章）： http://www.jianshu.com/u/8bfccfb12c0d 开源中国： https://my.oschina.net/u/2424163、 以上地址均可看到我的博客文章。回到Git上，关于如何搭建hexo的静态博客。这里就不详述了。网上教程太多。我最早看得是下面的博客，当然后面参考了很多简书和各种平台的。 https://my.oschina.net/z707z/blog/824830 尽管最早是想在OSChina上搭建，不过老是出bug，最后还是选择了github。bug总结起来就是，https连接靠不住，git大法好。用github生成ssh秘钥，然后连接，更为方便。具体的过程下面这篇文章讲得已经很详细了。 http://blog.csdn.net/wfdtxz/article/details/8678982 关键的几个命令就是。 #查看是否有秘钥 cd ~/.ssh ls #没有的话就生成一下，引号里填你github账户的邮箱。 ssh-keygen -t rsa -C &quot;your_email@youremail.com&quot; 后面就去本地文件夹搜索下你的秘钥文件id_rsa.pub。复制内容，并打开github，从settings里面找到如下的选项。 接着点击New Key，然后把秘钥文件里的内容复制过去。启用即可。可以用下面的命令测试下是否成功。 ssh -T git@github.com 这个就是之前搭建博客时提交博客老出错的解决方案。顺带记录下hexo博客的典型命令。 hexo clean hexo generate hexo deploy hexo server -p 5000 同时，最近刚好完成了ArcGIS中OLS回归工具结果可视化的R语言版本代码（见上一篇博客），顺带就托管到github上，就尝试了下如何push。在需要托管的本地文件夹右击Git Bash，接着输入如下的命令。这里就每次都输下自己的账户密码吧。比较安全。 #添加需要更新上传的文件 git add . #commit一下 git commit -a -m &quot;备注信息&quot; #最后push上传 git push]]></content>
      <categories>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R语言读取空间数据以及ArcGIS中OLS工具回归结果可视化R语言版]]></title>
    <url>%2F2017%2F04%2F24%2FR%E8%AF%AD%E8%A8%80%E8%AF%BB%E5%8F%96%E7%A9%BA%E9%97%B4%E6%95%B0%E6%8D%AE%E4%BB%A5%E5%8F%8AArcGIS%E4%B8%ADOLS%E5%B7%A5%E5%85%B7%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96R%E8%AF%AD%E8%A8%80%E7%89%88%2F</url>
    <content type="text"><![CDATA[前面已经介绍过R语言读取excel的方法了，当然读取数据来说，个人还是推荐csv或txt存储（针对小数据量）。大数据量的数据的话建议还是用数据库，此外也可以考虑data.table包读取，这个包也是个神包，后面学习完可能来谈谈。这个都是题外话，今天主要目的还是来介绍R语言读取空间数据的方法。主要是之前有同学问过读取的方法。我就顺带整理下，另外虾神今天刚发了一篇关于ArcGIS的OLS工具回归结果可视化内容，并贴出了Python版可视化的代码（文末贴链接），所以对应写个R语言版。本文介绍的空间数据类型主要包括了三种：矢量数据（以最普遍的的shapefile为例），栅格（raster，这个格式就比较多了，不过大同小异）,地理数据库（geodatabase也就是.gdb文件，Esri的数据库）。 1.矢量数据矢量数据其实主要包括了三类：点，线和面，能读取的方式有很多种。下面列举几种。先从点线面分别读取的方式来看，主要包括readShapePoints（读取点），readShapeLines（读取线要素），readShapePoly（读取面要素）。这几个函数都是maptools包里面的。所以第一步如果没安装的话请先安装。 install.packages(&apos;maptools&apos;) library(maptools) 接着定位到我们所需读取数据的工作路径上，然后就可以开始读取对应的数据了。 fujian&lt;-readShapePoints(&apos;fujian.shp&apos;) nanhailine&lt;-readShapeLines(&quot;linesnation.shp&quot;) province&lt;-readShapePoly(&quot;province.shp&quot;) 如果不需要什么其他操作，读取数据只需要填入文件名字作为传入的参数即可。这几个函数完整的参数大体差不多，主要包括下面几个。fn——文件名，一般能读的是.shp文件，.shx文件和.dbf文件proj4string = CRS(as.character(NA))——坐标系的CRS字符串，关于坐标系的问题，这里不详讲。其实就是一个坐标系对应一个ID，把对应ID读进去，按照对应坐标系读取，这个是遵循规范的。一般前两个参数用得多。后面这些只介绍这三个函数共有的参数，其他参数就请参照帮助文档。verbose = FALSE——默认为False，这个主要是在读取数据后是否返回读入要素的类型和数量。repair=FALSE——这个参数的话，主要是考虑到.shx索引文件太大，默认False会跳过读取数据，TRUE的话，会进行内部修复，读取这类文件。而maptools同样提供了另外一个函数readShapeSpatial，这个就可以读以上的三类要素。 fujian&lt;-readShapeSpatial(&quot;fujian.shp&quot;) 当然除了maptools，还有其他包可以读取，事实上，maptools提供的函数读取只能传输较差分辨率的空间数据，所以更推荐的是用rgdal包的OGR驱动程序来读取。熟悉开源GIS的同学对GDAL会比较熟悉，事实上rgdal就是GDAL的R接口（当然没装还是要先装，方法同上），读取方式如下，参数也是传入文件名即可简单读取，不过这个参数可以读具体文件也可以读文件夹名。对应上面proj4string也有一个参数p4s，其他参数参照文档。 fujian&lt;-readOGR(&quot;fujian.shp&quot;) 此外还有shapefiles包也可以进行读取。读取方式（可以读取shp和shx，shx读取结果为空间索引）如下： fujian&lt;-read.shp(&quot;fujian.shp&quot;) 矢量数据读取主要通过以上几种方式就可以实现。 2.栅格数据栅格数据的话，格式还是多种多样的。这边主要提供几种不同格式的读取方法（.img文件，.tif文件，ASCII码文件和.asc文件）。栅格数据读取主要是基于rgdal包，读取方式如下，img和tif都可以通过readGDAL直接读取。 co2&lt;-readGDAL(&quot;CO22008.img&quot;) co2&lt;-readGDAL(&quot;CO22008.tif&quot;) 这里面的参数我就不详细介绍了，主要解释几个个人认为比较重要的参数。有兴趣的同学可以去查询官方文档。band——波段数，单纯栅格无所谓。做遥感影像数据处理时就会遇到需要几个波段的问题，如果缺省的话，是全部导入。p4s——等同于上面的proj4stringtype——像素深度：8bit，16bit等除了rgdal之外，也可以通过raster包进行读取.img文件和.tif文件，这个更方便些。读取方式如下 co2&lt;-raster(&quot;CO22008.img&quot;) co2&lt;-raster(&quot;CO22008.tif&quot;) 当然栅格数据还有较为普遍的以ASCII码文件存储的方式。这里也提供下如何读取ASCII码文件，这个方法是基于sp包的，所以需要先安装和载入sp包，这个包是R语言空间数据的基础包，指定了空间数据库的方法和对象。 co2&lt;-read.asciigrid(&quot;co22008.txt&quot;) 当然ASCII码文件可能是以.asc文件存储的，只需把后缀名改成.asc即可读取。栅格的读取大概就如上。 3.地理数据库（Geodatabase）数据读取Geodatabase是Esri在ArcInfo8之后引入的一种全新的面向对象的空间数据模型。具体简介可以自己搜索。也就是说Geodatabase是Esri官方提出的一种数据库，没有ArcGIS是无法创建Geodatabase的。读取的话其实也相对麻烦些。目前看到的只有Esri官方给出的一个R包可以读取Geodatabase数据。R语言与ArcGIS的结合在未来将很有潜力。目前Esri已经在github上开源了部分工具，2015年全球用户大会上也秀出了R-ArcGIS的Sample工具。具体开源地址： https://r-arcgis.github.io/ 本次用来读取Geodatabase的包就是R-ArcGIS中的一个关键包——arcgisbinding。这个包目前没有在cran上，建议下载之后离线安装。下载地址： https://github.com/R-ArcGIS/r-bridge/releases/tag/v1.0.0.125。 这个包的官方文档可以从官网下载，也可以从下面的连接下载。 http://download.csdn.net/detail/esa_dsq/9823403 具体安装，同时安装完之后需要先确认ArcGIS的许可（要求应该是ArcGIS10.4以上的版本或者ArcGIS pro1.1以上），具体代码如下： install.packages(&quot;G:/GIS/Esri/ArcGIS Plugin/arcgisbinding_1.0.0.125.zip&quot;, repos = NULL, type = &quot;win.binary&quot;) arc.check_product() 读取的方式稍微复杂些，用到了arc.open，arc.select，arc.data2sp三个函数，arc.open是打开gdb文件里的featureclass（支持的格式还包括layers等），arc.select是将打开的featureclass按照需要的字段和sql读成R语言中熟悉的数据框，arc.data2sp是将数据框转化成空间要素。使用方式如下。 china&lt;-arc.open(&quot;china.gdb/province&quot;) chinapop&lt;-arc.select(china,fields = c(&apos;Pop_Rural&apos;,&apos;Pop_Urban&apos;,&apos;POPU&apos;)) chinapopsp&lt;-arc.data2sp(chinapop) 当然读取完了我们还是要来可视化一下。用的是spplot函数，这里就不展开讲了，只贴出图（当然只是随手画的，色带啥的都没调）。 4.ArcGIS中OLS工具回归结果可视化（R语言版）最后的最后。对应虾神文章的Python版本ArcGIS中OLS工具回归结果可视化，写个R语言版本。 12345678910111213141516171819202122232425#载入包#如果没安装，请先安装，如果已安装，请注释#install.packages(".../arcgisbinding_1.0.0.125.zip", repos = NULL, type = "win.binary")…表示arcgisbinding离线包的路径#install.packages("car")#install.packages("GGally")#install.packages("ggplot2")library(arcgisbinding)library(car)library(GGally)library(ggplot2)#设置工作路径setwd("F:/R/demo/readdata")#检查ArcGIS产品许可arc.check_product()#读取数据并将数据转换为数据框olsdata&lt;-arc.open("china.gdb/olstest")olsdataolsdataframe&lt;-arc.select(olsdata,fields = c("gdp","Index_2000","Pop_Urban","POPU","PRODUCT","Estimated","Residual","StdResid"))#把因变量和自变量单独分离出来并用car包里的spm函数绘图variableframe&lt;-olsdataframe[,c(1:5)]spm(variableframe,diagonal="hist") 感觉似乎不是很好看，换个方式。 12#利用GGally的ggpairs函数画图ggpairs(variableframe,upper = list(continuous="cor"),lower = list(continuous="smooth"),diag = list(continuous="barDiag")) 12345#绘制标准残差的分布，用ggplot2画图a&lt;-ggplot(olsdataframe,aes(x=StdResid))+ geom_histogram(aes(y=..density..),binwidth = 0.5,colour="white",fill="grey")+ geom_line(stat='density',colour="#FF6666")a 1234567891011#绘制标准残差和观测值的散点图opar&lt;-par(no.readonly = T)par(fig=c(0,0.8,0,0.8))plot(olsdataframe$gdp,olsdataframe$StdResid,col="grey",pch=16)par(fig=c(0,0.8,0.7,1),new=T)hist(olsdataframe$gdp,col="grey")par(fig=c(0.75,1,0,0.8),new=T)hist(olsdataframe$StdResid,col="grey") 主要是为了和虾神最后的效果类似，事实上，在读取完数据框之后，纯属散点图矩阵可视化方面的内容。最后贴出虾神的公众号和博客。 微信公众号：虾神daxialu——以推广空间分析和空间数据挖掘为己任，致力于在GIS界传递分析价值。虾神博客原文地址：《白话空间统计二十三：回归分析番外-ArcGIS中的OLS（三）》 http://blog.csdn.net/allenlu2008/article/details/70456024]]></content>
      <categories>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>ArcGIS</tag>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从定性遥感到定量遥感——大数据时代的空间数据科学]]></title>
    <url>%2F2017%2F04%2F22%2F%E4%BB%8E%E5%AE%9A%E6%80%A7%E9%81%A5%E6%84%9F%E5%88%B0%E5%AE%9A%E9%87%8F%E9%81%A5%E6%84%9F%E2%80%94%E2%80%94%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E4%BB%A3%E7%9A%84%E7%A9%BA%E9%97%B4%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[笔者最近一周之内连续听了四场关于定量遥感前沿进展的讲座（内容遍布目前定量遥感的诸多热点领域）。这四场讲座分别从定量遥感信息技术整体的前沿进展、气溶胶（民众最关心的PM2.5）定量遥感、植被生态环境定量遥感（高光谱遥感、多光谱遥感）到最后一个雷达遥感（SAR）。可以说十分丰富，信息量也非常大。所以听完之后，有些想法和思考来谈一谈关于从定性遥感到定量遥感的发展以及必然。首先会有很多人会疑惑什么是定量遥感？和普通的遥感有啥区别？那么我想还是先从遥感的起源和定义说起。遥感，也就是Remote Sensing。最早是由这位美国海军的老太太提出的，具体的故事看下图介绍。 从广义来说，遥感就是以非直接接触形式探测物体的一种方法，最广泛的一种方式就是以电磁波来进行探测。物体之间的差异，造成了对不同波长的电磁波反射特性不尽相同，通过这个特点，通过传感器接收物体反射回来的电磁波信息，就是典型的遥感探测，当然我们也可以称之为被动遥感。而通过传感器主动发射电磁波并接收物体反射回来的电磁波，同样是遥感探测，也可以称之为主动遥感（有点像海豚的超声波定位原理）。被动遥感的典型案例包括目前多数光学卫星遥感，主动遥感则是近年来兴起的微波遥感、激光雷达遥感等。当然从广义角度来说，无人机航拍这类也可以算是遥感的一类，但是从狭义来说，我认为它还算不上遥感。而遥感的狭义定义就是定量遥感的基础，遥感的狭义定义应当是指通过接收记录物体反射电磁波特性来探测物体性质的方法。所以狭义的遥感的关键是物体反射的电磁波特性。嗯，敲黑板、划重点。三个字：电磁波！电磁波！电磁波！重要事情说三遍。遥感的价值就在于遥感探测得到的电磁波信息，那么电磁波信息能带给我们什么呢？初中老师（也有可能是高中老师吧）曾经曰过：“太阳光是白光（其实它五颜六色），物体在人的视觉里呈现不同颜色，就是因为它吸收了部分光，反射了部分光，这一部分信息被人的视觉所接收并生成图像。”事实上，卫星遥感同样是这个道理，卫星遥感大部分接受的电磁波信息就是地物反射的太阳光波谱信息，所以，不了解遥感的人总觉得它就是在给地球拍照。 图片来源：ArcGIS Earth 其实准确说来也没错。人类视觉接收到的也是地物反射的太阳光波谱信息，相机和卫星接收到的也是如此，只不过人类视觉接收的信息可能以某种生物信息或者信号方式记录存在，而相机（RGB值）和卫星则均使用数字来记录。所以我们会觉得卫星遥感影像有时候很直观，看着跟眼睛看到的真实事物或者拍出来的照片一样，三者者事实上也是一样的道理（盗用知乎的一句话：传感器接受到外界光子，要么形成电压信号，要么形成电流信号，然后转换成我们所熟悉的pixel，尽管三者有些许差别，总的来说核心的问题并没有太大差异，文末贴链接）。一个很核心的问题在于，人类视觉可以接受的波谱信息有限制，也就是我们说的可见光部分。但是卫星遥感数据接收的范围则远比这广得多——微波（合成孔径雷达）、热红外等。但是无论是可见光部分或者非可见光部分，卫星所接收的都是用来描述电磁波的信息，而电磁波的信息又是反映物体的物理特性的（前面提到过，物体间的差异导致的反射波谱不同）。所以遥感技术应用的核心就是将电磁波信息转化为对人类有用的Knowlegde。那么如何转化呢？熟悉遥感的同学会知道目视解译这个词。也就是说看图识物。就像前面提到的，由于遥感成像原理与人眼成像原理类似，我们可以把它当地球拍的一系列照片，通过看着一系列照片，我们可以做监测（就像警察叔叔看监控找犯人一样），监测地球发生的变化，也可以监测某种地物的变化（从监控视频中找出犯人）。就像下面的图片。 图片来源：Google Earth Engine 然而一切的发明都是从偷懒开始的，一定会开始想方设法降低自己看照片的工作量。这就出现了计算机上面的一大分支——图像识别与图像分类。正如我上文提到的，卫星遥感的原理跟相机成像原理最核心部分是类似的。那么也就是说卫星遥感影像某种程度上也可以看成是特殊的“图片”。嗯，讲了很久。貌似都是遥感的基础概念。接下来！！！敲黑板，再次划重点。什么是定性遥感？什么是定量遥感？定性遥感就是类似于看图识物，通过将遥感影像当做特殊的“图片”，通过诸如计算机的图像识别、分类的方法去进行分析和处理得到我们所需要的Knowledge。比如简单的土地利用分类、面向对象的分割与分类或者监测变化等，仅仅是定性的划分。 GLC30 m土地覆被 而定量遥感，我就套用一下李传荣老师讲座时的定义：向社会和公众提供有用信息的技术。要精准描述构成地物状态特征的物理化学要素，以及导致地物目标变化的物理化学动力驱动机制。事实上，它的核心就是第一个重点，遥感目前的根本在于电磁波。那么电磁波这么一个物理现象，要做的不仅仅是将电磁波谱映射成普通图像去做解译、分类。既然我们通过卫星接收到了很多人类无法肉眼观测到的电磁波信息，那么我们就希望通过建立具有物理意义的方程以及模型，将电磁波信息转化为对人类更有用的Knowledge。这就是定量遥感所要做的事情。而定量遥感的典型分析方法就是耳熟能详的遥感定量反演。为什么叫反演？其实就是我想知道B的具体值，但是我无法直接观测B的具体值，但是我能观测到A的具体值，而A和B相互之间的关系，可以通过物理学意义的模型或者是其他模型进行表达，那么我就同过A的值去反推出B的值，这就是反演。相信大家就会很清楚，在定量遥感里，A就是传感器接受的波谱信息，而B则可以有很多种东西。比如：1.二氧化碳探测——温室气体 来源：GOSAT卫星反演结果。 https://data2.gosat.nies.go.jp/index_en.html 动图链接：https://pan.baidu.com/s/1hsNyTrY 去年我们国家在12月26日刚刚发射了TanSAT卫星，使得中国成为了第三个发射监测温室气体排放卫星的国家（前两个是美国和日本，分别是OCO-2卫星和GOSAT卫星），这件事对于我们的意义是什么呢？过去我们没有自己的碳卫星，发达国家在气候变化和谈上说中国排放多，我们难以反驳。现在我们有了自己的碳卫星，我们就有了谈判的手段。同时TanSAT卫星尽管有些方面不如前两个卫星，但是有些方面性能远超前两个卫星。2.地表温度地表温度反演，探究城市热环境的影响因素与空间格局分布。探究冷热岛效应成因分布。 MODIS的LST产品 3.气溶胶反演气溶胶，可能如PM2.5，PM10这些更耳熟能详。但是要注意的就是气溶胶光学厚度≠PM2.5，它是介质消光系数在大气垂直方向上的积分（简单说它没有分离PM2.5、PM10等气溶胶物质）。所以不做垂直订正和湿度订正就拿来跟PM2.5建立关系的AOD都是耍流氓。 PM2.5分布图，来源：网络 MODIS气溶胶产品 4.生态环境定量遥感植被动态变化与生态环境相关要素如叶面积指数、NDVI、光合有效辐射、NPP、GPP等。 VPM模型求算的GPP 除了以上四个，还有很多对应的定量遥感产品——水体叶绿素浓度产品、雪深产品等等。那么定量遥感的核心就是如何通过卫星接收的数据和具有实际物理意义的模型去反演得到我们所需要的产品。而从模型来说，主要分为几类：经验模型、半经验模型、物理模型。经验模型可以被认为是统计意义上的模型，即无视具体的物理过程，单纯靠统计方法建立的模型。半经验模型一般是结合了部分的物理意义的统计模型。物理模型则是严格按照电磁波谱和辐射传输特性经过推导的到的机理模型。可以说物理模型才是定量遥感真正的核心，因为前面两个模型建立之后通常无法复制。物理模型其实有两大部分，即包括了辐射传输方程推导的模型以及几何光学模型（普及下，这就是布鞋院士李小文院士从事的研究）。然而，定量遥感的研究已经有一段时间，却依旧处在一个不为人知的和无法广泛应用的时代。主要是由于地表的复杂性、定量遥感反演目前出现的病态反演问题、定量遥感产品缺乏验证和质量控制的多重因素影响。复杂性——地表非朗伯体特性、地形起伏等影响；病态反演问题——参数求解过程大部分是求解参数大于方程数；缺乏验证和质量控制——不确定性，缺乏统一标准等。当然可以说现在这些问题开始有逐步的改善——毕竟一句话，问题都解决了，还要搞研究的人干什么。但是可以肯定的是，目前计算机技术的发展、卫星载荷的发展、传感器的发展、多颗卫星共同监测、高光谱遥感、激光雷达、合成孔径雷达的普及的情况下，定量遥感的很多问题将会得到改善。所以是时候从定性遥感走向定量遥感，因为这是必然。随着技术的发展和模型精度提高，遥感应用产品也将更加普及。毕竟现在是大数据时代，我们不断强调的是数据即服务和软件即服务，定量遥感作为地球观测的客观载体，将会迸发出更大的潜力。很多人都会这么说，遥感数据是GIS里面的天生大数据，确实遥感数据满足了大数据的4V或者说5V的特征，但是它又跟计算机意义上的大数据有所不同。计算机上的大数据普遍的格式是什么呢？比较典型包括像文本、图片、视频，而甚至像LBS这样的经纬度数据。简而言之，它是高频数据，时间间隔非常短（诸如5min这样的时间间隔）。而遥感数据却不同，卫星的重访周期基本上很难达到5min，所以它非高频数据，那么从这个角度来说，很多大数据算法是否适用呢？同时它具有非常丰富的物理信息，所以就像老师们说的，我们更应该考虑的是这个不太一般的大数据如何去用，不是简单的拿来主义，用这些所谓的机器学习、深度学习去看图识物，可能更值得考虑的事如何将定量遥感的物理模型与大数据的数据挖掘、机器学习等手段相结合。最后的最后，回到主题——大数据时代的空间数据科学。对于空间数据科学来说，这可能是个最好的时代。因为我们不缺数据，但是可能也是个最坏的时代，因为我们多的是黑箱的算法，更缺少的是内在机制的理论研究。毕竟，科学家还是要有梦去追。 关于卫星遥感影像与相机成像原理差异知乎解答： https://www.zhihu.com/question/29835925 后记——从这四场讲座中，了解到很多前沿，对于定量遥感了解可能会更透彻。也了解到自己很多不足。一些简单的想法。就当是学习之后的呓语。最后的最后，推荐两本定量遥感的书吧（京东和当当有满减活动，不用谢，叫我雷锋）。]]></content>
      <categories>
        <category>RS</category>
      </categories>
      <tags>
        <tag>Consideration of Ecology and Geography</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google Earth网页初探]]></title>
    <url>%2F2017%2F04%2F19%2FGoogle%20Earth%E7%BD%91%E9%A1%B5%E7%89%88%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[最近三维GIS界有个比较爆炸性的消息，嗯，没错，Google Earth Enterprise（谷歌地球企业版）宣告开源，47万行代码的大项目就此开放。（深深感觉谷歌就是要搞事情）。概括下GEE开源可以提供些啥。官方github的Readme文档如是写道：1.Fusion（融合）——影像、矢量与地形（或者可以说集成）的多源数据融合，导入三维地球（可以漫游飞行）或者集成的二维地图。2.Server（服务器）——可以通过融合用户自定义的多源数据定制一个私人三维地球服务器（基于Apache或者Tornado）。3.Client（客户端）——谷歌地球企业版客户端和谷歌地图Javascript API V3版用来浏览三维地球和二维地图。可以说这个开源项目强大异常，感觉GEE开源对于GIS和Web三维开发者是个很大的福利。GEE开源地址： https://github.com/google/earthenterprise 接着近日，早前预告过会在世界地球日（4 月 22 日）前发布新版 Google Earth的 Google，今天公布了新消息，各位期待已久的网页版 Google Earth终于来了。现在 Google Earth在电脑端不再是只以应用形式存在，Google 为桌面浏览器推出了专门版本，不过目前只支持 Chrome，未来会增加对其它主流浏览器的支持。移动端方面，则是 Android 版先行，而 iOS 版会在未来得到更新（该段文字引自《Google 地球网页版上线，点击探索者按钮开启环球旅程吧》，文末贴链接）。嗯，终于进入了本篇的正题，虽然跑题的篇幅略长（其实我觉得也不算跑题）。首先贴主页面，感觉整个页面很舒服。 接下来介绍下功能。 主菜单其实包括了下面的几个功能，也可以登录你的谷歌账户。 定义地图样式。 而网页版最主要的两个功能（探索者和知识卡片，貌似官方把知识卡片称之为运气不错）。探索者是实现了在世界知名景点的虚拟旅游，包括嵌入了360°全景、VR以及谷歌街景。ps，给我一个谷歌地球，我能在跑步机上走遍世界。 这里尝试了下BBC的纪录片的这个虚拟旅游（在Youtube上，请自备梯子），这里只放了简单的gif图，后面有详细视频的链接。 知识卡片功能显示（随机到达2万个地点，都有知识卡片和简介） Google Earth在2005年推出之后，引起了全球对GIS、空间科学的新一轮思考与认知。是Google Earth真真正正让GIS、VGI（Volunteer Geographical information，志愿者地理信息）和空间科学的理念普及到了千家万户。而这一次Google Earth网页版的推出，又将驱动VR（Vitrual Reality，虚拟现实技术）与三维GIS的进一步发展。对于地理教学方面，Gooogle Earth真正提供了一个0门槛，高质量的集成。可以期待的是，随着GEE开源，接下来GEE的发展将再次焕发生机。 分享的链接： https://earth.google.com/web/@39.9388838,116.3974589,54.08506769a,142593.39805527d,35y,0h,0t,0r/data=CkwaShJECiUweDM1ZjA1Mjk2ZTcxNDJjYjk6MHhiOTYyNTYyMGFmMGZhOThhGXvXoC-980NAIXNjesISGl1AKgnljJfkuqzluIIYAiAB 谷歌地球网页版： https://earth.google.com/web/ 《Google 地球网页版上线，点击探索者按钮开启环球旅程吧》： http://cn.technode.com/post/2017-04-19/google-earth-voyager/ 体验视频链接： http://v.youku.com/v_show/id_XMjcxNzgyODYzMg==.html?spm=a2hzp.8253869.0.0&amp;from=y1.7-2#pactionhttp://v.youku.com/v_show/id_XMjcxNzgyNjIzMg==.html?spm=a2hzp.8253869.0.0&amp;from=y1.7-2#paction]]></content>
      <categories>
        <category>GIS</category>
      </categories>
      <tags>
        <tag>Google Earth</tag>
        <tag>3D-GIS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MODIS数据的简介和下载（三）——MODIS数据下载方式（基于MODIS Web Service）]]></title>
    <url>%2F2017%2F04%2F15%2FMODIS%E6%95%B0%E6%8D%AE%E7%9A%84%E7%AE%80%E4%BB%8B%E5%92%8C%E4%B8%8B%E8%BD%BD%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94MODIS%E6%95%B0%E6%8D%AE%E4%B8%8B%E8%BD%BD%E6%96%B9%E5%BC%8F%EF%BC%88%E5%9F%BA%E4%BA%8EMODIS%20Web%20Service%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这是MODIS数据的简介和下载的最后一篇，下载方式的进阶版——基于MODIS Web Service的下载方式。这篇是笔者课程上机实习内容之一，做些简要总结和整理。事实上MODIS产品系列就如前面提到的，由于搭载在Terra星和Aqua星上，所以产品就包括了Terra星、Aqua星以及二者集成的产品。分别以MOD（Terra星）、MYD（Aqua星）、MCD（二者集成）作区分。具体的产品查询网站除了前面文章简单提到的之外，还可以查看官网。 https://lpdaac.usgs.gov/dataset_discovery/modis/modis_products_table/ 当然这一次进阶版的下载方式是基于Web Service的。那么Web Service是什么呢？这边引用了课程ppt的一段话。 Web service是一个平台独立的，低耦合的，自包含的、基于可编程的web的应用程序，可使用开放的XML（标准通用标记语言下的一个子集）标准来描述、发布、发现、协调和配置这些应用程序，用于开发分布式的互操作的应用程序。 Web Service技术， 能使得运行在不同机器上的不同应用无须借助附加的、专门的第三方软件或硬件， 就可相互交换数据或集成。依据Web Service规范实施的应用之间， 无论它们所使用的语言、 平台或内部协议是什么， 都可以相互交换数据。Web Service是自描述、 自包含的可用网络模块， 可以执行具体的业务功能。Web Service也很容易部署， 因为它们基于一些常规的产业标准以及已有的一些技术，诸如标准通用标记语言下的子集XML、HTTP。Web Service减少了应用接口的花费。Web Service为整个企业甚至多个组织之间的业务流程的集成提供了一个通用机制。 简单说，这是个方便你下载的插件。具体的下载地址就在下面了。官方提供了多种语言的客户端，包括Java，Perl，Python，Kepler，Matlab和R。本篇主要介绍Matlab和R的客户端如何下载MODIS数据。 https://modis.ornl.gov/data/modis_webservice.html 先介绍Matlab的客户端，首先在官网下载Matlab的客户端。 客户端压缩包文件： 客户端在Matlab的部署非常简单。只需要拷贝到Matlab的工作目录即可。当然使用的时候要求位于如图的路径中。 接下来就可以愉快地使用了。当然，由于官方的镜像搬迁的问题，需要更新对应的镜像地址。 对应在Matlab客户端的modisClient.m文件中找到替换的镜像地址，保存后即可开始使用。 在Matlab中调用不含任何参数的modisClient，可以看到可供下载的MODIS产品列表。 modisClient() 用产品名称作为参数，则可以看到该产品下所有数据集。 modisClient(&apos;MOD15A2&apos;) modisClient(&apos;MOD15A2&apos;,&apos;Lai_1km&apos;,36.833,116.567) 加上数据集名称（以1 km的叶面积指数数据为例）以及经纬度坐标。结果为对应的数据集（该数据8天为间隔）所在的时间范围。 在前面的基础上加上时间范围即可用客户端下载对应的MODIS数据。数据的下载格式是一个多元结构体。包括了数据、转换因子、对应时间序列和单位等等。这样我们就用Matlab下载到了对应的MODIS数据。 YC_LAI2009=modisClient(&apos;MOD15A2&apos;,&apos;Lai_1km&apos;,36.833,116.567,2009000,2009365) 接下来我们讲的是R语言的客户端及其下载方式。R语言的客户端有两种配置方法，一个是基础设置（基于SSOAP来进行），一个是高级设置（MODISTools包）。笔者个人使用的是高级设置，基础设置没有配置过。只是对官方给出的例子做了下翻译，具体的demo如下： 123456789101112131415161718##确定你安装了SSOAP包，否则先安装SSOAP，用install.packages("SSOAP"）##接着你就可以尝试用命令行的方式来下载裁切你想要的MODIS影像数据了## 载入包library(SSOAP)## 获取SOAP服务ornlMODIS = processWSDL("http://daac.ornl.gov/cgibin/MODIS/GLBVIZ_1_Glb_subset/MODIS_webservice.wsdl")## 定义函数设置ornlMODISFuncs = genSOAPClientInterface(operations=ornlMODIS@operations[[1]], def=ornlMODIS)## 使用获取裁切影像的函数设定result = ornlMODISFuncs@functions$getsubset(40.115,-110.025,"MOD11A2","LST_Day_1km","A2001001","A2001025",1,1)##打印结果print(result) 基础设置R语言demo地址： https://modis.ornl.gov/files/modiswebservice/R_getsubset.r 接下来用MODISTools包来做测试，GetProducts函数类似于Matlab的moidsClient（）： GetProducts() 而对应的查看数据集的函数并不是在GetProdcuts函数中填入参数，而是使用GetBands函数。 GetBands(&quot;MOD15A2&quot;) 对应查看数据时间范围的函数为GetDates。 GetDates(36.833,116.567,&quot;MOD15A2&quot;) 而类似于Matlab客户端下载数据的函数则为GetSubset和MODISSubsets。 YC_LAI2009&lt;-GetSubset(36.833,116.567,&quot;MOD15A2&quot;,&quot;Lai_1km&quot;,&quot;A2009001&quot;,&quot;A2009081&quot;,KmAboveBelow = 0,KmLeftRight = 0) GetSubset函数较为简单。但笔者测试时，发现终止时间仅能到达第81天（LAI数据为8天合成产品）。目前尚不清楚具体原因，故最后使用MODISSubsets获取对应的数据。MODISSubsets必须先建立一个数据框作为经纬度（lat,long为字段名)，时间限制范围（start.date，end.date为字段名）的数据。而函数中比较重要的参数还包括了Size和TimeSeriesLength，Size可以用默认值（经纬度位置所处瓦片数量，c(0,0)表示像元中心值），TimeSeriesLength表示时间序列长度，等于1代表从一年的开头到结尾。运行程序，会发现在工作目录下生成了一个.asc文件（即对应MODIS下载下来的数据）。 yclai2009&lt;-data.frame(lat=36.833,long=116.567,start.date=2009,end.date=2009) MODISSubsets(LoadDat = yclai2009,Products = &quot;MOD15A2&quot;,Bands = &quot;Lai_1km&quot;,Size = c(0,0),StartDate = T,TimeSeriesLength = 1) 最后对获取的LAI数据进行绘图可视化。 123456#Matlab中Puredata=[YC_LAI2009.data(:,:)]plot([0:(length(Puredata)-1)]*8+1,Puredata*YC_LAI2009.scale,'b-')ylabel=(YC_LAI2009.units)xlabel=('day of year')title=('禹城站2009年LAI') 1234#R中a&lt;-read.table("Lat36.83300Lon116.56700Start2009-01-01End2009-12-31___MOD15A2.asc",sep = ",")lai&lt;-data.frame(day=seq(1,365,8),lai=a$V11*0.1)plot(lai,type="l",pch=16,col="blue",xlab="day of year",ylab="LAI",main="禹城站2009年LAI") Matlab绘图结果 R绘图结果 总的来说，Matlab和R的客户端下载各有优缺点，而基于MODIS Web Service的下载方式最大好处就是在于它的Subset功能，而不是需要先下载整景影像再处理。在做单点模型的时候是非常快捷的。当然客户端的其它函数还有很多，包括像质量控制。本文没有对数据进行质量控制。实际研究中这个是必须进行的步骤（也可以基于客户端的函数来进行，譬如R里面的QualityCheck函数，Matlab的modisClientGetQC等）。此外地理所也开发了在线平台，研究人员只需填写所需参数即可下载。 http://159.226.110.142/carboncloud/datetool/toolmethod?url=onlinedo&amp;pId=3]]></content>
      <categories>
        <category>RS</category>
      </categories>
      <tags>
        <tag>MODIS</tag>
        <tag>R</tag>
        <tag>Matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MODIS数据的简介和下载（二）——MODIS数据下载方式（FTP）]]></title>
    <url>%2F2017%2F04%2F14%2FMODIS%E6%95%B0%E6%8D%AE%E7%9A%84%E7%AE%80%E4%BB%8B%E5%92%8C%E4%B8%8B%E8%BD%BD%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94MODIS%E6%95%B0%E6%8D%AE%E4%B8%8B%E8%BD%BD%E6%96%B9%E5%BC%8F%EF%BC%88FTP%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前一篇我们已经介绍了MODIS数据的简介、参数以及相关的典型应用。这一篇我们来介绍下MODIS数据的下载方式。当然这边主要是介绍国外网站的下载方式，国内网站的普遍是在地理空间数据云和遥感集市下载。国外网站（NASA官网）下载方式主要介绍两种。本篇主要针对第一种方式，基于完整的一景影像下载的过程（FTP工具）。后面一篇更新的是基于MODIS Web Service的客户端下载的方式（Matlab和R）。FTP下载工具以及完整影像下载的方法笔者最早也是参照网上某篇博客学习的下载方式，老规矩，文末贴链接，这里先感谢这位写博客的同学。同时NASA官网最近刚刚改版了网站，新版网站的下载方式暂时没有看到其他人整理，故稍作些整理。NASA官方遥感影像数据下载网址（里面不止有MODIS，还有ENVISAT，NPP-VIIRS等）： https://ladsweb.modaps.eosdis.nasa.gov/ 选择DATA DISCOVERY的Search&amp;Order：进入这个页面：找到你要下载的卫星传感器（这里以Terra星做例子）：选择我们需要的产品（这里选了1 km、500 m的二级产品，以及经纬度校正的产品，具体产品介绍看上一篇博客的表格以及列出的链接）：结束后直接点击“TIME”，根据需要选择数据的时间跨度。接着点击下一步，“LOCATION”。有不同选择需要影像范围的方法。Country→按国家的行政边界来选择数据Tiles→按瓦片选择，或者说按照MODIS数据全球逐行逐列划分的格网（也可以说是MODIS观测时的行列号）来选择数据Validation Sites→按全球分布的验证的观测站点或者生态网络观测站点选择（可以结合观测站点数据做研究）Draw Custom Box（Classic）→按照用户自定义画出来的框选选择数据（用过老版网站下载的同学会对这个功能比较熟悉）Enter Coordinates→用经纬度坐标来选择数据（这个也是类似于老版网站的下载功能），其实就是研究区的四至坐标。这里选择了按瓦片选择的方式，点击箭头。下一步，“FILES”。这一步会把符合要求的所有数据列出来，但是由于前面搜索的时间跨度太长，web端无法完全显示，所以我们重新修改下相关的需求数据（仅选择2010年的数据，而且选择下载合成产品）。接下来是新版网站的一个比较不同的地方，这里的”FILES“，可以直接下载数据。老版网站一般需要Order，提交订单之后才能下载。这样使用比原来更自由些。现在选择所有数据，提交订单即可。不过新版网站现在要求要注册账户。所以在下载数据前，记得先申请一个Earthdata的账户（这也就是比较麻烦的点了，目前笔者测试了163和qq邮箱，都没有办法收到激活邮件。目前只有谷歌邮箱能激活，所以必须去注册一个gmail的邮箱）。注意，需要在自己的账户中的“My Application”下面启动“LAADS Web”，最后才能提交订单。FTP的下载必须等到订单出现availlable，才能下载。官方给出的等待时间是5min到10天左右。一般很快就OK了。FTP下载方式的话，需要用FlashFXP这类软件来下载，有需要这个软件的可以在评论区留邮箱或在我的主页（友情链接里面）搜索下我的邮箱。 地址根据你的订单来决定。ftp: ladsweb.modaps.eosdis.nasa.govusername:anoymouspassword:你账户申请用的邮箱（gmail邮箱）anoymous意为FTP里的匿名传输。接下来只需要简单地拖拽就可以将上面的数据拷贝到本地文件夹了。这就是关于MODIS数据下载方式(FTP)的内容。最后附上旧版网站下载方式教程博客网址： http://blog.sina.com.cn/s/blog_8684880b010149ue.html]]></content>
      <categories>
        <category>RS</category>
      </categories>
      <tags>
        <tag>MODIS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MODIS数据的简介和下载（一）——MODIS数据简介]]></title>
    <url>%2F2017%2F04%2F13%2FMODIS%E6%95%B0%E6%8D%AE%E7%9A%84%E7%AE%80%E4%BB%8B%E5%92%8C%E4%B8%8B%E8%BD%BD%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94MODIS%E6%95%B0%E6%8D%AE%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[借最近上课实习上机内容，来介绍MODIS数据相关方面内容。本部分主要包括了MODIS数据的简介和下载的问题。本篇是第一部分，MODIS的简介。主要分为三个部分：1.MODIS传感器简介及参数；2.MODIS产品及命名规则；3.MODIS的典型应用。1.MODIS传感器简介及参数首先来纠正件很容易被误解的事，MODIS是传感器而不是卫星，尽管我们平常称呼的时候更习惯叫MODIS数据（以传感器来称呼），Landsat数据（以卫星来称呼）。MODIS传感器的全称为中分辨率成像光谱仪（moderate-resolution imaging spectroradiometer）,主要搭载在Terra和Aqua星上。Terrra的简介如下（摘自百度百科和遥感集市）：EOS（Earth Observation System）卫星是美国地球观测系统计划中一系列卫星的简称。经过长达8年的制造和前期预研究准备工作，第一颗EOS的上午轨道卫星于1999年12月18日发射升空，发射成功的卫星命名为Terra（拉丁语“地球”的意思），主要目的是观测地球表面。它是一个用一系列低轨道卫星对地球进行连续综合观测的计划。它的主要目的是：实现从单系列极轨空间平台上对太阳辐射、大气、海洋和陆地进行综合观测，获取有关海洋、陆地、冰雪圈和太阳动力系统等信息；进行土地利用和土地覆盖研究、气候的季节和年际变化研究、自然灾害监测和分析研究、长期气候变率和变化以及大气臭氧变化研究等；进而实现对大气和地球环境变化的长期观测和研究的总体（战略）目标。EOS卫星轨道高度为距地球705公里，目前的第一颗上午轨道卫星（Terra）过境时间为地方时10:30am左右，一天最多可以获得4条过境轨道资料。Terra卫星于1999年12月18日发射成功，Aqua卫星于2002年5月4日发射成功。Terra为上午星，从北向南于地方时10:30左右通过赤道，Aqua为下午星，从南向北于地方时13：30左右通过赤道。两颗星相互配合每1-2天可重复观测整个地球表面，得到36个波段的观测数据EOS系列卫星上的最主要的仪器是中分辨率成像光谱仪（MODIS），其最大空间分辨率可达250米。对应的MODIS传感器的简介如下（摘自百度百科和遥感集市）：MODIS是当前世界上新一代“图谱合一”的光学遥感仪器，有36个离散光谱波段，光谱范围宽，从0.4微米（可见光）到14.4微米（热红外）全光谱覆盖。MODIS的多波段数据可以同时提供反映陆地表面状况、云边界、云特性、海洋水色、浮游植物、生物地理、化学、大气中水汽、气溶胶、地表温度、云顶温度、大气温度、臭氧和云顶高度等特征的信息。可用于对地表、生物圈、固态地球、大气和海洋进行长期全球观测。中分辨率成像光谱仪（MODIS）最大空间分辨率可达250米，扫描宽度2330公里。MODIS是CZCS、AVHRR、HIRS和TM等仪器的继续。MODIS是被动式成像分光辐射计。共有490个探测器，分布在36个光谱波段，从0.4微米（可见光）到14.4微米（热红外）全光谱覆盖。MODIS仪器的地面分辨率为250m、500m和1000m，扫描宽度为2330km。在对地观测过程中，每秒可同时获得11兆比特的来自大气、海洋和陆地表面信息，日或每两日可获取一次全球观测数据。MODIS参数（摘自百度百科和遥感集市）空间分辨率——250 m (1-2波段)；500 m (3-7波段)；1000 m (8-36波段)扫描宽度——2330km时间分辨率——1天光谱波段——36个离散光谱波段，光谱范围宽，从0.4微米（可见光）到14.4微米（热红外）全光谱覆盖 。轨道——705KM，降轨上午10:30过境，升轨下午1:30过境；太阳同步；近极地圆轨道设计寿命——5年2.MODIS产品及命名规则按处理级别划分，可以分为以下5种：0级产品：也称原始数据;1级产品：指L1A数据，已经被赋予定标参数;2级产品：经过定标定位后数据，本系统产品是国际标准 的EOS-HDF格式。包含所有波段数据，是应用比较广泛的一类数据。;3级产品：在1B数据的基础上，对由遥感器成像过程产生的边缘畸变(Bowtie效应)进行校正，产生L3级产品;4级产品：由参数文件提供的参数，对图像进行几何纠正，辐射校正，使图像的每一点都有精确的地理编码、反射率和辐射率。L4级产品的MODIS图像进行不同时相的匹配时，误差小于1个像元。该级产品是应用级产品不可缺少的基础;5级及以上产品：根据各种应用模型开发L5级产品。MODIS命名规则如下（摘自CSDN博客）：MOD04 是产品名称，表示MODIS气溶胶产品。L2 表示 产品级别，Level2。A2005224 表示产品时间2005年第224天（以每年1月1日为第一天）。0205 表示卫星过境时间，换算成北京时间要加8小时。005表示产品版本，Version005，之前是v004，相比之前版本有很多改进。2006225195920 表示的是产品处理时间。3.MODIS的典型应用MODIS数据的简介大部分是从网上找的资源，链接都附在后面了，已经有很多人做了很多详细介绍，本篇就借花献佛，主要做简单地整理与引用，没有过多赘述。笔者自己写的核心内容主要是MODIS的一些典型应用和相关的一些研究进展。由于MODIS数据是免费获取的，并且具有高时间分辨率，在生态学和地理学研究中有很多广泛的应用。从产品就可以发现它可以监测的相关内容。植被动态监测以及植被生理生态的多种产品遥感反演典型的是NDVI合成产品、NPP（净初级生产力）、LAI（叶面积指数）、植被动态变化。这一部分内容主要应用生态环境监测中，尤其是生态学相关研究。因为NDVI和LAI是宏观尺度上可以反映植物生理生态的两个重要参数，目前演化出来的相关应用非常的多。在农学上，引入这两个参数来进行区域的农作物估产。在林学上，估算森林生物量、NPP、GPP以及光合有效辐射等。通过NDVI和LAI来进行物候变化监测。通过遥感数据去驱动生态模型，生态系统对于全球变化的响应的相关研究上，MODIS的这一系列产品都起到了很大的作用。基于MODIS的LAI产品结合模拟退火算法和DSSAT模型进行玉米估产模型的参数优化（数据同化） 地表温度反演以及相关产品遥感反演运用MODIS的热红外波段，通过劈窗算法（类似AVHRR）可以反演地表温度。包括温度异常和林火产品。在林学上有广泛的应用。基于劈窗算法的MODIS反演地表温度 光学气溶胶厚度反演这部分是近些年来关注的重点，由于PM2.5或者说雾霾的造成的环境问题日益严重，如何从遥感监测PM2.5是一个研究热点，现在比较普遍的使用MODIS产品来进行光学气溶胶（AOD)反演，然后反演PM2.5。基于DDV算法的MODIS光学气溶胶厚度反演 其他应用MODIS的应用还有非常多，比如像海表温度反演——事实上这方面的温度反演精度要高于地表温度反演，主要是海水从性质上说属于近似黑体；叶绿素浓度反演；离水辐射；冰雪覆盖监测（以NDSI为例）等。最后的部分我们用一个简单的知网检索结果、词云可视化以及列出了Web of Science上被引频次最高的10篇文章（MODIS作为keywords，可视化用HistCite）来看看目前MODIS研究的一些进展以及经典文献。遥感集市MODIS简介： http://bbs.rscloudmart.com/forum.php?mod=viewthread&amp;tid=1761&amp;highlight=MODIS MODIS百度百科： http://baike.baidu.com/link?url=IdbXlrWPCG8JX8fUQRmrRSPWjWx4Q7-r_reaPgNTsL88llgGTFPjk_eXrS-5S_bTJwqBCvHJlNIA3MZiV3mbgK MODIS命名规则的CSDN博客： http://blog.csdn.net/xiaoxiang22/article/details/8363469http://blog.csdn.net/rumswell/article/details/9003215 MODIS数据处理相关博客——以ENVI为主，ENVI/IDL官方博客： http://blog.sina.com.cn/s/articlelist_1984634525_0_1.html]]></content>
      <categories>
        <category>RS</category>
      </categories>
      <tags>
        <tag>MODIS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Ubuntu中安装R的几种方式总结]]></title>
    <url>%2F2017%2F04%2F09%2F%E5%9C%A8Ubuntu%E4%B8%AD%E5%AE%89%E8%A3%85R%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[近来笔者由于研究需要，开始研究Linux系统，并动手安装了VMware和Ubuntu软件。 因缘际会（主要是自己开始入坑Github）发现之前在Windows下安装失败的一个R包bignmf无法安装原因。这个包只能在Ubuntu上测试运行。所以之前在windows上根本无法编译和安装。所以笔者打算在Ubuntu上安装R并安装这个包进行使用。这里简单解释下bignmf包的用处，它是基于Rcpp和RcppEigen两个包，通过底层C++代码调用实现的一个R包，实现的算法是NMF（Nonnegative Matrix Factorization，非负矩阵分解)，作者是爱荷华州立大学的潘岚峰大神。当然R本身自带也有NMF包，不过语法不是很友好的感觉，此外最近笔者也发现了另外的可以在windows上运行的NMF的R包，NMF的理论和应用方面，包括bignmf的编译安装，后面有时间会更新（先挖坑），这里不做详细介绍。回到本篇的主要目的，如何在Ubuntu中安装R。这里提供三个方法：1.Linux安装软件的普遍方法——命令行；2.新立得软件包；3.从官网下载R语言环境源码，自行编译安装。1.基于命令行的方法首先先进入/etc/apt/sources.list，变换软件源，同时进入管理员权限 cd /etc/apt/ gedit sources.list 在最下面添加一行，deb后面的网址是镜像，根据你的喜好选一个（反正我推荐清华的，速度快，不过之前用厦大的也不错），具体的镜像地址见后面的网址。 deb https://mirrors.tuna.tsinghua.edu.cn/CRAN/bin/linux/ubuntu xenial/ https://cran.r-project.org/mirrors.html 而ubuntu xenial则是根据ubuntu版本确定的。我的是16.04，所以是xenial。具体的看官方说明，文末贴链接。完了之后先更新下软件源。就可以开始安装R了。如果我们需要自行编译R包并且安装的话，就需要在安装r-base-dev。不过笔者测试过，3.3.3版本的r-base自带了r-base-dev。所以不需要进行额外安装。 apt-get update apt-get install r-base apt-get install r-base-dev 完了之后，官方推荐还可以再加个软件源，是关于R的拓展包的。这里贴出命令的通用格式，可以根据需求替换&lt;&gt;的内容。也可以添加下载的公共秘钥。 apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 51716619E084DAB9 deb https://&lt;my.favorite.ubuntu.mirror&gt;/ trusty-backports main restricted universe 完了之后，在命令行里敲入r，出现下面的页面说明安装成功。2.基于新立得软件管理包新立得软件管理包是Linux下的神器，可以很方便的管理各类软件和依赖库等（上篇提到的WRF-DA模块编译依赖库有些是用这个安装的，具体过程等介绍WRF安装时补充）。当然一开始我没在我的Ubuntu软件里找到新立得。后面仔细翻了下软件列表。发现了这个软件——Synaptic Package Manager，这个就是新立得软件管理包了。启动它，搜索r-base，如图，右击标记安装，然后应用。3.基于自行编译的方法自行编译的方法，笔者没有具体尝试。但是看了下官方文档。大致的流程如下：官方推荐是组织一个文件夹进行安装，一级文件夹为R_Home，然后把源码解压到R_Home下面，并在下面建立src, doc等多个二级文件夹。然后回到R_Home文件夹。以管理员身份进入。 ./configure make make check make check-all make check-all是针对全部的编译的（可选），最后在安装即可。 make install 可以改变安装路径 ./configure --prefix=/where/you/want/R/to/go make prefix=/path/to/here install 具体可以见官方文档（链接见文末）在R装好的情况下，为了写代码方便，推荐安装R最好的IDE，Rstudio。这边Rstudio的安装就不展开讲了。下载好deb安装文件，直接加命令行安装即可。 dpkg -i rstudio-1.0.136-amd64.deb 在Linux中用Rstudio简单画个散点图。 R语言linux安装官方文档： https://cran.r-project.org/bin/linux/ubuntu/README R语言镜像地址： https://cran.r-project.org/mirrors.html R语言linux编译安装官方文档： https://cran.r-project.org/doc/manuals/r-release/R-admin.html#Installation]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WRF-DA代码编译与安装（二）——WRF-DA模块的编译与安装]]></title>
    <url>%2F2017%2F04%2F04%2FWRF-DA%E4%BB%A3%E7%A0%81%E7%BC%96%E8%AF%91%E4%B8%8E%E5%AE%89%E8%A3%85(%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94WRF-DA%E6%A8%A1%E5%9D%97%E7%9A%84%E7%BC%96%E8%AF%91%E4%B8%8E%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[上一篇已经将所有WRF-DA模块所需要的依赖库都编译安装成功。接下来的步骤就是WRF-DA模块的编译与安装。 笔者已经事先从WRF官网下载了该模块的源码（版本为最新的3.8.1）。首先在设置个目录专门来存放WRF的主程序。选择在Home下面新建一个mode。命令如下： $ sudo mkdir mode 先进入管理员模式（sudo su命令），然后将WRFDA的压缩包全部复制到刚刚建好的文件夹中。 cp -r WRFDA_V3.8.1.tar.gz /home/mode/ 到刚刚建好的WRF文件夹里，同样进入管理员模式，并解压文件夹，到WRFDA目录中，配置环境变量，并设置编译类型。其中，rttov看是否需要，也可以不考虑安装。如若要安装，环境变量配置的路径为可以找到lib/librttov11.*.a的文件目录。 tar zxf WRFDA_V3.8.1.tar.gz cd WRFDA export NETCDF=/usr/local/NETCDF/ export hdf5=/usr/local/hdf5/ export rttov=/usr/rttov/ ./configure wrfda 然后出现了很多选项。选择 x86_64 Linux, gfortran compiler with gcc (serial)，键入32，回车。32到35分别代表32为serial 表示串行计算； 33为smpar 表示内存共享并行计算(shared memory option)，即使用openMP，大部分多核电脑都支持这项功能； 34为dmpar 表示分布式并行计算(distributed memory option)，即使用MPI 进行并行计算，主要用在计算集群，单个电脑就没必要用了； 35为dm+sm 表示同时使用openMP与MPI两种并行方式. 根据实际需要选择即可，最保险的方法就是选择 serial，不过这样编译出来的程序运行最慢（引自xg1990的博客）。笔者初步测试，选择串行计算的版本，而且根据官方文档和编译结果，其他模式还需要有其他相关的依赖库。选择完编译选项后，会出现提示选择嵌套选项，一般就选 basic 选项即可。当然，这边编译器不同的话，序号也有所不同。同时官方文档已声明3.8.1版本不支持dm和dm+sm版本。搞定之后，看到一条振奋人心的消息。接下来，就输入如下命令： ./compile all_wrfvar&gt;&amp;checkwrfda.log 然后等它编译完成就好了。当然，到这一步我还是有问题，因为我只编译安装了43个exe，完全成功应该有44个exe。并且发现这个缺少的exe是主程序，da_wfrda.exe。查看生成exe的命令。 ls -l var/build/*exe var/obsproc/src/obsproc.exe 接着就回头去看log文件以及官方编译要求。发现大部分是路径错误。于是重新配置安装依赖库，并将WRF所需的其他库一并安装，重新编译。终于成功。 以上就是WRF-DA模块的编译与安装。后面会更新WRF主程序的编译与安装方面的内容（具体时间待定）。最后再次感谢以下博客文档的帮助。 https://wenku.baidu.com/view/58851bc269eae009591bec0a.htmlhttps://nwpsaf.eu/site/software/rttov/rttov-v12/http://www2.mmm.ucar.edu/wrf/users/wrfda/updates-3.8.1.htmlhttps://xg1990.com/blog/archives/190 http://blog.csdn.net/luoying_1993/article/details/53228473 https://wenku.baidu.com/view/57e27fd14a7302768e9939f4.html?re=view http://www2.mmm.ucar.edu/wrf/users/wrfda/Docs/user_guide_V3.8.1/users_guide_chap6.htm#_Installing_WRF-Var]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>WRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WRF-DA代码编译与安装（一）——依赖库的编译与安装]]></title>
    <url>%2F2017%2F04%2F02%2FWRF-DA%E4%BB%A3%E7%A0%81%E7%BC%96%E8%AF%91%E4%B8%8E%E5%AE%89%E8%A3%85(%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E4%BE%9D%E8%B5%96%E5%BA%93%E7%9A%84%E7%BC%96%E8%AF%91%E4%B8%8E%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[由于笔者的研究需要用到数据同化技术，所以开始学习WRF相关内容（主要是WRF-DA模块）。这里先解释下WRF是什么东西。 WRF全称Weather Research and Forecasting Model, 是一个天气研究与预报模型.可以用来进行精细尺度的天气模拟与预报。 二战后，由于计算机技术的迅猛发展，气象预报技术也随之突飞猛进。短短的几十年里，世界各地的气象研究机关开发出了各自的相对独立的气象模式。这些模式之间缺少互换性，对科研及业务上的交流极其不便。从上世纪90年代后半开始，美国对这种乱立的模式状况进行反省。最后由美国环境预测中心（NCEP），美国国家大气研究中心（NCAR）等美国的科研机构为中心开始着手开发一种统一的气象模式。终于于2000 年开发出了WRF模式。同时，为使研究成果能够迅速地应用到现实的天气预报当中去，WRF模式分为ARW(the Advanced Research WRF)和NMM(the Nonhydrostatic Mesoscale Model)两种，即研究用和业务用两种形式，分别由NCEP和NCAR管理维持着。具体的可以见官网： http://www2.mmm.ucar.edu/wrf/users/ WRF模拟系统主要包含WPS和WRF两部分模块： WPS模块全称为WRF Pre-processing System，即WRF预处理系统，用来为WRF模型准备输入数据；如果只是做理想实验(idealized modeling)，就不需要用WPS处理真实数据。但是理想实验不在本文介绍范围内，本文介绍的是进行真实数据模拟的操作。 WRF模块就是数值求解的模块，它有两个版本：ARW(Advanced Research WRF) 和 NMM(Nonhydrostatic Mesoscale Model)。大多数研究者主要用的都是ARW版本，本文所有的介绍也都基于ARW版本。 除了WPS与WRF两大核心模块外，WRF系统还有很多附加模块：比如用于数据同化的WRF-DA，用于化学传输的WRF-chem，用于林火模拟的WRF-fire（该段文字引自xg1990的博客，具体地址文末贴出，感谢该位大神的分享）。安装运行WRF模拟系统必须在Linux系统。而笔者又无法放弃windows系统，同时目前工作还处于前期测试阶段，故决定选择用VMware虚拟机搭建一个Linux系统来测试。选用的VMware版本：12.5.2。Linux系统：Ubuntu 16.04具体安装过程不是本文重点。详情可见 https://jingyan.baidu.com/article/c275f6ba07e269e33d756714.html 此外附上Ubuntu官网链接 https://www.ubuntu.com/download/desktop WRF-DA编译与安装主要参照官方提供的ppt和文档（地址文末会贴出）。 首先看下WRF-DA编译与安装的需求。 上面提到了需求如下：1.Linux/Mac系统，基于Unix或Linux的系统2.（3DVAR）三维变分的案例内存占用不大，大的（4DVAR）四维变分内存消耗较大。3.支持C和Fortran的编译器（ifort/icc, gfortran/gcc,pgf90/pgcc）4..需要的一些库，类似于WRF。包括：Zlib，netCDF C/Fortran，MPI（MPICH），BUFR，CRTM，RTTOV，HDF5。系统已经安装完毕，而内存部分目前暂时不考虑。接下来看C和Fortran的编译器。Ubuntu内置了gcc的编译器。可以通过命令来查看。 ~$ gcc -v 结果如下： 接下来安装gfortran，也是通过命令进行安装。 ~$ sudo apt-get install gfortran 通过命令查看是否安装成功。 ~$ gfortran -v 接下来是几个库的下载与安装。zlib： http://www.zlib.net/ netCDF C/Fortran： http://www.unidata.ucar.edu/downloads/netcdf/index.jsp MPI（MPICH）： http://www.mpich.org/downloads/ BUFR：包含在WRF源代码中 CRTM：包含在WRF源代码中 RTTOV： https://nwpsaf.eu/site/software/rttov/ 需注册，最好自备梯子。 HDF5： https://support.hdfgroup.org/HDF5/ 将如上的几个库的安装包通过共享文件夹放入虚拟机中（mnt/hgfs/Share)。 zlib和hdf5和netCDF 4相关。具体安装步骤和教程借鉴了官方文档 http://www.unidata.ucar.edu/software/netcdf/docs/getting_and_building_netcdf.html#build_nc4_dap_from_source 1.zlib的安装：解压到/usr/下 $ cp -r zlib-1.2.11.tar.gz /usr/ $ tar zvf zlib-1.2.11.tar.gz 然后进入解压文件夹，并安装 $ ./configure --prefix=/usr/local/zlib $ make check $ make install 修改环境变量。 gedit ~/.bashrc # for zlib export ZLIB_HOME=/usr/local/zlib export LD_LIBRARY_PATH=$ZLIB_HOME/lib:$LD_LIBRARY_PATH 2.HDF5的安装：解压到/usr/下 $ tar -xvf hdf5-1.8.18.tar 然后进入解压文件夹，并安装 $ ./configure --with-zlib=/usr/local --prefix=/usr/local/hdf5 $ make $ make check $ make install HDF5的安装和检验参照： ./configure --prefix=/usr/local/hdf5 --with-zlib=/usr/local/zlib http://blog.csdn.net/luoying_1993/article/details/53228473 HDF5还需配置一个环境变量，避免下面的netCDF C安装报错。 $ gedit ~/.bashrc #for hd5 export CPPFLAGS=-I$PRO_PATH/usr/local/hdf5/include export LDFLAGS=-L$PRO_PATH/usr/local/hdf5/lib export LD_LIBRARY_PATH=$PRO_PATH/usr/local/hdf5/lib $ source ~/.bashrc 3.netCDF C/Fortran安装先装netCDF C： $ export CPPFLAGS=-I$PRO_PATH/usr/local/hdf5/include $ export LDFLAGS=-L$PRO_PATH/usr/local/hdf5/lib $ export LD_LIBRARY_PATH=$PRO_PATH/usr/local/hdf5/lib $ ./configure --prefix=/usr/local/NETCDF --enable-netcdf-4 $ make $ make check $ make install 接着装netCDF Fortran：先声明环境变量： $ export CPPFLAGS=-I/usr/local/NETCDF/include $ export LDFLAGS=-L/usr/local/NETCDF/lib 然后进行下一步编译。 $ ./configure --prefix=/usr/local/NETCDF FC=gfortran 4.mpich的安装：解压之类的步骤同上，同样放到usr下面。解压到指定路径。 $ tar zxf mpich-3.2.tar.gz $ ./configure -prefix=/usr/local/mpi/ 5.rttov的安装：rttov解压出来东西较多，同样新建个path来存放。 $ tar zxf rttov121.tar.gz $ cd src $ ../build/rttov_compile.sh 打完收工。目前应该就完成了WRF-DA编译安装前所有需要的依赖库的编译及安装。下一篇更新WRF-DA具体的编译与安装。由于对Linux系统不熟悉，加上坑爹的rttov，博客写了两三天。从内心坚持要提醒大家的一点，Linux编译环境一定要注意环境变量！！！ 最后重点鸣谢几位主要参考大神的博客以及相关文档： https://wenku.baidu.com/view/58851bc269eae009591bec0a.htmlhttps://nwpsaf.eu/site/software/rttov/rttov-v12/http://www2.mmm.ucar.edu/wrf/users/wrfda/updates-3.8.1.htmlhttps://xg1990.com/blog/archives/190 http://blog.csdn.net/luoying_1993/article/details/53228473]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>WRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一行代码更新R语言]]></title>
    <url>%2F2017%2F03%2F23%2F%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%E6%9B%B4%E6%96%B0R%E8%AF%AD%E8%A8%80%2F</url>
    <content type="text"><![CDATA[博客中已经陆续更新了两篇关于R语言的文章（相关系数矩阵可视化和读取Excel），按照上一篇挖的坑，这一期讲的是如何只用一行代码更新R语言。这里还是重新认真介绍下R语言（我真的只是凑个字数）好了，这里安利大家一本书。卡巴科弗. R语言实战[M]. 人民邮电出版社, 2016.事实上，我放的截图是2013年第一版，2016年有再版，建议大家可以购买纸质版。在第一版的时候，附录里提到了这么一件事。可以看到当时的2.13.0的版本R仍然没有什么可以自动更新R的方法。不过时至今日，R的版本已经到了3.3.3，在这三年间，R在编程语言排行榜上不断前行。已经有了长久的进步，当然，也出现了可以自动更新R的方法啦。这里介绍的就是R的一个包：installr。 installr {installr} R Documentation Installing software from RDescriptionGives the user the option to download software from within R. 上面是installr的官方文档介绍。接下来来讲所谓的一行代码更新R语言。这里有两个注意点：1.你的installr必须跟你的R版本对应，因为R语言默认安装的包都是适配最新的R语言版本。2.使用installr更新R语言必须在原生R里面，Rstudio里面无法进行（笔者没有尝试过其他R的IDE，有童鞋若有尝试也可以进行指正）。这里第一步先改下默认R的镜像（相信有很多童鞋应该改过了）。原生R更改设定为：程序包→设定CRAN镜像无论Python或者R，镜像统统选清华！。 1234#安装installrinstall.packages（installr)library(installr)updater()#就是这句。真得劲。一键更新 后面只要一路确定就好了。这个方法的好处在于，你可以不用重新安装你已经有的包。可以完整保留。注意的是这个包还依赖于stringr,stringi,magrittr。最后贴下这个包的官方文档航和新增的函数（super强大，还可以一键安装Python，RStudio等)。 NEW FUNCTIONS: install.python - Downloads and installs python 2 or 3UPDATED FUNCTIONS: install.URL now gives warning if there is suspicion that the user is not connected to the internet. updateR - added cran_mirror option]]></content>
      <categories>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R语言读取Excel的神器——openxlsx]]></title>
    <url>%2F2017%2F03%2F22%2FR%E8%AF%AD%E8%A8%80%E8%AF%BB%E5%8F%96Excel%E7%9A%84%E7%A5%9E%E5%99%A8%E2%80%94%E2%80%94openxlsx%2F</url>
    <content type="text"><![CDATA[这篇文章讲得是如何用R语言优雅地读取excel，当然前提是用R语言花式读excel，然后最优雅。作为非程序猿的各位同志们，可能最擅长的数据整理软件或者统计软件就是——嗯，没有错，它就是集万千宠爱于一身的E~~X~~O。咳咳咳，好了。隆重推出我们的主角——Excel事实上，Excel是个super强大的软件。基本上用它已经能完成大量的统计分析了。For example各类数理统计 线性规划（LINGO表示欲哭无泪，你丫的抢我饭碗）。当然，很久很久之前有这门本神书：陈彦光. 基于Excel的地理数据分析[M]. 科学出版社, 2010.当然，作为新时代的研究生，我们怎么能仅用Excel来完成一切的科研任务呢？用老师的话说，你们用Excel做的图，人家审稿都嫌low。这个时候R就登场了。关于R的简介我就不提了。欢迎各种度娘，扯了这么久的淡。终于要进入正题了。今天讲的是R语言的第一步，读数据——读Excel的数据。以下有三种方法:1.将Excel转存为csv格式文件，读csv文件。 a&lt;-read.csv(&quot;exercise1.csv&quot;,header = T) 2.用RODBC包读取Excel。 ab&lt;-odbcConnectExcel2007(&quot;exercise1.xls&quot;)#连接excel，32位系统使用odbcConnectExcel函数 sqlTables(ab) 根据需求读取对应的sheet1 a&lt;-sqlFetch(ab,&quot;Sheet1$&quot;) odbcClose(ab)#关闭句柄，此句是必须。 3.用openxlsx包读取Excel a&lt;-read.xlsx(&quot;exercise1.xlsx&quot;,sheet=1)#文件名+sheet的序号，简单粗暴 综合来看，openxlsx的方法简单粗暴，而且经多名骨灰级玩家证明，罕有bug出现。乃R语言和Excel读取的绝对神器。不过笔者也发现，openxlsx包仅适用于.xlsx格式文件。前期的xls格式文件可能还需要前两种方法来读取。除了以上三种方法，还有类似的包如xlsx、readxl。此处依旧强推神器openxlsx。首先，.xlsx文件存储行数大大提升，从65536行数据提升到了104万条数据。其次，它十分便捷，函数所需参数较少。当然最后的最后，它可能需要的R的版本比较的新，下一篇的预告：如何通过一行代码升级R。最后贴出全文的代码。 1234567891011121314151617181920#设置工作路径setwd("F:/R/applicationstatics")#第一种方法：读取csva&lt;-read.csv("exercise1.csv",header = T)#第二种方法：RODBC包#安装载入RODBC包，如果已安装，请跳过第一句语句install.packages(RODBC)library(RODBC)ab&lt;-odbcConnectExcel2007("exercise1.xls")#连接excel，32位系统使用odbcConnectExcel函数sqlTables(ab)a&lt;-sqlFetch(ab,"Sheet1$")odbcClose(ab)#关闭句柄，此句是必须。#第三种方法：openxlsxinstall.packages(openxlsx)library(openxlsx)a&lt;-read.xlsx("exercise1.xlsx",sheet=1)#文件名+sheet的序号，简单粗暴 当然文末小福利：《基于Excel的地理数据分析》的电子版。需要的童鞋可以在评论区留邮箱或在我的主页（友情链接里面）搜索下我的邮箱。。]]></content>
      <categories>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fortran语言初探及Win7 64位下Fortran开发环境配置]]></title>
    <url>%2F2017%2F01%2F16%2FFortran%E8%AF%AD%E8%A8%80%E5%88%9D%E6%8E%A2%E5%8F%8AWin7%2064%E4%BD%8D%E4%B8%8BFortran%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[笔者作为一只游走在生态、遥感、GIS与计算机的学生狗，最近终于因缘际会各种巧合下开始学习Fortran。 还记得遥感物理课上牛柳两位老师（真是一个折磨萌萌哒台湾腔南方银口音的老师组合）的辐射传输方程、几何光学模型时时出现Fortran的身影。好了，扯淡完毕，首先先来简介下Fortran语言。Fortran源自于“公式翻译”（英语：FormulaTranslation）的缩写，是一种编程语言。它是世界上最早出现的计算机高级程序设计语言，广泛应用于科学和工程计算领域。FORTRAN语言以其特有的功能在数值、科学和工程计算领域发挥着重要作用。Fortran 90之前的版本是人们所知晓的FORTRAN（全部字母大写），从Fortran 90以及以后的版本都写成Fortran（仅有第一个字母大写）（ps，来自度娘百科）。可以说Fortran是属于计算机编程语言中的老古董了，但是另一个重要特点就是在科学和工程计算领域应用广泛，主要是其编程语言本身在数组计算上的一些优点决定的。从TIOBE 2017年1月的编程语言排行榜来看Fortran排在第28位，仍居前30之列，说明该语言仍旧具有广泛适用人群。那么Fortran在地理学、生态学与遥感方面的应用典型有哪些呢？事实上，在地理学、生态学与遥感领域，Fortran可以说有大量的学者使用并建立开发了大量的模型。比如遥感方面，大气辐射传输6S模型、MODTRAN辐射传输模型；生态学方面，WOFOST作物生长模型、DSSAT作物生长模型、景观中性模型模拟软件RULE等。同时Fortran对数组处理的优势使得它能在遥感数据的处理方面担当举足轻重的角色（类比语言IDL、Matlab、Python的numpy），这也是笔者学习的初衷。当然，正如前面提到了，Fortran是个典型的老古董语言，应用广泛的相关模型基于的Fortran版本的编译器在Win 7及以上系统中基本无法正常安装，故Win 7 64位系统如何配置Fortran开发环境是Fortran语言学习的第一步。由于传统的Visual Fortran 6.6.0及以下版本在Win 7 64位无法兼容，网上虽有帖子提出了相关解决法方法，但笔者亲自尝试的结果是hello world无法运行，故这边介绍其他方法。这里有两种配置方法是可以的：第一种，安装Visual Studio。作为微软主推的IDE，VS在诸多IDE中确实功能突出，优点颇多，作为商业软件，简单的开发环境配置方法也是一大优势。只需勾选Fortran相关编译器安装，即可配置成功。第二种，安装其他IDE，由于VS的简便性导致将其分为一类，其他IDE只需有Fortran编译器即可。VS在简便性上确实很优秀，但是相对而言，VS是个典型的重量级IDE。相对而言，笔者最近喜欢轻量级IDE，故搜索了其他IDE，以Code::Blocks为例，偏爱它的另一个原因就是因为它是免费开源软件（开源大法好）。1.首先下载带有Fortran编译器的Code::Blocks软件。 http://www.codeblocks.org/ 选择最后一个2.直接安装即可，确认安装所有部分3.安装完毕后，打开IDE在菜单栏中找到“Setting”→“Compiler”复制一个编译器，自定义名字接着点“Toolchain executables”将画框部分的文件全部改成gfortran.exe点击ok即可。4.Hello World 编写在菜单栏找到”File”→”New”→”Project”，建立一个Fortran工程文件。工程命名选择自定义的编译器添加hello world项目的Fortran文件编写如下的hello world进行测试。 1234program helloworld implicit none write(*,*) 'Hello world'end program 5.生成exe文件无法打开的处理方法某些时候生成的exe文件打开会报错。类似“找不到*.dll”“这个应用程序安装/配置不正确，重新安装…”这样的错误。这样的情况下，只需在系统变量里面PATH加上对应的路径即可。]]></content>
      <categories>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>Fortran</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何做出相关系数矩阵可视化图]]></title>
    <url>%2F2016%2F12%2F11%2F%E5%A6%82%E4%BD%95%E5%81%9A%E5%87%BA%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E7%9F%A9%E9%98%B5%E5%8F%AF%E8%A7%86%E5%8C%96%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[如何做出相关系数矩阵可视化图 12345678910install.packages("psych")install.packages("corrplot")#安装包，如果已安装，请略过library(psych)library(corrplot)#载入两个包data(iris)#机器学习常用神奇数据集——鸢尾花数据集head(iris)#查看下数据集前五行irisnew&lt;-iris[,-5]#去除第五列种类变量cormat&lt;-corr.test(irisnew)#相关系数分析及显著性检验#最简单的相关系数矩阵可视化corrplot(cormat$r) corrplot(cormat$r,method=&quot;square&quot;) corrplot(cormat$r,method = &quot;number&quot;) corrplot(cormat$r,method = &quot;shade&quot;) corrplot(cormat$r,method=&quot;ellipse&quot;) corrplot(cormat$r,method = &quot;pie&quot;) corrplot(cormat$r,method=&quot;square&quot;,type=&quot;lower&quot;,title = &quot;Correlation of iris&quot;) #含显著性检验的相关系数矩阵可视化 cormatp&lt;-cormat$p#单独取出p值矩阵 cormatp[upper.tri(cormatp)]=0#设置p值矩阵上三角等于0 corrplot(cormat$r,method=&quot;square&quot;,type=&quot;lower&quot;,title = &quot;Correlation of iris&quot;,tl.cex=1.5,tl.pos = &quot;lt&quot;,number.cex=1,p.mat=cormatp,sig.level=0.05,insig=c(&quot;pch&quot;)) corrplot(cormat$r,method=&quot;square&quot;,type=&quot;full&quot;,title = &quot;Correlation of iris&quot;,tl.cex=1.5,tl.pos = &quot;lt&quot;,number.cex=1,p.mat=cormatp,sig.level=0.05,insig=c(&quot;pch&quot;)) corrplot.mixed(cormat$r,upper = &quot;square&quot;,lower = &quot;number&quot;,diag = &quot;u&quot;,tl.cex=1.5,tl.pos = &quot;lt&quot;,number.cex=1,p.mat=cormatp,sig.level=0.05,insig=c(&quot;pch&quot;))]]></content>
      <categories>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于ArcGIS Pro的二三维一体化]]></title>
    <url>%2F2016%2F12%2F07%2Farcgispro%2F</url>
    <content type="text"><![CDATA[最近尝试了下ArcGIS Pro的二三维一体化管理功能。感觉相当不错，记录一下使用过程与操作。ArcGIS Pro是一款全新的桌面应用程序，它改变了桌面GIS的工作方式，以满足新一代WebGIS应用模式。ArcGIS Pro采用Ribbon界面风格，给人全新的用户体验。它作为一个高级的应用程序，可以对来自本地、ArcGIS Online、或者Portal for ArcGIS的数据进行可视化、编辑、分析。同时，实现了二三维一体化的数据可视化、管理、分析和发布。此外，ArcGIS Pro原生64位应用，支持多线程处理，极大提高软件性能。关于ArcGIS Pro的安装与使用请参考如下的网址（ps，可以无限试用）。 http://blog.csdn.net/kikitamoon/article/details/44102383 从GIS数据的种类来说，主要是两种：矢量数据和栅格数据。三维数据也可以基于这两种分别完成。介绍下这两类的三维化。（一）栅格的三维化栅格数据的三维化主要是类似于DEM拉伸成三维的山体。首先打开ArcGIS Pro新建一个二维和三维窗口右击三维场景属性，如下图，设置要三维化的栅格，此外可以根据需求做拉伸效果如下点击二三维联动按钮二三维联动效果：（二）矢量操作矢量的数据可以通过字段拉伸或者rpk生成。ArcGIS Pro在这方面无缝集成了City Engine的规则建模。因此首先在City Engine中写好CGA，通过City Engine生成规则包rpk。 123@StartRulelot--&gt;extrude(rand(3, 50)) 接着点击工具 生成三维体图层 二三维联动效果如下 ArcGIS Pro的整体界面非常友好，而且功能强大，也是目前esri主推的新软件，不仅是二三维一体化，在今年esri用户大会上，还演示了基于ArcGIS Pro的各类大数据应用。大数据时代，让我们拥抱ArcGIS Pro。]]></content>
      <categories>
        <category>GIS</category>
      </categories>
      <tags>
        <tag>ArcGIS Pro</tag>
        <tag>3D-GIS</tag>
        <tag>CGA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[桌面GIS连接Postgresql总结]]></title>
    <url>%2F2016%2F08%2F14%2F%E6%A1%8C%E9%9D%A2GIS%E8%BF%9E%E6%8E%A5Postgresql%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[对于非开发人员的GISer而言，数据库这东西更多停留在mdb，gdb的层面，相对而言这些数据的使用无论是在处理还是管理上，门槛相对较低。但是目前所处的信息爆炸的大数据时代，仅仅依靠桌面GIS本身的数据存储远远不够，在存储大量数据的时候，仍然需要专门的数据库管理。所以桌面GIS如何在关系型数据库中写入空间数据也是一个重要的过程。 此文是在阅读了网上的部分博客及自己的亲身经验写成。主要介绍桌面GIS中两大代表——Esri的ArcGIS以及开源的QGIS。使用的关系型数据库是Postgresql，它的空间扩展是PostGIS。桌面GIS：Esri ArcGIS 10.2Esri ArcSDE 10.2QGIS 2.8.2关系型数据库及空间扩展:Postgresql 9.5.0_x64PostGIS 2.2以上软件的安装略过了，网上均有教程。 （一）QGIS连接Postgresql个人最喜爱QGIS的一点就是它与PostGIS以及其他各类数据库的无缝衔接，确实可以说是直连数据库。主要是通过这个数据库的操作先新建一个连接，输入名称、主机、数据库、调整SSL模式、用户名、密码，最后测试连接。如果跳出这个页面，就证明你成功啦。接下来按确定之后，只要在最开始的页面点击“连接”，就已经愉快地连上了。如果你打开Postgresql，会发现全局架构是对应的，所以确认是连接成功的。!这边选择了一个2008年2月3日北京市的一辆出租车轨迹数据来做测试QGIS中，基于出租车轨迹生成的热图 （二）ArcGIS Desktop 连接PostgresqlArcGIS Desktop 10.2之后提供了Postgresql直连的功能，当然这里的直连，我认为可以叫伪直连，因为它仍然需要ArcSDE的支持，而不像QGIS可以直接连接。当然直连的的方法还是相对简单的，不过我也遇到了一个问题，我的Postgresql是64位。但是ArcGIS Desktop目前只有32位。所以即使安装了ArcSDE，也无法直接连接。需要Postgresql32位里面的一些dll文件。将这些Postgresql对应版本32位的dll文件复制粘贴到ArcGIS安装目录下面的”/ArcGIS/Desktop 10.2/bin的文件夹里，接着可以打开ArcGIS进行连接了。 选择同一个测试数据导入PostGIS基于ArcGIS连接Postgresql里面的数据制作的核密度图]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>ArcGIS</tag>
        <tag>PostGIS</tag>
        <tag>QGIS</tag>
      </tags>
  </entry>
</search>